{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import proj1_helpers as helpers\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import implementations as implementations\n",
    "from run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y, tX, ids = helpers.load_csv_data(\"../data/small.csv\")\n",
    "column_labels = \"Id\tPrediction\tDER_mass_MMC\tDER_mass_transverse_met_lep\tDER_mass_vis\tDER_pt_h\tDER_deltaeta_jet_jet\tDER_mass_jet_jet\tDER_prodeta_jet_jet\tDER_deltar_tau_lep\tDER_pt_tot\tDER_sum_pt\tDER_pt_ratio_lep_tau\tDER_met_phi_centrality\tDER_lep_eta_centrality\tPRI_tau_pt\tPRI_tau_eta\tPRI_tau_phi\tPRI_lep_pt\tPRI_lep_eta\tPRI_lep_phi\tPRI_met\tPRI_met_phi\tPRI_met_sumet\tPRI_jet_num\tPRI_jet_leading_pt\tPRI_jet_leading_eta\tPRI_jet_leading_phi\tPRI_jet_subleading_pt\tPRI_jet_subleading_eta\tPRI_jet_subleading_phi\tPRI_jet_all_pt\".split(\"\\t\")[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a237983958a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#tX = implementations.standardize(tX)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX' is not defined"
     ]
    }
   ],
   "source": [
    "tX[tX[:, :] == -999] = np.nan  \n",
    "#tX = implementations.standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_features(dataset):\n",
    "    for i in range(len(column_labels)):\n",
    "        plot.title(column_labels[i])\n",
    "        plot.hist(tX[np.isfinite(tX[:, i]), i], bins = 200)\n",
    "        plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[a,b,c,d] = run.bucket_events(tX)\n",
    "ya = y[a]\n",
    "tXa = run.prepare_data(tX[a])\n",
    "idsa = ids[a]\n",
    "yb = y[b]\n",
    "tXb = run.prepare_data(tX[b])\n",
    "idsb = ids[b]\n",
    "yc = y[c]\n",
    "tXc = run.prepare_data(tX[c])\n",
    "idsc = ids[c]\n",
    "yd = y[d]\n",
    "tXd = run.prepare_data(tX[d])\n",
    "idsd = ids[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/4999): loss=1.0\n",
      "Gradient Descent(1/4999): loss=0.9288189044851143\n",
      "Gradient Descent(2/4999): loss=0.8794269036715959\n",
      "Gradient Descent(3/4999): loss=0.8448859971940497\n",
      "Gradient Descent(4/4999): loss=0.8204723488922528\n",
      "Gradient Descent(5/4999): loss=0.8029695717791817\n",
      "Gradient Descent(6/4999): loss=0.7901875836141318\n",
      "Gradient Descent(7/4999): loss=0.780635035001008\n",
      "Gradient Descent(8/4999): loss=0.7732962924247877\n",
      "Gradient Descent(9/4999): loss=0.7674796041718168\n",
      "Gradient Descent(10/4999): loss=0.7627137288394341\n",
      "Gradient Descent(11/4999): loss=0.7586775580499975\n",
      "Gradient Descent(12/4999): loss=0.7551522022147584\n",
      "Gradient Descent(13/4999): loss=0.7519883695482245\n",
      "Gradient Descent(14/4999): loss=0.7490841570046808\n",
      "Gradient Descent(15/4999): loss=0.7463699298407563\n",
      "Gradient Descent(16/4999): loss=0.743798027244255\n",
      "Gradient Descent(17/4999): loss=0.7413357536378414\n",
      "Gradient Descent(18/4999): loss=0.7389606069313711\n",
      "Gradient Descent(19/4999): loss=0.7366570297311694\n",
      "Gradient Descent(20/4999): loss=0.7344141974078302\n",
      "Gradient Descent(21/4999): loss=0.7322245120779488\n",
      "Gradient Descent(22/4999): loss=0.7300825771867169\n",
      "Gradient Descent(23/4999): loss=0.7279844992941552\n",
      "Gradient Descent(24/4999): loss=0.7259274126293516\n",
      "Gradient Descent(25/4999): loss=0.7239091553110104\n",
      "Gradient Descent(26/4999): loss=0.7219280488269599\n",
      "Gradient Descent(27/4999): loss=0.7199827478159908\n",
      "Gradient Descent(28/4999): loss=0.7180721377145136\n",
      "Gradient Descent(29/4999): loss=0.7161952649921475\n",
      "Gradient Descent(30/4999): loss=0.7143512895760965\n",
      "Gradient Descent(31/4999): loss=0.7125394523836819\n",
      "Gradient Descent(32/4999): loss=0.7107590531423672\n",
      "Gradient Descent(33/4999): loss=0.7090094352152553\n",
      "Gradient Descent(34/4999): loss=0.7072899751975594\n",
      "Gradient Descent(35/4999): loss=0.7056000757627359\n",
      "Gradient Descent(36/4999): loss=0.7039391607225044\n",
      "Gradient Descent(37/4999): loss=0.7023066715955562\n",
      "Gradient Descent(38/4999): loss=0.7007020652048057\n",
      "Gradient Descent(39/4999): loss=0.6991248119762623\n",
      "Gradient Descent(40/4999): loss=0.6975743947169255\n",
      "Gradient Descent(41/4999): loss=0.6960503077201181\n",
      "Gradient Descent(42/4999): loss=0.6945520560950341\n",
      "Gradient Descent(43/4999): loss=0.6930791552501995\n",
      "Gradient Descent(44/4999): loss=0.6916311304829479\n",
      "Gradient Descent(45/4999): loss=0.6902075166422842\n",
      "Gradient Descent(46/4999): loss=0.6888078578428971\n",
      "Gradient Descent(47/4999): loss=0.6874317072151453\n",
      "Gradient Descent(48/4999): loss=0.6860786266806793\n",
      "Gradient Descent(49/4999): loss=0.6847481867466143\n",
      "Gradient Descent(50/4999): loss=0.6834399663134316\n",
      "Gradient Descent(51/4999): loss=0.6821535524932809\n",
      "Gradient Descent(52/4999): loss=0.6808885404364112\n",
      "Gradient Descent(53/4999): loss=0.6796445331641537\n",
      "Gradient Descent(54/4999): loss=0.6784211414073638\n",
      "Gradient Descent(55/4999): loss=0.6772179834495581\n",
      "Gradient Descent(56/4999): loss=0.676034684974206\n",
      "Gradient Descent(57/4999): loss=0.6748708789157845\n",
      "Gradient Descent(58/4999): loss=0.6737262053143157\n",
      "Gradient Descent(59/4999): loss=0.6726003111731732\n",
      "Gradient Descent(60/4999): loss=0.6714928503199927\n",
      "Gradient Descent(61/4999): loss=0.6704034832705597\n",
      "Gradient Descent(62/4999): loss=0.6693318770955661\n",
      "Gradient Descent(63/4999): loss=0.668277705290146\n",
      "Gradient Descent(64/4999): loss=0.6672406476461147\n",
      "Gradient Descent(65/4999): loss=0.6662203901268376\n",
      "Gradient Descent(66/4999): loss=0.6652166247446667\n",
      "Gradient Descent(67/4999): loss=0.6642290494408838\n",
      "Gradient Descent(68/4999): loss=0.6632573679680944\n",
      "Gradient Descent(69/4999): loss=0.6623012897750198\n",
      "Gradient Descent(70/4999): loss=0.6613605298936317\n",
      "Gradient Descent(71/4999): loss=0.6604348088285835\n",
      "Gradient Descent(72/4999): loss=0.6595238524488866\n",
      "Gradient Descent(73/4999): loss=0.658627391881787\n",
      "Gradient Descent(74/4999): loss=0.6577451634087935\n",
      "Gradient Descent(75/4999): loss=0.656876908363815\n",
      "Gradient Descent(76/4999): loss=0.6560223730333602\n",
      "Gradient Descent(77/4999): loss=0.655181308558762\n",
      "Gradient Descent(78/4999): loss=0.6543534708403763\n",
      "Gradient Descent(79/4999): loss=0.6535386204437234\n",
      "Gradient Descent(80/4999): loss=0.6527365225075248\n",
      "Gradient Descent(81/4999): loss=0.6519469466536008\n",
      "Gradient Descent(82/4999): loss=0.6511696668985861\n",
      "Gradient Descent(83/4999): loss=0.6504044615674308\n",
      "Gradient Descent(84/4999): loss=0.6496511132086451\n",
      "Gradient Descent(85/4999): loss=0.6489094085112552\n",
      "Gradient Descent(86/4999): loss=0.6481791382234335\n",
      "Gradient Descent(87/4999): loss=0.6474600970727685\n",
      "Gradient Descent(88/4999): loss=0.6467520836881414\n",
      "Gradient Descent(89/4999): loss=0.6460549005231752\n",
      "Gradient Descent(90/4999): loss=0.6453683537812251\n",
      "Gradient Descent(91/4999): loss=0.6446922533418789\n",
      "Gradient Descent(92/4999): loss=0.6440264126889317\n",
      "Gradient Descent(93/4999): loss=0.6433706488398131\n",
      "Gradient Descent(94/4999): loss=0.6427247822764283\n",
      "Gradient Descent(95/4999): loss=0.6420886368773897\n",
      "Gradient Descent(96/4999): loss=0.6414620398516079\n",
      "Gradient Descent(97/4999): loss=0.6408448216732153\n",
      "Gradient Descent(98/4999): loss=0.6402368160177941\n",
      "Gradient Descent(99/4999): loss=0.639637859699883\n",
      "Gradient Descent(100/4999): loss=0.6390477926117352\n",
      "Gradient Descent(101/4999): loss=0.6384664576633019\n",
      "Gradient Descent(102/4999): loss=0.637893700723419\n",
      "Gradient Descent(103/4999): loss=0.6373293705621682\n",
      "Gradient Descent(104/4999): loss=0.6367733187943909\n",
      "Gradient Descent(105/4999): loss=0.636225399824332\n",
      "Gradient Descent(106/4999): loss=0.6356854707913879\n",
      "Gradient Descent(107/4999): loss=0.6351533915169386\n",
      "Gradient Descent(108/4999): loss=0.6346290244522402\n",
      "Gradient Descent(109/4999): loss=0.6341122346273572\n",
      "Gradient Descent(110/4999): loss=0.6336028896011129\n",
      "Gradient Descent(111/4999): loss=0.6331008594120358\n",
      "Gradient Descent(112/4999): loss=0.632606016530285\n",
      "Gradient Descent(113/4999): loss=0.6321182358105311\n",
      "Gradient Descent(114/4999): loss=0.6316373944457748\n",
      "Gradient Descent(115/4999): loss=0.6311633719220855\n",
      "Gradient Descent(116/4999): loss=0.6306960499742361\n",
      "Gradient Descent(117/4999): loss=0.6302353125422243\n",
      "Gradient Descent(118/4999): loss=0.6297810457286516\n",
      "Gradient Descent(119/4999): loss=0.6293331377569539\n",
      "Gradient Descent(120/4999): loss=0.628891478930456\n",
      "Gradient Descent(121/4999): loss=0.6284559615922415\n",
      "Gradient Descent(122/4999): loss=0.6280264800858155\n",
      "Gradient Descent(123/4999): loss=0.6276029307165489\n",
      "Gradient Descent(124/4999): loss=0.6271852117138844\n",
      "Gradient Descent(125/4999): loss=0.6267732231942909\n",
      "Gradient Descent(126/4999): loss=0.6263668671249513\n",
      "Gradient Descent(127/4999): loss=0.6259660472881677\n",
      "Gradient Descent(128/4999): loss=0.6255706692464698\n",
      "Gradient Descent(129/4999): loss=0.625180640308414\n",
      "Gradient Descent(130/4999): loss=0.6247958694950548\n",
      "Gradient Descent(131/4999): loss=0.624416267507081\n",
      "Gradient Descent(132/4999): loss=0.6240417466925979\n",
      "Gradient Descent(133/4999): loss=0.623672221015546\n",
      "Gradient Descent(134/4999): loss=0.6233076060247407\n",
      "Gradient Descent(135/4999): loss=0.6229478188235227\n",
      "Gradient Descent(136/4999): loss=0.6225927780400056\n",
      "Gradient Descent(137/4999): loss=0.6222424037979086\n",
      "Gradient Descent(138/4999): loss=0.6218966176879632\n",
      "Gradient Descent(139/4999): loss=0.6215553427398816\n",
      "Gradient Descent(140/4999): loss=0.6212185033948748\n",
      "Gradient Descent(141/4999): loss=0.6208860254787134\n",
      "Gradient Descent(142/4999): loss=0.6205578361753138\n",
      "Gradient Descent(143/4999): loss=0.6202338640008438\n",
      "Gradient Descent(144/4999): loss=0.6199140387783365\n",
      "Gradient Descent(145/4999): loss=0.6195982916128012\n",
      "Gradient Descent(146/4999): loss=0.6192865548668213\n",
      "Gradient Descent(147/4999): loss=0.6189787621366298\n",
      "Gradient Descent(148/4999): loss=0.6186748482286558\n",
      "Gradient Descent(149/4999): loss=0.6183747491365251\n",
      "Gradient Descent(150/4999): loss=0.6180784020185139\n",
      "Gradient Descent(151/4999): loss=0.6177857451754418\n",
      "Gradient Descent(152/4999): loss=0.6174967180289967\n",
      "Gradient Descent(153/4999): loss=0.6172112611004823\n",
      "Gradient Descent(154/4999): loss=0.6169293159899807\n",
      "Gradient Descent(155/4999): loss=0.6166508253559219\n",
      "Gradient Descent(156/4999): loss=0.6163757328950502\n",
      "Gradient Descent(157/4999): loss=0.6161039833227818\n",
      "Gradient Descent(158/4999): loss=0.6158355223539447\n",
      "Gradient Descent(159/4999): loss=0.6155702966838926\n",
      "Gradient Descent(160/4999): loss=0.6153082539699881\n",
      "Gradient Descent(161/4999): loss=0.6150493428134434\n",
      "Gradient Descent(162/4999): loss=0.6147935127415156\n",
      "Gradient Descent(163/4999): loss=0.6145407141900481\n",
      "Gradient Descent(164/4999): loss=0.6142908984863487\n",
      "Gradient Descent(165/4999): loss=0.614044017832402\n",
      "Gradient Descent(166/4999): loss=0.6138000252884075\n",
      "Gradient Descent(167/4999): loss=0.6135588747566347\n",
      "Gradient Descent(168/4999): loss=0.6133205209655929\n",
      "Gradient Descent(169/4999): loss=0.6130849194545067\n",
      "Gradient Descent(170/4999): loss=0.6128520265580926\n",
      "Gradient Descent(171/4999): loss=0.6126217993916293\n",
      "Gradient Descent(172/4999): loss=0.6123941958363168\n",
      "Gradient Descent(173/4999): loss=0.6121691745249189\n",
      "Gradient Descent(174/4999): loss=0.6119466948276834\n",
      "Gradient Descent(175/4999): loss=0.6117267168385316\n",
      "Gradient Descent(176/4999): loss=0.6115092013615182\n",
      "Gradient Descent(177/4999): loss=0.6112941098975492\n",
      "Gradient Descent(178/4999): loss=0.6110814046313572\n",
      "Gradient Descent(179/4999): loss=0.6108710484187267\n",
      "Gradient Descent(180/4999): loss=0.610663004773968\n",
      "Gradient Descent(181/4999): loss=0.6104572378576271\n",
      "Gradient Descent(182/4999): loss=0.6102537124644377\n",
      "Gradient Descent(183/4999): loss=0.6100523940115018\n",
      "Gradient Descent(184/4999): loss=0.609853248526697\n",
      "Gradient Descent(185/4999): loss=0.6096562426373084\n",
      "Gradient Descent(186/4999): loss=0.6094613435588778\n",
      "Gradient Descent(187/4999): loss=0.6092685190842654\n",
      "Gradient Descent(188/4999): loss=0.6090777375729243\n",
      "Gradient Descent(189/4999): loss=0.6088889679403791\n",
      "Gradient Descent(190/4999): loss=0.608702179647904\n",
      "Gradient Descent(191/4999): loss=0.608517342692403\n",
      "Gradient Descent(192/4999): loss=0.60833442759648\n",
      "Gradient Descent(193/4999): loss=0.6081534053986998\n",
      "Gradient Descent(194/4999): loss=0.6079742476440375\n",
      "Gradient Descent(195/4999): loss=0.6077969263745064\n",
      "Gradient Descent(196/4999): loss=0.6076214141199688\n",
      "Gradient Descent(197/4999): loss=0.6074476838891196\n",
      "Gradient Descent(198/4999): loss=0.6072757091606437\n",
      "Gradient Descent(199/4999): loss=0.6071054638745405\n",
      "Gradient Descent(200/4999): loss=0.6069369224236154\n",
      "Gradient Descent(201/4999): loss=0.6067700596451313\n",
      "Gradient Descent(202/4999): loss=0.6066048508126217\n",
      "Gradient Descent(203/4999): loss=0.6064412716278574\n",
      "Gradient Descent(204/4999): loss=0.6062792982129667\n",
      "Gradient Descent(205/4999): loss=0.6061189071027062\n",
      "Gradient Descent(206/4999): loss=0.6059600752368773\n",
      "Gradient Descent(207/4999): loss=0.6058027799528879\n",
      "Gradient Descent(208/4999): loss=0.6056469989784539\n",
      "Gradient Descent(209/4999): loss=0.6054927104244403\n",
      "Gradient Descent(210/4999): loss=0.6053398927778383\n",
      "Gradient Descent(211/4999): loss=0.6051885248948742\n",
      "Gradient Descent(212/4999): loss=0.6050385859942499\n",
      "Gradient Descent(213/4999): loss=0.6048900556505108\n",
      "Gradient Descent(214/4999): loss=0.6047429137875401\n",
      "Gradient Descent(215/4999): loss=0.6045971406721752\n",
      "Gradient Descent(216/4999): loss=0.6044527169079456\n",
      "Gradient Descent(217/4999): loss=0.6043096234289274\n",
      "Gradient Descent(218/4999): loss=0.604167841493717\n",
      "Gradient Descent(219/4999): loss=0.6040273526795152\n",
      "Gradient Descent(220/4999): loss=0.6038881388763253\n",
      "Gradient Descent(221/4999): loss=0.6037501822812597\n",
      "Gradient Descent(222/4999): loss=0.6036134653929528\n",
      "Gradient Descent(223/4999): loss=0.6034779710060819\n",
      "Gradient Descent(224/4999): loss=0.6033436822059883\n",
      "Gradient Descent(225/4999): loss=0.6032105823634013\n",
      "Gradient Descent(226/4999): loss=0.6030786551292602\n",
      "Gradient Descent(227/4999): loss=0.6029478844296366\n",
      "Gradient Descent(228/4999): loss=0.6028182544607474\n",
      "Gradient Descent(229/4999): loss=0.6026897496840654\n",
      "Gradient Descent(230/4999): loss=0.6025623548215203\n",
      "Gradient Descent(231/4999): loss=0.602436054850789\n",
      "Gradient Descent(232/4999): loss=0.6023108350006755\n",
      "Gradient Descent(233/4999): loss=0.6021866807465767\n",
      "Gradient Descent(234/4999): loss=0.602063577806033\n",
      "Gradient Descent(235/4999): loss=0.6019415121343625\n",
      "Gradient Descent(236/4999): loss=0.6018204699203767\n",
      "Gradient Descent(237/4999): loss=0.6017004375821766\n",
      "Gradient Descent(238/4999): loss=0.6015814017630277\n",
      "Gradient Descent(239/4999): loss=0.6014633493273094\n",
      "Gradient Descent(240/4999): loss=0.6013462673565455\n",
      "Gradient Descent(241/4999): loss=0.6012301431455028\n",
      "Gradient Descent(242/4999): loss=0.6011149641983662\n",
      "Gradient Descent(243/4999): loss=0.6010007182249839\n",
      "Gradient Descent(244/4999): loss=0.6008873931371825\n",
      "Gradient Descent(245/4999): loss=0.6007749770451513\n",
      "Gradient Descent(246/4999): loss=0.6006634582538933\n",
      "Gradient Descent(247/4999): loss=0.6005528252597419\n",
      "Gradient Descent(248/4999): loss=0.6004430667469443\n",
      "Gradient Descent(249/4999): loss=0.6003341715843052\n",
      "Gradient Descent(250/4999): loss=0.6002261288218962\n",
      "Gradient Descent(251/4999): loss=0.600118927687824\n",
      "Gradient Descent(252/4999): loss=0.6000125575850589\n",
      "Gradient Descent(253/4999): loss=0.5999070080883233\n",
      "Gradient Descent(254/4999): loss=0.5998022689410364\n",
      "Gradient Descent(255/4999): loss=0.599698330052317\n",
      "Gradient Descent(256/4999): loss=0.5995951814940393\n",
      "Gradient Descent(257/4999): loss=0.5994928134979471\n",
      "Gradient Descent(258/4999): loss=0.5993912164528171\n",
      "Gradient Descent(259/4999): loss=0.5992903809016779\n",
      "Gradient Descent(260/4999): loss=0.5991902975390778\n",
      "Gradient Descent(261/4999): loss=0.5990909572084065\n",
      "Gradient Descent(262/4999): loss=0.5989923508992617\n",
      "Gradient Descent(263/4999): loss=0.5988944697448683\n",
      "Gradient Descent(264/4999): loss=0.5987973050195426\n",
      "Gradient Descent(265/4999): loss=0.5987008481362048\n",
      "Gradient Descent(266/4999): loss=0.5986050906439364\n",
      "Gradient Descent(267/4999): loss=0.5985100242255827\n",
      "Gradient Descent(268/4999): loss=0.5984156406954001\n",
      "Gradient Descent(269/4999): loss=0.5983219319967447\n",
      "Gradient Descent(270/4999): loss=0.5982288901998069\n",
      "Gradient Descent(271/4999): loss=0.5981365074993833\n",
      "Gradient Descent(272/4999): loss=0.5980447762126933\n",
      "Gradient Descent(273/4999): loss=0.5979536887772328\n",
      "Gradient Descent(274/4999): loss=0.5978632377486698\n",
      "Gradient Descent(275/4999): loss=0.5977734157987767\n",
      "Gradient Descent(276/4999): loss=0.5976842157134007\n",
      "Gradient Descent(277/4999): loss=0.5975956303904728\n",
      "Gradient Descent(278/4999): loss=0.5975076528380509\n",
      "Gradient Descent(279/4999): loss=0.5974202761724017\n",
      "Gradient Descent(280/4999): loss=0.5973334936161138\n",
      "Gradient Descent(281/4999): loss=0.5972472984962489\n",
      "Gradient Descent(282/4999): loss=0.5971616842425251\n",
      "Gradient Descent(283/4999): loss=0.5970766443855331\n",
      "Gradient Descent(284/4999): loss=0.5969921725549849\n",
      "Gradient Descent(285/4999): loss=0.5969082624779959\n",
      "Gradient Descent(286/4999): loss=0.5968249079773966\n",
      "Gradient Descent(287/4999): loss=0.5967421029700755\n",
      "Gradient Descent(288/4999): loss=0.5966598414653524\n",
      "Gradient Descent(289/4999): loss=0.5965781175633817\n",
      "Gradient Descent(290/4999): loss=0.5964969254535835\n",
      "Gradient Descent(291/4999): loss=0.596416259413104\n",
      "Gradient Descent(292/4999): loss=0.5963361138053046\n",
      "Gradient Descent(293/4999): loss=0.5962564830782764\n",
      "Gradient Descent(294/4999): loss=0.5961773617633832\n",
      "Gradient Descent(295/4999): loss=0.5960987444738315\n",
      "Gradient Descent(296/4999): loss=0.596020625903263\n",
      "Gradient Descent(297/4999): loss=0.595943000824378\n",
      "Gradient Descent(298/4999): loss=0.5958658640875784\n",
      "Gradient Descent(299/4999): loss=0.5957892106196383\n",
      "Gradient Descent(300/4999): loss=0.5957130354223984\n",
      "Gradient Descent(301/4999): loss=0.5956373335714824\n",
      "Gradient Descent(302/4999): loss=0.5955621002150382\n",
      "Gradient Descent(303/4999): loss=0.5954873305725013\n",
      "Gradient Descent(304/4999): loss=0.5954130199333802\n",
      "Gradient Descent(305/4999): loss=0.5953391636560639\n",
      "Gradient Descent(306/4999): loss=0.5952657571666506\n",
      "Gradient Descent(307/4999): loss=0.5951927959577985\n",
      "Gradient Descent(308/4999): loss=0.595120275587596\n",
      "Gradient Descent(309/4999): loss=0.5950481916784524\n",
      "Gradient Descent(310/4999): loss=0.5949765399160099\n",
      "Gradient Descent(311/4999): loss=0.5949053160480726\n",
      "Gradient Descent(312/4999): loss=0.5948345158835581\n",
      "Gradient Descent(313/4999): loss=0.5947641352914645\n",
      "Gradient Descent(314/4999): loss=0.5946941701998579\n",
      "Gradient Descent(315/4999): loss=0.5946246165948776\n",
      "Gradient Descent(316/4999): loss=0.5945554705197597\n",
      "Gradient Descent(317/4999): loss=0.5944867280738764\n",
      "Gradient Descent(318/4999): loss=0.5944183854117941\n",
      "Gradient Descent(319/4999): loss=0.5943504387423492\n",
      "Gradient Descent(320/4999): loss=0.5942828843277362\n",
      "Gradient Descent(321/4999): loss=0.5942157184826179\n",
      "Gradient Descent(322/4999): loss=0.5941489375732468\n",
      "Gradient Descent(323/4999): loss=0.5940825380166045\n",
      "Gradient Descent(324/4999): loss=0.5940165162795559\n",
      "Gradient Descent(325/4999): loss=0.5939508688780173\n",
      "Gradient Descent(326/4999): loss=0.5938855923761428\n",
      "Gradient Descent(327/4999): loss=0.5938206833855199\n",
      "Gradient Descent(328/4999): loss=0.5937561385643841\n",
      "Gradient Descent(329/4999): loss=0.5936919546168454\n",
      "Gradient Descent(330/4999): loss=0.5936281282921282\n",
      "Gradient Descent(331/4999): loss=0.5935646563838252\n",
      "Gradient Descent(332/4999): loss=0.5935015357291655\n",
      "Gradient Descent(333/4999): loss=0.5934387632082936\n",
      "Gradient Descent(334/4999): loss=0.5933763357435629\n",
      "Gradient Descent(335/4999): loss=0.5933142502988409\n",
      "Gradient Descent(336/4999): loss=0.593252503878827\n",
      "Gradient Descent(337/4999): loss=0.5931910935283818\n",
      "Gradient Descent(338/4999): loss=0.5931300163318697\n",
      "Gradient Descent(339/4999): loss=0.593069269412511\n",
      "Gradient Descent(340/4999): loss=0.593008849931747\n",
      "Gradient Descent(341/4999): loss=0.5929487550886158\n",
      "Gradient Descent(342/4999): loss=0.5928889821191388\n",
      "Gradient Descent(343/4999): loss=0.592829528295719\n",
      "Gradient Descent(344/4999): loss=0.5927703909265484\n",
      "Gradient Descent(345/4999): loss=0.5927115673550271\n",
      "Gradient Descent(346/4999): loss=0.5926530549591916\n",
      "Gradient Descent(347/4999): loss=0.5925948511511547\n",
      "Gradient Descent(348/4999): loss=0.5925369533765529\n",
      "Gradient Descent(349/4999): loss=0.5924793591140058\n",
      "Gradient Descent(350/4999): loss=0.592422065874583\n",
      "Gradient Descent(351/4999): loss=0.5923650712012822\n",
      "Gradient Descent(352/4999): loss=0.5923083726685152\n",
      "Gradient Descent(353/4999): loss=0.5922519678816024\n",
      "Gradient Descent(354/4999): loss=0.5921958544762785\n",
      "Gradient Descent(355/4999): loss=0.5921400301182039\n",
      "Gradient Descent(356/4999): loss=0.5920844925024868\n",
      "Gradient Descent(357/4999): loss=0.5920292393532123\n",
      "Gradient Descent(358/4999): loss=0.5919742684229807\n",
      "Gradient Descent(359/4999): loss=0.591919577492453\n",
      "Gradient Descent(360/4999): loss=0.591865164369905\n",
      "Gradient Descent(361/4999): loss=0.5918110268907883\n",
      "Gradient Descent(362/4999): loss=0.5917571629173006\n",
      "Gradient Descent(363/4999): loss=0.5917035703379611\n",
      "Gradient Descent(364/4999): loss=0.5916502470671947\n",
      "Gradient Descent(365/4999): loss=0.5915971910449245\n",
      "Gradient Descent(366/4999): loss=0.5915444002361684\n",
      "Gradient Descent(367/4999): loss=0.5914918726306451\n",
      "Gradient Descent(368/4999): loss=0.5914396062423862\n",
      "Gradient Descent(369/4999): loss=0.5913875991093555\n",
      "Gradient Descent(370/4999): loss=0.591335849293073\n",
      "Gradient Descent(371/4999): loss=0.5912843548782484\n",
      "Gradient Descent(372/4999): loss=0.5912331139724176\n",
      "Gradient Descent(373/4999): loss=0.5911821247055885\n",
      "Gradient Descent(374/4999): loss=0.5911313852298914\n",
      "Gradient Descent(375/4999): loss=0.5910808937192344\n",
      "Gradient Descent(376/4999): loss=0.5910306483689678\n",
      "Gradient Descent(377/4999): loss=0.5909806473955503\n",
      "Gradient Descent(378/4999): loss=0.5909308890362245\n",
      "Gradient Descent(379/4999): loss=0.5908813715486962\n",
      "Gradient Descent(380/4999): loss=0.590832093210819\n",
      "Gradient Descent(381/4999): loss=0.5907830523202848\n",
      "Gradient Descent(382/4999): loss=0.5907342471943203\n",
      "Gradient Descent(383/4999): loss=0.5906856761693874\n",
      "Gradient Descent(384/4999): loss=0.5906373376008889\n",
      "Gradient Descent(385/4999): loss=0.5905892298628805\n",
      "Gradient Descent(386/4999): loss=0.5905413513477868\n",
      "Gradient Descent(387/4999): loss=0.5904937004661207\n",
      "Gradient Descent(388/4999): loss=0.5904462756462112\n",
      "Gradient Descent(389/4999): loss=0.5903990753339323\n",
      "Gradient Descent(390/4999): loss=0.5903520979924379\n",
      "Gradient Descent(391/4999): loss=0.5903053421019023\n",
      "Gradient Descent(392/4999): loss=0.5902588061592625\n",
      "Gradient Descent(393/4999): loss=0.5902124886779678\n",
      "Gradient Descent(394/4999): loss=0.5901663881877312\n",
      "Gradient Descent(395/4999): loss=0.5901205032342866\n",
      "Gradient Descent(396/4999): loss=0.5900748323791498\n",
      "Gradient Descent(397/4999): loss=0.5900293741993821\n",
      "Gradient Descent(398/4999): loss=0.5899841272873607\n",
      "Gradient Descent(399/4999): loss=0.5899390902505504\n",
      "Gradient Descent(400/4999): loss=0.58989426171128\n",
      "Gradient Descent(401/4999): loss=0.5898496403065232\n",
      "Gradient Descent(402/4999): loss=0.5898052246876824\n",
      "Gradient Descent(403/4999): loss=0.5897610135203759\n",
      "Gradient Descent(404/4999): loss=0.5897170054842299\n",
      "Gradient Descent(405/4999): loss=0.589673199272672\n",
      "Gradient Descent(406/4999): loss=0.5896295935927307\n",
      "Gradient Descent(407/4999): loss=0.5895861871648364\n",
      "Gradient Descent(408/4999): loss=0.5895429787226252\n",
      "Gradient Descent(409/4999): loss=0.589499967012749\n",
      "Gradient Descent(410/4999): loss=0.5894571507946852\n",
      "Gradient Descent(411/4999): loss=0.589414528840552\n",
      "Gradient Descent(412/4999): loss=0.5893720999349257\n",
      "Gradient Descent(413/4999): loss=0.5893298628746613\n",
      "Gradient Descent(414/4999): loss=0.5892878164687162\n",
      "Gradient Descent(415/4999): loss=0.5892459595379765\n",
      "Gradient Descent(416/4999): loss=0.589204290915087\n",
      "Gradient Descent(417/4999): loss=0.5891628094442829\n",
      "Gradient Descent(418/4999): loss=0.5891215139812247\n",
      "Gradient Descent(419/4999): loss=0.5890804033928366\n",
      "Gradient Descent(420/4999): loss=0.5890394765571466\n",
      "Gradient Descent(421/4999): loss=0.5889987323631297\n",
      "Gradient Descent(422/4999): loss=0.5889581697105535\n",
      "Gradient Descent(423/4999): loss=0.5889177875098257\n",
      "Gradient Descent(424/4999): loss=0.5888775846818469\n",
      "Gradient Descent(425/4999): loss=0.5888375601578616\n",
      "Gradient Descent(426/4999): loss=0.5887977128793149\n",
      "Gradient Descent(427/4999): loss=0.5887580417977102\n",
      "Gradient Descent(428/4999): loss=0.58871854587447\n",
      "Gradient Descent(429/4999): loss=0.5886792240807975\n",
      "Gradient Descent(430/4999): loss=0.5886400753975427\n",
      "Gradient Descent(431/4999): loss=0.5886010988150682\n",
      "Gradient Descent(432/4999): loss=0.5885622933331199\n",
      "Gradient Descent(433/4999): loss=0.5885236579606968\n",
      "Gradient Descent(434/4999): loss=0.5884851917159257\n",
      "Gradient Descent(435/4999): loss=0.5884468936259365\n",
      "Gradient Descent(436/4999): loss=0.5884087627267388\n",
      "Gradient Descent(437/4999): loss=0.5883707980631032\n",
      "Gradient Descent(438/4999): loss=0.5883329986884412\n",
      "Gradient Descent(439/4999): loss=0.5882953636646897\n",
      "Gradient Descent(440/4999): loss=0.5882578920621955\n",
      "Gradient Descent(441/4999): loss=0.5882205829596034\n",
      "Gradient Descent(442/4999): loss=0.5881834354437445\n",
      "Gradient Descent(443/4999): loss=0.588146448609527\n",
      "Gradient Descent(444/4999): loss=0.5881096215598294\n",
      "Gradient Descent(445/4999): loss=0.5880729534053943\n",
      "Gradient Descent(446/4999): loss=0.5880364432647245\n",
      "Gradient Descent(447/4999): loss=0.58800009026398\n",
      "Gradient Descent(448/4999): loss=0.5879638935368793\n",
      "Gradient Descent(449/4999): loss=0.5879278522245978\n",
      "Gradient Descent(450/4999): loss=0.5878919654756718\n",
      "Gradient Descent(451/4999): loss=0.5878562324459028\n",
      "Gradient Descent(452/4999): loss=0.5878206522982617\n",
      "Gradient Descent(453/4999): loss=0.5877852242027976\n",
      "Gradient Descent(454/4999): loss=0.5877499473365451\n",
      "Gradient Descent(455/4999): loss=0.5877148208834351\n",
      "Gradient Descent(456/4999): loss=0.5876798440342057\n",
      "Gradient Descent(457/4999): loss=0.5876450159863154\n",
      "Gradient Descent(458/4999): loss=0.5876103359438577\n",
      "Gradient Descent(459/4999): loss=0.5875758031174758\n",
      "Gradient Descent(460/4999): loss=0.5875414167242807\n",
      "Gradient Descent(461/4999): loss=0.5875071759877686\n",
      "Gradient Descent(462/4999): loss=0.5874730801377401\n",
      "Gradient Descent(463/4999): loss=0.5874391284102227\n",
      "Gradient Descent(464/4999): loss=0.5874053200473912\n",
      "Gradient Descent(465/4999): loss=0.5873716542974915\n",
      "Gradient Descent(466/4999): loss=0.5873381304147656\n",
      "Gradient Descent(467/4999): loss=0.587304747659376\n",
      "Gradient Descent(468/4999): loss=0.5872715052973345\n",
      "Gradient Descent(469/4999): loss=0.587238402600428\n",
      "Gradient Descent(470/4999): loss=0.5872054388461496\n",
      "Gradient Descent(471/4999): loss=0.5871726133176269\n",
      "Gradient Descent(472/4999): loss=0.587139925303555\n",
      "Gradient Descent(473/4999): loss=0.5871073740981277\n",
      "Gradient Descent(474/4999): loss=0.5870749590009712\n",
      "Gradient Descent(475/4999): loss=0.5870426793170789\n",
      "Gradient Descent(476/4999): loss=0.5870105343567464\n",
      "Gradient Descent(477/4999): loss=0.5869785234355083\n",
      "Gradient Descent(478/4999): loss=0.586946645874075\n",
      "Gradient Descent(479/4999): loss=0.5869149009982719\n",
      "Gradient Descent(480/4999): loss=0.5868832881389784\n",
      "Gradient Descent(481/4999): loss=0.5868518066320677\n",
      "Gradient Descent(482/4999): loss=0.5868204558183489\n",
      "Gradient Descent(483/4999): loss=0.5867892350435078\n",
      "Gradient Descent(484/4999): loss=0.5867581436580515\n",
      "Gradient Descent(485/4999): loss=0.5867271810172504\n",
      "Gradient Descent(486/4999): loss=0.5866963464810848\n",
      "Gradient Descent(487/4999): loss=0.5866656394141887\n",
      "Gradient Descent(488/4999): loss=0.5866350591857971\n",
      "Gradient Descent(489/4999): loss=0.5866046051696928\n",
      "Gradient Descent(490/4999): loss=0.5865742767441549\n",
      "Gradient Descent(491/4999): loss=0.5865440732919065\n",
      "Gradient Descent(492/4999): loss=0.5865139942000658\n",
      "Gradient Descent(493/4999): loss=0.586484038860094\n",
      "Gradient Descent(494/4999): loss=0.5864542066677492\n",
      "Gradient Descent(495/4999): loss=0.5864244970230358\n",
      "Gradient Descent(496/4999): loss=0.5863949093301581\n",
      "Gradient Descent(497/4999): loss=0.5863654429974732\n",
      "Gradient Descent(498/4999): loss=0.586336097437445\n",
      "Gradient Descent(499/4999): loss=0.5863068720665989\n",
      "Gradient Descent(500/4999): loss=0.5862777663054762\n",
      "Gradient Descent(501/4999): loss=0.5862487795785915\n",
      "Gradient Descent(502/4999): loss=0.5862199113143871\n",
      "Gradient Descent(503/4999): loss=0.5861911609451931\n",
      "Gradient Descent(504/4999): loss=0.5861625279071824\n",
      "Gradient Descent(505/4999): loss=0.5861340116403305\n",
      "Gradient Descent(506/4999): loss=0.5861056115883753\n",
      "Gradient Descent(507/4999): loss=0.5860773271987746\n",
      "Gradient Descent(508/4999): loss=0.5860491579226685\n",
      "Gradient Descent(509/4999): loss=0.5860211032148392\n",
      "Gradient Descent(510/4999): loss=0.5859931625336724\n",
      "Gradient Descent(511/4999): loss=0.5859653353411193\n",
      "Gradient Descent(512/4999): loss=0.5859376211026592\n",
      "Gradient Descent(513/4999): loss=0.5859100192872632\n",
      "Gradient Descent(514/4999): loss=0.5858825293673563\n",
      "Gradient Descent(515/4999): loss=0.5858551508187825\n",
      "Gradient Descent(516/4999): loss=0.5858278831207699\n",
      "Gradient Descent(517/4999): loss=0.5858007257558947\n",
      "Gradient Descent(518/4999): loss=0.5857736782100473\n",
      "Gradient Descent(519/4999): loss=0.5857467399723989\n",
      "Gradient Descent(520/4999): loss=0.5857199105353673\n",
      "Gradient Descent(521/4999): loss=0.5856931893945846\n",
      "Gradient Descent(522/4999): loss=0.5856665760488644\n",
      "Gradient Descent(523/4999): loss=0.58564007000017\n",
      "Gradient Descent(524/4999): loss=0.5856136707535831\n",
      "Gradient Descent(525/4999): loss=0.5855873778172714\n",
      "Gradient Descent(526/4999): loss=0.5855611907024604\n",
      "Gradient Descent(527/4999): loss=0.5855351089234009\n",
      "Gradient Descent(528/4999): loss=0.5855091319973399\n",
      "Gradient Descent(529/4999): loss=0.5854832594444918\n",
      "Gradient Descent(530/4999): loss=0.5854574907880087\n",
      "Gradient Descent(531/4999): loss=0.5854318255539526\n",
      "Gradient Descent(532/4999): loss=0.585406263271266\n",
      "Gradient Descent(533/4999): loss=0.5853808034717453\n",
      "Gradient Descent(534/4999): loss=0.585355445690013\n",
      "Gradient Descent(535/4999): loss=0.5853301894634905\n",
      "Gradient Descent(536/4999): loss=0.5853050343323719\n",
      "Gradient Descent(537/4999): loss=0.5852799798395968\n",
      "Gradient Descent(538/4999): loss=0.585255025530826\n",
      "Gradient Descent(539/4999): loss=0.5852301709544134\n",
      "Gradient Descent(540/4999): loss=0.5852054156613841\n",
      "Gradient Descent(541/4999): loss=0.5851807592054064\n",
      "Gradient Descent(542/4999): loss=0.5851562011427689\n",
      "Gradient Descent(543/4999): loss=0.5851317410323567\n",
      "Gradient Descent(544/4999): loss=0.5851073784356255\n",
      "Gradient Descent(545/4999): loss=0.5850831129165804\n",
      "Gradient Descent(546/4999): loss=0.5850589440417516\n",
      "Gradient Descent(547/4999): loss=0.5850348713801703\n",
      "Gradient Descent(548/4999): loss=0.5850108945033485\n",
      "Gradient Descent(549/4999): loss=0.5849870129852549\n",
      "Gradient Descent(550/4999): loss=0.5849632264022935\n",
      "Gradient Descent(551/4999): loss=0.5849395343332814\n",
      "Gradient Descent(552/4999): loss=0.584915936359428\n",
      "Gradient Descent(553/4999): loss=0.5848924320643135\n",
      "Gradient Descent(554/4999): loss=0.5848690210338677\n",
      "Gradient Descent(555/4999): loss=0.5848457028563505\n",
      "Gradient Descent(556/4999): loss=0.5848224771223298\n",
      "Gradient Descent(557/4999): loss=0.5847993434246631\n",
      "Gradient Descent(558/4999): loss=0.5847763013584765\n",
      "Gradient Descent(559/4999): loss=0.5847533505211457\n",
      "Gradient Descent(560/4999): loss=0.5847304905122769\n",
      "Gradient Descent(561/4999): loss=0.5847077209336868\n",
      "Gradient Descent(562/4999): loss=0.5846850413893852\n",
      "Gradient Descent(563/4999): loss=0.584662451485555\n",
      "Gradient Descent(564/4999): loss=0.5846399508305344\n",
      "Gradient Descent(565/4999): loss=0.5846175390347996\n",
      "Gradient Descent(566/4999): loss=0.5845952157109456\n",
      "Gradient Descent(567/4999): loss=0.584572980473669\n",
      "Gradient Descent(568/4999): loss=0.5845508329397514\n",
      "Gradient Descent(569/4999): loss=0.5845287727280412\n",
      "Gradient Descent(570/4999): loss=0.5845067994594365\n",
      "Gradient Descent(571/4999): loss=0.5844849127568693\n",
      "Gradient Descent(572/4999): loss=0.5844631122452881\n",
      "Gradient Descent(573/4999): loss=0.584441397551642\n",
      "Gradient Descent(574/4999): loss=0.584419768304864\n",
      "Gradient Descent(575/4999): loss=0.5843982241358561\n",
      "Gradient Descent(576/4999): loss=0.5843767646774716\n",
      "Gradient Descent(577/4999): loss=0.5843553895645021\n",
      "Gradient Descent(578/4999): loss=0.58433409843366\n",
      "Gradient Descent(579/4999): loss=0.584312890923564\n",
      "Gradient Descent(580/4999): loss=0.5842917666747246\n",
      "Gradient Descent(581/4999): loss=0.5842707253295287\n",
      "Gradient Descent(582/4999): loss=0.5842497665322249\n",
      "Gradient Descent(583/4999): loss=0.5842288899289094\n",
      "Gradient Descent(584/4999): loss=0.5842080951675114\n",
      "Gradient Descent(585/4999): loss=0.5841873818977794\n",
      "Gradient Descent(586/4999): loss=0.5841667497712664\n",
      "Gradient Descent(587/4999): loss=0.5841461984413169\n",
      "Gradient Descent(588/4999): loss=0.5841257275630537\n",
      "Gradient Descent(589/4999): loss=0.5841053367933626\n",
      "Gradient Descent(590/4999): loss=0.584085025790881\n",
      "Gradient Descent(591/4999): loss=0.5840647942159837\n",
      "Gradient Descent(592/4999): loss=0.5840446417307704\n",
      "Gradient Descent(593/4999): loss=0.5840245679990524\n",
      "Gradient Descent(594/4999): loss=0.5840045726863405\n",
      "Gradient Descent(595/4999): loss=0.5839846554598319\n",
      "Gradient Descent(596/4999): loss=0.5839648159883978\n",
      "Gradient Descent(597/4999): loss=0.5839450539425717\n",
      "Gradient Descent(598/4999): loss=0.5839253689945368\n",
      "Gradient Descent(599/4999): loss=0.5839057608181139\n",
      "Gradient Descent(600/4999): loss=0.5838862290887501\n",
      "Gradient Descent(601/4999): loss=0.5838667734835067\n",
      "Gradient Descent(602/4999): loss=0.5838473936810473\n",
      "Gradient Descent(603/4999): loss=0.5838280893616273\n",
      "Gradient Descent(604/4999): loss=0.5838088602070817\n",
      "Gradient Descent(605/4999): loss=0.5837897059008144\n",
      "Gradient Descent(606/4999): loss=0.5837706261277864\n",
      "Gradient Descent(607/4999): loss=0.5837516205745062\n",
      "Gradient Descent(608/4999): loss=0.5837326889290179\n",
      "Gradient Descent(609/4999): loss=0.5837138308808902\n",
      "Gradient Descent(610/4999): loss=0.583695046121207\n",
      "Gradient Descent(611/4999): loss=0.5836763343425563\n",
      "Gradient Descent(612/4999): loss=0.5836576952390192\n",
      "Gradient Descent(613/4999): loss=0.5836391285061613\n",
      "Gradient Descent(614/4999): loss=0.5836206338410204\n",
      "Gradient Descent(615/4999): loss=0.5836022109420985\n",
      "Gradient Descent(616/4999): loss=0.5835838595093505\n",
      "Gradient Descent(617/4999): loss=0.5835655792441753\n",
      "Gradient Descent(618/4999): loss=0.5835473698494053\n",
      "Gradient Descent(619/4999): loss=0.5835292310292972\n",
      "Gradient Descent(620/4999): loss=0.5835111624895226\n",
      "Gradient Descent(621/4999): loss=0.5834931639371586\n",
      "Gradient Descent(622/4999): loss=0.583475235080678\n",
      "Gradient Descent(623/4999): loss=0.5834573756299407\n",
      "Gradient Descent(624/4999): loss=0.5834395852961841\n",
      "Gradient Descent(625/4999): loss=0.5834218637920144\n",
      "Gradient Descent(626/4999): loss=0.5834042108313978\n",
      "Gradient Descent(627/4999): loss=0.5833866261296511\n",
      "Gradient Descent(628/4999): loss=0.5833691094034337\n",
      "Gradient Descent(629/4999): loss=0.5833516603707377\n",
      "Gradient Descent(630/4999): loss=0.5833342787508815\n",
      "Gradient Descent(631/4999): loss=0.5833169642644989\n",
      "Gradient Descent(632/4999): loss=0.5832997166335322\n",
      "Gradient Descent(633/4999): loss=0.5832825355812236\n",
      "Gradient Descent(634/4999): loss=0.5832654208321068\n",
      "Gradient Descent(635/4999): loss=0.5832483721119984\n",
      "Gradient Descent(636/4999): loss=0.5832313891479913\n",
      "Gradient Descent(637/4999): loss=0.5832144716684454\n",
      "Gradient Descent(638/4999): loss=0.5831976194029797\n",
      "Gradient Descent(639/4999): loss=0.5831808320824651\n",
      "Gradient Descent(640/4999): loss=0.5831641094390166\n",
      "Gradient Descent(641/4999): loss=0.5831474512059853\n",
      "Gradient Descent(642/4999): loss=0.5831308571179508\n",
      "Gradient Descent(643/4999): loss=0.5831143269107137\n",
      "Gradient Descent(644/4999): loss=0.5830978603212886\n",
      "Gradient Descent(645/4999): loss=0.583081457087896\n",
      "Gradient Descent(646/4999): loss=0.5830651169499557\n",
      "Gradient Descent(647/4999): loss=0.5830488396480784\n",
      "Gradient Descent(648/4999): loss=0.5830326249240605\n",
      "Gradient Descent(649/4999): loss=0.5830164725208749\n",
      "Gradient Descent(650/4999): loss=0.583000382182665\n",
      "Gradient Descent(651/4999): loss=0.582984353654738\n",
      "Gradient Descent(652/4999): loss=0.5829683866835573\n",
      "Gradient Descent(653/4999): loss=0.5829524810167356\n",
      "Gradient Descent(654/4999): loss=0.5829366364030293\n",
      "Gradient Descent(655/4999): loss=0.58292085259233\n",
      "Gradient Descent(656/4999): loss=0.5829051293356594\n",
      "Gradient Descent(657/4999): loss=0.5828894663851618\n",
      "Gradient Descent(658/4999): loss=0.582873863494098\n",
      "Gradient Descent(659/4999): loss=0.5828583204168383\n",
      "Gradient Descent(660/4999): loss=0.582842836908857\n",
      "Gradient Descent(661/4999): loss=0.5828274127267247\n",
      "Gradient Descent(662/4999): loss=0.5828120476281035\n",
      "Gradient Descent(663/4999): loss=0.5827967413717394\n",
      "Gradient Descent(664/4999): loss=0.5827814937174574\n",
      "Gradient Descent(665/4999): loss=0.582766304426154\n",
      "Gradient Descent(666/4999): loss=0.5827511732597921\n",
      "Gradient Descent(667/4999): loss=0.5827360999813945\n",
      "Gradient Descent(668/4999): loss=0.5827210843550383\n",
      "Gradient Descent(669/4999): loss=0.5827061261458486\n",
      "Gradient Descent(670/4999): loss=0.5826912251199926\n",
      "Gradient Descent(671/4999): loss=0.5826763810446742\n",
      "Gradient Descent(672/4999): loss=0.5826615936881278\n",
      "Gradient Descent(673/4999): loss=0.5826468628196126\n",
      "Gradient Descent(674/4999): loss=0.5826321882094074\n",
      "Gradient Descent(675/4999): loss=0.5826175696288045\n",
      "Gradient Descent(676/4999): loss=0.5826030068501038\n",
      "Gradient Descent(677/4999): loss=0.5825884996466082\n",
      "Gradient Descent(678/4999): loss=0.5825740477926175\n",
      "Gradient Descent(679/4999): loss=0.5825596510634227\n",
      "Gradient Descent(680/4999): loss=0.5825453092353012\n",
      "Gradient Descent(681/4999): loss=0.5825310220855114\n",
      "Gradient Descent(682/4999): loss=0.5825167893922872\n",
      "Gradient Descent(683/4999): loss=0.5825026109348319\n",
      "Gradient Descent(684/4999): loss=0.5824884864933147\n",
      "Gradient Descent(685/4999): loss=0.5824744158488643\n",
      "Gradient Descent(686/4999): loss=0.5824603987835639\n",
      "Gradient Descent(687/4999): loss=0.5824464350804465\n",
      "Gradient Descent(688/4999): loss=0.5824325245234895\n",
      "Gradient Descent(689/4999): loss=0.5824186668976095\n",
      "Gradient Descent(690/4999): loss=0.5824048619886582\n",
      "Gradient Descent(691/4999): loss=0.5823911095834163\n",
      "Gradient Descent(692/4999): loss=0.5823774094695895\n",
      "Gradient Descent(693/4999): loss=0.582363761435803\n",
      "Gradient Descent(694/4999): loss=0.5823501652715974\n",
      "Gradient Descent(695/4999): loss=0.5823366207674232\n",
      "Gradient Descent(696/4999): loss=0.5823231277146361\n",
      "Gradient Descent(697/4999): loss=0.582309685905493\n",
      "Gradient Descent(698/4999): loss=0.5822962951331465\n",
      "Gradient Descent(699/4999): loss=0.582282955191641\n",
      "Gradient Descent(700/4999): loss=0.5822696658759069\n",
      "Gradient Descent(701/4999): loss=0.5822564269817577\n",
      "Gradient Descent(702/4999): loss=0.5822432383058842\n",
      "Gradient Descent(703/4999): loss=0.5822300996458504\n",
      "Gradient Descent(704/4999): loss=0.5822170108000889\n",
      "Gradient Descent(705/4999): loss=0.5822039715678969\n",
      "Gradient Descent(706/4999): loss=0.582190981749431\n",
      "Gradient Descent(707/4999): loss=0.5821780411457037\n",
      "Gradient Descent(708/4999): loss=0.5821651495585787\n",
      "Gradient Descent(709/4999): loss=0.5821523067907661\n",
      "Gradient Descent(710/4999): loss=0.5821395126458192\n",
      "Gradient Descent(711/4999): loss=0.5821267669281291\n",
      "Gradient Descent(712/4999): loss=0.5821140694429215\n",
      "Gradient Descent(713/4999): loss=0.5821014199962518\n",
      "Gradient Descent(714/4999): loss=0.5820888183950009\n",
      "Gradient Descent(715/4999): loss=0.5820762644468722\n",
      "Gradient Descent(716/4999): loss=0.5820637579603857\n",
      "Gradient Descent(717/4999): loss=0.5820512987448756\n",
      "Gradient Descent(718/4999): loss=0.5820388866104856\n",
      "Gradient Descent(719/4999): loss=0.5820265213681642\n",
      "Gradient Descent(720/4999): loss=0.5820142028296623\n",
      "Gradient Descent(721/4999): loss=0.5820019308075277\n",
      "Gradient Descent(722/4999): loss=0.581989705115102\n",
      "Gradient Descent(723/4999): loss=0.5819775255665167\n",
      "Gradient Descent(724/4999): loss=0.5819653919766891\n",
      "Gradient Descent(725/4999): loss=0.5819533041613181\n",
      "Gradient Descent(726/4999): loss=0.5819412619368813\n",
      "Gradient Descent(727/4999): loss=0.5819292651206307\n",
      "Gradient Descent(728/4999): loss=0.581917313530589\n",
      "Gradient Descent(729/4999): loss=0.5819054069855449\n",
      "Gradient Descent(730/4999): loss=0.5818935453050519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(731/4999): loss=0.5818817283094216\n",
      "Gradient Descent(732/4999): loss=0.5818699558197223\n",
      "Gradient Descent(733/4999): loss=0.581858227657774\n",
      "Gradient Descent(734/4999): loss=0.5818465436461455\n",
      "Gradient Descent(735/4999): loss=0.5818349036081507\n",
      "Gradient Descent(736/4999): loss=0.5818233073678447\n",
      "Gradient Descent(737/4999): loss=0.581811754750021\n",
      "Gradient Descent(738/4999): loss=0.5818002455802066\n",
      "Gradient Descent(739/4999): loss=0.5817887796846603\n",
      "Gradient Descent(740/4999): loss=0.5817773568903676\n",
      "Gradient Descent(741/4999): loss=0.5817659770250384\n",
      "Gradient Descent(742/4999): loss=0.581754639917103\n",
      "Gradient Descent(743/4999): loss=0.5817433453957088\n",
      "Gradient Descent(744/4999): loss=0.5817320932907167\n",
      "Gradient Descent(745/4999): loss=0.5817208834326985\n",
      "Gradient Descent(746/4999): loss=0.5817097156529323\n",
      "Gradient Descent(747/4999): loss=0.5816985897834007\n",
      "Gradient Descent(748/4999): loss=0.5816875056567858\n",
      "Gradient Descent(749/4999): loss=0.581676463106468\n",
      "Gradient Descent(750/4999): loss=0.5816654619665205\n",
      "Gradient Descent(751/4999): loss=0.5816545020717073\n",
      "Gradient Descent(752/4999): loss=0.5816435832574804\n",
      "Gradient Descent(753/4999): loss=0.5816327053599757\n",
      "Gradient Descent(754/4999): loss=0.5816218682160096\n",
      "Gradient Descent(755/4999): loss=0.5816110716630772\n",
      "Gradient Descent(756/4999): loss=0.5816003155393479\n",
      "Gradient Descent(757/4999): loss=0.5815895996836626\n",
      "Gradient Descent(758/4999): loss=0.5815789239355312\n",
      "Gradient Descent(759/4999): loss=0.5815682881351284\n",
      "Gradient Descent(760/4999): loss=0.5815576921232917\n",
      "Gradient Descent(761/4999): loss=0.581547135741518\n",
      "Gradient Descent(762/4999): loss=0.5815366188319598\n",
      "Gradient Descent(763/4999): loss=0.581526141237424\n",
      "Gradient Descent(764/4999): loss=0.5815157028013666\n",
      "Gradient Descent(765/4999): loss=0.5815053033678919\n",
      "Gradient Descent(766/4999): loss=0.581494942781748\n",
      "Gradient Descent(767/4999): loss=0.5814846208883245\n",
      "Gradient Descent(768/4999): loss=0.58147433753365\n",
      "Gradient Descent(769/4999): loss=0.5814640925643876\n",
      "Gradient Descent(770/4999): loss=0.5814538858278344\n",
      "Gradient Descent(771/4999): loss=0.5814437171719168\n",
      "Gradient Descent(772/4999): loss=0.5814335864451878\n",
      "Gradient Descent(773/4999): loss=0.5814234934968253\n",
      "Gradient Descent(774/4999): loss=0.5814134381766279\n",
      "Gradient Descent(775/4999): loss=0.5814034203350132\n",
      "Gradient Descent(776/4999): loss=0.5813934398230145\n",
      "Gradient Descent(777/4999): loss=0.5813834964922777\n",
      "Gradient Descent(778/4999): loss=0.5813735901950594\n",
      "Gradient Descent(779/4999): loss=0.5813637207842234\n",
      "Gradient Descent(780/4999): loss=0.5813538881132385\n",
      "Gradient Descent(781/4999): loss=0.5813440920361752\n",
      "Gradient Descent(782/4999): loss=0.5813343324077038\n",
      "Gradient Descent(783/4999): loss=0.5813246090830909\n",
      "Gradient Descent(784/4999): loss=0.5813149219181974\n",
      "Gradient Descent(785/4999): loss=0.5813052707694755\n",
      "Gradient Descent(786/4999): loss=0.5812956554939661\n",
      "Gradient Descent(787/4999): loss=0.5812860759492962\n",
      "Gradient Descent(788/4999): loss=0.5812765319936765\n",
      "Gradient Descent(789/4999): loss=0.5812670234858986\n",
      "Gradient Descent(790/4999): loss=0.5812575502853321\n",
      "Gradient Descent(791/4999): loss=0.5812481122519229\n",
      "Gradient Descent(792/4999): loss=0.5812387092461898\n",
      "Gradient Descent(793/4999): loss=0.5812293411292224\n",
      "Gradient Descent(794/4999): loss=0.5812200077626789\n",
      "Gradient Descent(795/4999): loss=0.5812107090087826\n",
      "Gradient Descent(796/4999): loss=0.5812014447303202\n",
      "Gradient Descent(797/4999): loss=0.5811922147906393\n",
      "Gradient Descent(798/4999): loss=0.5811830190536459\n",
      "Gradient Descent(799/4999): loss=0.5811738573838009\n",
      "Gradient Descent(800/4999): loss=0.58116472964612\n",
      "Gradient Descent(801/4999): loss=0.5811556357061685\n",
      "Gradient Descent(802/4999): loss=0.5811465754300613\n",
      "Gradient Descent(803/4999): loss=0.5811375486844587\n",
      "Gradient Descent(804/4999): loss=0.581128555336565\n",
      "Gradient Descent(805/4999): loss=0.5811195952541262\n",
      "Gradient Descent(806/4999): loss=0.5811106683054265\n",
      "Gradient Descent(807/4999): loss=0.5811017743592877\n",
      "Gradient Descent(808/4999): loss=0.5810929132850656\n",
      "Gradient Descent(809/4999): loss=0.5810840849526476\n",
      "Gradient Descent(810/4999): loss=0.5810752892324513\n",
      "Gradient Descent(811/4999): loss=0.5810665259954217\n",
      "Gradient Descent(812/4999): loss=0.5810577951130285\n",
      "Gradient Descent(813/4999): loss=0.5810490964572643\n",
      "Gradient Descent(814/4999): loss=0.5810404299006431\n",
      "Gradient Descent(815/4999): loss=0.581031795316196\n",
      "Gradient Descent(816/4999): loss=0.5810231925774708\n",
      "Gradient Descent(817/4999): loss=0.5810146215585293\n",
      "Gradient Descent(818/4999): loss=0.5810060821339447\n",
      "Gradient Descent(819/4999): loss=0.5809975741787996\n",
      "Gradient Descent(820/4999): loss=0.5809890975686837\n",
      "Gradient Descent(821/4999): loss=0.5809806521796922\n",
      "Gradient Descent(822/4999): loss=0.5809722378884228\n",
      "Gradient Descent(823/4999): loss=0.580963854571974\n",
      "Gradient Descent(824/4999): loss=0.5809555021079429\n",
      "Gradient Descent(825/4999): loss=0.580947180374423\n",
      "Gradient Descent(826/4999): loss=0.580938889250002\n",
      "Gradient Descent(827/4999): loss=0.5809306286137602\n",
      "Gradient Descent(828/4999): loss=0.5809223983452673\n",
      "Gradient Descent(829/4999): loss=0.5809141983245814\n",
      "Gradient Descent(830/4999): loss=0.5809060284322463\n",
      "Gradient Descent(831/4999): loss=0.58089788854929\n",
      "Gradient Descent(832/4999): loss=0.5808897785572218\n",
      "Gradient Descent(833/4999): loss=0.5808816983380308\n",
      "Gradient Descent(834/4999): loss=0.5808736477741837\n",
      "Gradient Descent(835/4999): loss=0.5808656267486231\n",
      "Gradient Descent(836/4999): loss=0.5808576351447649\n",
      "Gradient Descent(837/4999): loss=0.5808496728464965\n",
      "Gradient Descent(838/4999): loss=0.5808417397381752\n",
      "Gradient Descent(839/4999): loss=0.5808338357046257\n",
      "Gradient Descent(840/4999): loss=0.5808259606311381\n",
      "Gradient Descent(841/4999): loss=0.5808181144034663\n",
      "Gradient Descent(842/4999): loss=0.5808102969078258\n",
      "Gradient Descent(843/4999): loss=0.5808025080308921\n",
      "Gradient Descent(844/4999): loss=0.5807947476597976\n",
      "Gradient Descent(845/4999): loss=0.5807870156821314\n",
      "Gradient Descent(846/4999): loss=0.5807793119859359\n",
      "Gradient Descent(847/4999): loss=0.580771636459706\n",
      "Gradient Descent(848/4999): loss=0.5807639889923859\n",
      "Gradient Descent(849/4999): loss=0.5807563694733686\n",
      "Gradient Descent(850/4999): loss=0.580748777792493\n",
      "Gradient Descent(851/4999): loss=0.5807412138400425\n",
      "Gradient Descent(852/4999): loss=0.5807336775067432\n",
      "Gradient Descent(853/4999): loss=0.5807261686837614\n",
      "Gradient Descent(854/4999): loss=0.5807186872627021\n",
      "Gradient Descent(855/4999): loss=0.5807112331356082\n",
      "Gradient Descent(856/4999): loss=0.5807038061949567\n",
      "Gradient Descent(857/4999): loss=0.5806964063336578\n",
      "Gradient Descent(858/4999): loss=0.5806890334450542\n",
      "Gradient Descent(859/4999): loss=0.5806816874229173\n",
      "Gradient Descent(860/4999): loss=0.5806743681614465\n",
      "Gradient Descent(861/4999): loss=0.5806670755552674\n",
      "Gradient Descent(862/4999): loss=0.5806598094994302\n",
      "Gradient Descent(863/4999): loss=0.5806525698894067\n",
      "Gradient Descent(864/4999): loss=0.58064535662109\n",
      "Gradient Descent(865/4999): loss=0.5806381695907922\n",
      "Gradient Descent(866/4999): loss=0.5806310086952423\n",
      "Gradient Descent(867/4999): loss=0.580623873831585\n",
      "Gradient Descent(868/4999): loss=0.5806167648973786\n",
      "Gradient Descent(869/4999): loss=0.5806096817905931\n",
      "Gradient Descent(870/4999): loss=0.5806026244096095\n",
      "Gradient Descent(871/4999): loss=0.5805955926532169\n",
      "Gradient Descent(872/4999): loss=0.5805885864206107\n",
      "Gradient Descent(873/4999): loss=0.5805816056113928\n",
      "Gradient Descent(874/4999): loss=0.5805746501255674\n",
      "Gradient Descent(875/4999): loss=0.5805677198635407\n",
      "Gradient Descent(876/4999): loss=0.5805608147261195\n",
      "Gradient Descent(877/4999): loss=0.5805539346145087\n",
      "Gradient Descent(878/4999): loss=0.58054707943031\n",
      "Gradient Descent(879/4999): loss=0.5805402490755203\n",
      "Gradient Descent(880/4999): loss=0.5805334434525298\n",
      "Gradient Descent(881/4999): loss=0.580526662464121\n",
      "Gradient Descent(882/4999): loss=0.5805199060134663\n",
      "Gradient Descent(883/4999): loss=0.5805131740041268\n",
      "Gradient Descent(884/4999): loss=0.5805064663400507\n",
      "Gradient Descent(885/4999): loss=0.5804997829255713\n",
      "Gradient Descent(886/4999): loss=0.5804931236654062\n",
      "Gradient Descent(887/4999): loss=0.5804864884646547\n",
      "Gradient Descent(888/4999): loss=0.5804798772287971\n",
      "Gradient Descent(889/4999): loss=0.5804732898636928\n",
      "Gradient Descent(890/4999): loss=0.5804667262755783\n",
      "Gradient Descent(891/4999): loss=0.580460186371066\n",
      "Gradient Descent(892/4999): loss=0.5804536700571435\n",
      "Gradient Descent(893/4999): loss=0.5804471772411703\n",
      "Gradient Descent(894/4999): loss=0.5804407078308776\n",
      "Gradient Descent(895/4999): loss=0.5804342617343664\n",
      "Gradient Descent(896/4999): loss=0.5804278388601055\n",
      "Gradient Descent(897/4999): loss=0.5804214391169308\n",
      "Gradient Descent(898/4999): loss=0.5804150624140435\n",
      "Gradient Descent(899/4999): loss=0.5804087086610082\n",
      "Gradient Descent(900/4999): loss=0.5804023777677518\n",
      "Gradient Descent(901/4999): loss=0.5803960696445618\n",
      "Gradient Descent(902/4999): loss=0.580389784202085\n",
      "Gradient Descent(903/4999): loss=0.5803835213513259\n",
      "Gradient Descent(904/4999): loss=0.5803772810036452\n",
      "Gradient Descent(905/4999): loss=0.5803710630707584\n",
      "Gradient Descent(906/4999): loss=0.5803648674647344\n",
      "Gradient Descent(907/4999): loss=0.5803586940979942\n",
      "Gradient Descent(908/4999): loss=0.5803525428833087\n",
      "Gradient Descent(909/4999): loss=0.5803464137337984\n",
      "Gradient Descent(910/4999): loss=0.5803403065629306\n",
      "Gradient Descent(911/4999): loss=0.5803342212845193\n",
      "Gradient Descent(912/4999): loss=0.5803281578127231\n",
      "Gradient Descent(913/4999): loss=0.5803221160620439\n",
      "Gradient Descent(914/4999): loss=0.5803160959473254\n",
      "Gradient Descent(915/4999): loss=0.5803100973837516\n",
      "Gradient Descent(916/4999): loss=0.5803041202868463\n",
      "Gradient Descent(917/4999): loss=0.58029816457247\n",
      "Gradient Descent(918/4999): loss=0.5802922301568202\n",
      "Gradient Descent(919/4999): loss=0.5802863169564291\n",
      "Gradient Descent(920/4999): loss=0.5802804248881624\n",
      "Gradient Descent(921/4999): loss=0.5802745538692183\n",
      "Gradient Descent(922/4999): loss=0.5802687038171254\n",
      "Gradient Descent(923/4999): loss=0.580262874649742\n",
      "Gradient Descent(924/4999): loss=0.5802570662852548\n",
      "Gradient Descent(925/4999): loss=0.5802512786421766\n",
      "Gradient Descent(926/4999): loss=0.5802455116393465\n",
      "Gradient Descent(927/4999): loss=0.5802397651959268\n",
      "Gradient Descent(928/4999): loss=0.5802340392314034\n",
      "Gradient Descent(929/4999): loss=0.5802283336655832\n",
      "Gradient Descent(930/4999): loss=0.5802226484185936\n",
      "Gradient Descent(931/4999): loss=0.5802169834108808\n",
      "Gradient Descent(932/4999): loss=0.580211338563208\n",
      "Gradient Descent(933/4999): loss=0.5802057137966554\n",
      "Gradient Descent(934/4999): loss=0.5802001090326177\n",
      "Gradient Descent(935/4999): loss=0.5801945241928035\n",
      "Gradient Descent(936/4999): loss=0.580188959199234\n",
      "Gradient Descent(937/4999): loss=0.5801834139742409\n",
      "Gradient Descent(938/4999): loss=0.5801778884404664\n",
      "Gradient Descent(939/4999): loss=0.5801723825208609\n",
      "Gradient Descent(940/4999): loss=0.5801668961386823\n",
      "Gradient Descent(941/4999): loss=0.5801614292174948\n",
      "Gradient Descent(942/4999): loss=0.5801559816811669\n",
      "Gradient Descent(943/4999): loss=0.580150553453871\n",
      "Gradient Descent(944/4999): loss=0.5801451444600823\n",
      "Gradient Descent(945/4999): loss=0.5801397546245762\n",
      "Gradient Descent(946/4999): loss=0.580134383872429\n",
      "Gradient Descent(947/4999): loss=0.5801290321290148\n",
      "Gradient Descent(948/4999): loss=0.5801236993200058\n",
      "Gradient Descent(949/4999): loss=0.5801183853713703\n",
      "Gradient Descent(950/4999): loss=0.5801130902093714\n",
      "Gradient Descent(951/4999): loss=0.5801078137605664\n",
      "Gradient Descent(952/4999): loss=0.5801025559518053\n",
      "Gradient Descent(953/4999): loss=0.5800973167102291\n",
      "Gradient Descent(954/4999): loss=0.5800920959632696\n",
      "Gradient Descent(955/4999): loss=0.5800868936386474\n",
      "Gradient Descent(956/4999): loss=0.5800817096643713\n",
      "Gradient Descent(957/4999): loss=0.5800765439687369\n",
      "Gradient Descent(958/4999): loss=0.5800713964803248\n",
      "Gradient Descent(959/4999): loss=0.5800662671280007\n",
      "Gradient Descent(960/4999): loss=0.5800611558409136\n",
      "Gradient Descent(961/4999): loss=0.580056062548494\n",
      "Gradient Descent(962/4999): loss=0.5800509871804541\n",
      "Gradient Descent(963/4999): loss=0.5800459296667857\n",
      "Gradient Descent(964/4999): loss=0.5800408899377594\n",
      "Gradient Descent(965/4999): loss=0.5800358679239229\n",
      "Gradient Descent(966/4999): loss=0.5800308635561015\n",
      "Gradient Descent(967/4999): loss=0.5800258767653943\n",
      "Gradient Descent(968/4999): loss=0.5800209074831761\n",
      "Gradient Descent(969/4999): loss=0.5800159556410939\n",
      "Gradient Descent(970/4999): loss=0.5800110211710671\n",
      "Gradient Descent(971/4999): loss=0.5800061040052861\n",
      "Gradient Descent(972/4999): loss=0.5800012040762107\n",
      "Gradient Descent(973/4999): loss=0.5799963213165699\n",
      "Gradient Descent(974/4999): loss=0.5799914556593598\n",
      "Gradient Descent(975/4999): loss=0.5799866070378439\n",
      "Gradient Descent(976/4999): loss=0.5799817753855502\n",
      "Gradient Descent(977/4999): loss=0.5799769606362718\n",
      "Gradient Descent(978/4999): loss=0.5799721627240648\n",
      "Gradient Descent(979/4999): loss=0.579967381583248\n",
      "Gradient Descent(980/4999): loss=0.5799626171484008\n",
      "Gradient Descent(981/4999): loss=0.5799578693543631\n",
      "Gradient Descent(982/4999): loss=0.5799531381362337\n",
      "Gradient Descent(983/4999): loss=0.5799484234293698\n",
      "Gradient Descent(984/4999): loss=0.5799437251693857\n",
      "Gradient Descent(985/4999): loss=0.5799390432921508\n",
      "Gradient Descent(986/4999): loss=0.5799343777337906\n",
      "Gradient Descent(987/4999): loss=0.5799297284306838\n",
      "Gradient Descent(988/4999): loss=0.579925095319462\n",
      "Gradient Descent(989/4999): loss=0.5799204783370089\n",
      "Gradient Descent(990/4999): loss=0.5799158774204591\n",
      "Gradient Descent(991/4999): loss=0.5799112925071968\n",
      "Gradient Descent(992/4999): loss=0.5799067235348551\n",
      "Gradient Descent(993/4999): loss=0.579902170441315\n",
      "Gradient Descent(994/4999): loss=0.5798976331647042\n",
      "Gradient Descent(995/4999): loss=0.5798931116433964\n",
      "Gradient Descent(996/4999): loss=0.5798886058160103\n",
      "Gradient Descent(997/4999): loss=0.579884115621408\n",
      "Gradient Descent(998/4999): loss=0.5798796409986946\n",
      "Gradient Descent(999/4999): loss=0.5798751818872172\n",
      "Gradient Descent(1000/4999): loss=0.5798707382265639\n",
      "Gradient Descent(1001/4999): loss=0.5798663099565626\n",
      "Gradient Descent(1002/4999): loss=0.5798618970172801\n",
      "Gradient Descent(1003/4999): loss=0.5798574993490214\n",
      "Gradient Descent(1004/4999): loss=0.5798531168923285\n",
      "Gradient Descent(1005/4999): loss=0.5798487495879795\n",
      "Gradient Descent(1006/4999): loss=0.5798443973769876\n",
      "Gradient Descent(1007/4999): loss=0.5798400602006001\n",
      "Gradient Descent(1008/4999): loss=0.5798357380002979\n",
      "Gradient Descent(1009/4999): loss=0.5798314307177937\n",
      "Gradient Descent(1010/4999): loss=0.5798271382950322\n",
      "Gradient Descent(1011/4999): loss=0.5798228606741882\n",
      "Gradient Descent(1012/4999): loss=0.5798185977976659\n",
      "Gradient Descent(1013/4999): loss=0.5798143496080984\n",
      "Gradient Descent(1014/4999): loss=0.5798101160483464\n",
      "Gradient Descent(1015/4999): loss=0.5798058970614971\n",
      "Gradient Descent(1016/4999): loss=0.5798016925908639\n",
      "Gradient Descent(1017/4999): loss=0.5797975025799851\n",
      "Gradient Descent(1018/4999): loss=0.579793326972623\n",
      "Gradient Descent(1019/4999): loss=0.5797891657127627\n",
      "Gradient Descent(1020/4999): loss=0.5797850187446121\n",
      "Gradient Descent(1021/4999): loss=0.5797808860126001\n",
      "Gradient Descent(1022/4999): loss=0.5797767674613763\n",
      "Gradient Descent(1023/4999): loss=0.5797726630358095\n",
      "Gradient Descent(1024/4999): loss=0.5797685726809877\n",
      "Gradient Descent(1025/4999): loss=0.579764496342216\n",
      "Gradient Descent(1026/4999): loss=0.5797604339650173\n",
      "Gradient Descent(1027/4999): loss=0.5797563854951299\n",
      "Gradient Descent(1028/4999): loss=0.5797523508785075\n",
      "Gradient Descent(1029/4999): loss=0.5797483300613183\n",
      "Gradient Descent(1030/4999): loss=0.5797443229899437\n",
      "Gradient Descent(1031/4999): loss=0.579740329610978\n",
      "Gradient Descent(1032/4999): loss=0.579736349871227\n",
      "Gradient Descent(1033/4999): loss=0.5797323837177075\n",
      "Gradient Descent(1034/4999): loss=0.5797284310976465\n",
      "Gradient Descent(1035/4999): loss=0.57972449195848\n",
      "Gradient Descent(1036/4999): loss=0.5797205662478524\n",
      "Gradient Descent(1037/4999): loss=0.5797166539136158\n",
      "Gradient Descent(1038/4999): loss=0.5797127549038289\n",
      "Gradient Descent(1039/4999): loss=0.5797088691667567\n",
      "Gradient Descent(1040/4999): loss=0.5797049966508684\n",
      "Gradient Descent(1041/4999): loss=0.5797011373048381\n",
      "Gradient Descent(1042/4999): loss=0.5796972910775429\n",
      "Gradient Descent(1043/4999): loss=0.5796934579180631\n",
      "Gradient Descent(1044/4999): loss=0.5796896377756801\n",
      "Gradient Descent(1045/4999): loss=0.5796858305998767\n",
      "Gradient Descent(1046/4999): loss=0.5796820363403357\n",
      "Gradient Descent(1047/4999): loss=0.5796782549469394\n",
      "Gradient Descent(1048/4999): loss=0.5796744863697686\n",
      "Gradient Descent(1049/4999): loss=0.5796707305591016\n",
      "Gradient Descent(1050/4999): loss=0.5796669874654143\n",
      "Gradient Descent(1051/4999): loss=0.5796632570393783\n",
      "Gradient Descent(1052/4999): loss=0.5796595392318604\n",
      "Gradient Descent(1053/4999): loss=0.5796558339939228\n",
      "Gradient Descent(1054/4999): loss=0.579652141276821\n",
      "Gradient Descent(1055/4999): loss=0.5796484610320034\n",
      "Gradient Descent(1056/4999): loss=0.5796447932111111\n",
      "Gradient Descent(1057/4999): loss=0.5796411377659766\n",
      "Gradient Descent(1058/4999): loss=0.5796374946486231\n",
      "Gradient Descent(1059/4999): loss=0.5796338638112638\n",
      "Gradient Descent(1060/4999): loss=0.5796302452063009\n",
      "Gradient Descent(1061/4999): loss=0.5796266387863257\n",
      "Gradient Descent(1062/4999): loss=0.5796230445041164\n",
      "Gradient Descent(1063/4999): loss=0.5796194623126388\n",
      "Gradient Descent(1064/4999): loss=0.5796158921650446\n",
      "Gradient Descent(1065/4999): loss=0.5796123340146712\n",
      "Gradient Descent(1066/4999): loss=0.5796087878150403\n",
      "Gradient Descent(1067/4999): loss=0.5796052535198581\n",
      "Gradient Descent(1068/4999): loss=0.5796017310830139\n",
      "Gradient Descent(1069/4999): loss=0.5795982204585792\n",
      "Gradient Descent(1070/4999): loss=0.5795947216008075\n",
      "Gradient Descent(1071/4999): loss=0.5795912344641335\n",
      "Gradient Descent(1072/4999): loss=0.5795877590031722\n",
      "Gradient Descent(1073/4999): loss=0.5795842951727178\n",
      "Gradient Descent(1074/4999): loss=0.5795808429277441\n",
      "Gradient Descent(1075/4999): loss=0.5795774022234024\n",
      "Gradient Descent(1076/4999): loss=0.5795739730150218\n",
      "Gradient Descent(1077/4999): loss=0.5795705552581083\n",
      "Gradient Descent(1078/4999): loss=0.5795671489083436\n",
      "Gradient Descent(1079/4999): loss=0.5795637539215849\n",
      "Gradient Descent(1080/4999): loss=0.5795603702538641\n",
      "Gradient Descent(1081/4999): loss=0.5795569978613869\n",
      "Gradient Descent(1082/4999): loss=0.5795536367005327\n",
      "Gradient Descent(1083/4999): loss=0.5795502867278526\n",
      "Gradient Descent(1084/4999): loss=0.5795469479000703\n",
      "Gradient Descent(1085/4999): loss=0.5795436201740805\n",
      "Gradient Descent(1086/4999): loss=0.5795403035069482\n",
      "Gradient Descent(1087/4999): loss=0.5795369978559085\n",
      "Gradient Descent(1088/4999): loss=0.5795337031783654\n",
      "Gradient Descent(1089/4999): loss=0.5795304194318914\n",
      "Gradient Descent(1090/4999): loss=0.5795271465742269\n",
      "Gradient Descent(1091/4999): loss=0.5795238845632794\n",
      "Gradient Descent(1092/4999): loss=0.5795206333571226\n",
      "Gradient Descent(1093/4999): loss=0.5795173929139963\n",
      "Gradient Descent(1094/4999): loss=0.5795141631923052\n",
      "Gradient Descent(1095/4999): loss=0.5795109441506188\n",
      "Gradient Descent(1096/4999): loss=0.5795077357476699\n",
      "Gradient Descent(1097/4999): loss=0.5795045379423548\n",
      "Gradient Descent(1098/4999): loss=0.5795013506937323\n",
      "Gradient Descent(1099/4999): loss=0.5794981739610228\n",
      "Gradient Descent(1100/4999): loss=0.579495007703608\n",
      "Gradient Descent(1101/4999): loss=0.5794918518810304\n",
      "Gradient Descent(1102/4999): loss=0.579488706452992\n",
      "Gradient Descent(1103/4999): loss=0.5794855713793545\n",
      "Gradient Descent(1104/4999): loss=0.5794824466201378\n",
      "Gradient Descent(1105/4999): loss=0.5794793321355205\n",
      "Gradient Descent(1106/4999): loss=0.5794762278858377\n",
      "Gradient Descent(1107/4999): loss=0.5794731338315817\n",
      "Gradient Descent(1108/4999): loss=0.5794700499334012\n",
      "Gradient Descent(1109/4999): loss=0.5794669761520999\n",
      "Gradient Descent(1110/4999): loss=0.5794639124486365\n",
      "Gradient Descent(1111/4999): loss=0.5794608587841241\n",
      "Gradient Descent(1112/4999): loss=0.5794578151198295\n",
      "Gradient Descent(1113/4999): loss=0.5794547814171723\n",
      "Gradient Descent(1114/4999): loss=0.5794517576377248\n",
      "Gradient Descent(1115/4999): loss=0.5794487437432106\n",
      "Gradient Descent(1116/4999): loss=0.5794457396955053\n",
      "Gradient Descent(1117/4999): loss=0.5794427454566341\n",
      "Gradient Descent(1118/4999): loss=0.5794397609887735\n",
      "Gradient Descent(1119/4999): loss=0.5794367862542481\n",
      "Gradient Descent(1120/4999): loss=0.5794338212155321\n",
      "Gradient Descent(1121/4999): loss=0.5794308658352477\n",
      "Gradient Descent(1122/4999): loss=0.5794279200761646\n",
      "Gradient Descent(1123/4999): loss=0.5794249839011998\n",
      "Gradient Descent(1124/4999): loss=0.5794220572734163\n",
      "Gradient Descent(1125/4999): loss=0.5794191401560238\n",
      "Gradient Descent(1126/4999): loss=0.5794162325123764\n",
      "Gradient Descent(1127/4999): loss=0.5794133343059732\n",
      "Gradient Descent(1128/4999): loss=0.5794104455004576\n",
      "Gradient Descent(1129/4999): loss=0.5794075660596165\n",
      "Gradient Descent(1130/4999): loss=0.5794046959473795\n",
      "Gradient Descent(1131/4999): loss=0.5794018351278191\n",
      "Gradient Descent(1132/4999): loss=0.579398983565149\n",
      "Gradient Descent(1133/4999): loss=0.5793961412237248\n",
      "Gradient Descent(1134/4999): loss=0.5793933080680425\n",
      "Gradient Descent(1135/4999): loss=0.5793904840627383\n",
      "Gradient Descent(1136/4999): loss=0.5793876691725879\n",
      "Gradient Descent(1137/4999): loss=0.5793848633625062\n",
      "Gradient Descent(1138/4999): loss=0.5793820665975465\n",
      "Gradient Descent(1139/4999): loss=0.5793792788429002\n",
      "Gradient Descent(1140/4999): loss=0.5793765000638957\n",
      "Gradient Descent(1141/4999): loss=0.5793737302259985\n",
      "Gradient Descent(1142/4999): loss=0.5793709692948104\n",
      "Gradient Descent(1143/4999): loss=0.5793682172360689\n",
      "Gradient Descent(1144/4999): loss=0.5793654740156465\n",
      "Gradient Descent(1145/4999): loss=0.579362739599551\n",
      "Gradient Descent(1146/4999): loss=0.5793600139539234\n",
      "Gradient Descent(1147/4999): loss=0.5793572970450391\n",
      "Gradient Descent(1148/4999): loss=0.579354588839306\n",
      "Gradient Descent(1149/4999): loss=0.579351889303265\n",
      "Gradient Descent(1150/4999): loss=0.5793491984035886\n",
      "Gradient Descent(1151/4999): loss=0.5793465161070811\n",
      "Gradient Descent(1152/4999): loss=0.5793438423806776\n",
      "Gradient Descent(1153/4999): loss=0.5793411771914435\n",
      "Gradient Descent(1154/4999): loss=0.5793385205065744\n",
      "Gradient Descent(1155/4999): loss=0.5793358722933951\n",
      "Gradient Descent(1156/4999): loss=0.5793332325193595\n",
      "Gradient Descent(1157/4999): loss=0.5793306011520495\n",
      "Gradient Descent(1158/4999): loss=0.5793279781591749\n",
      "Gradient Descent(1159/4999): loss=0.5793253635085734\n",
      "Gradient Descent(1160/4999): loss=0.5793227571682089\n",
      "Gradient Descent(1161/4999): loss=0.5793201591061721\n",
      "Gradient Descent(1162/4999): loss=0.579317569290679\n",
      "Gradient Descent(1163/4999): loss=0.5793149876900715\n",
      "Gradient Descent(1164/4999): loss=0.5793124142728159\n",
      "Gradient Descent(1165/4999): loss=0.5793098490075033\n",
      "Gradient Descent(1166/4999): loss=0.579307291862848\n",
      "Gradient Descent(1167/4999): loss=0.5793047428076882\n",
      "Gradient Descent(1168/4999): loss=0.5793022018109846\n",
      "Gradient Descent(1169/4999): loss=0.5792996688418207\n",
      "Gradient Descent(1170/4999): loss=0.5792971438694015\n",
      "Gradient Descent(1171/4999): loss=0.5792946268630532\n",
      "Gradient Descent(1172/4999): loss=0.5792921177922237\n",
      "Gradient Descent(1173/4999): loss=0.5792896166264806\n",
      "Gradient Descent(1174/4999): loss=0.5792871233355116\n",
      "Gradient Descent(1175/4999): loss=0.579284637889124\n",
      "Gradient Descent(1176/4999): loss=0.5792821602572442\n",
      "Gradient Descent(1177/4999): loss=0.5792796904099169\n",
      "Gradient Descent(1178/4999): loss=0.5792772283173051\n",
      "Gradient Descent(1179/4999): loss=0.5792747739496891\n",
      "Gradient Descent(1180/4999): loss=0.5792723272774665\n",
      "Gradient Descent(1181/4999): loss=0.5792698882711512\n",
      "Gradient Descent(1182/4999): loss=0.5792674569013742\n",
      "Gradient Descent(1183/4999): loss=0.5792650331388811\n",
      "Gradient Descent(1184/4999): loss=0.5792626169545334\n",
      "Gradient Descent(1185/4999): loss=0.5792602083193074\n",
      "Gradient Descent(1186/4999): loss=0.5792578072042937\n",
      "Gradient Descent(1187/4999): loss=0.5792554135806962\n",
      "Gradient Descent(1188/4999): loss=0.5792530274198334\n",
      "Gradient Descent(1189/4999): loss=0.5792506486931358\n",
      "Gradient Descent(1190/4999): loss=0.579248277372147\n",
      "Gradient Descent(1191/4999): loss=0.5792459134285223\n",
      "Gradient Descent(1192/4999): loss=0.5792435568340288\n",
      "Gradient Descent(1193/4999): loss=0.5792412075605452\n",
      "Gradient Descent(1194/4999): loss=0.5792388655800603\n",
      "Gradient Descent(1195/4999): loss=0.5792365308646734\n",
      "Gradient Descent(1196/4999): loss=0.579234203386594\n",
      "Gradient Descent(1197/4999): loss=0.579231883118141\n",
      "Gradient Descent(1198/4999): loss=0.5792295700317418\n",
      "Gradient Descent(1199/4999): loss=0.5792272640999331\n",
      "Gradient Descent(1200/4999): loss=0.5792249652953588\n",
      "Gradient Descent(1201/4999): loss=0.5792226735907718\n",
      "Gradient Descent(1202/4999): loss=0.5792203889590312\n",
      "Gradient Descent(1203/4999): loss=0.5792181113731034\n",
      "Gradient Descent(1204/4999): loss=0.5792158408060611\n",
      "Gradient Descent(1205/4999): loss=0.5792135772310834\n",
      "Gradient Descent(1206/4999): loss=0.5792113206214543\n",
      "Gradient Descent(1207/4999): loss=0.5792090709505635\n",
      "Gradient Descent(1208/4999): loss=0.5792068281919054\n",
      "Gradient Descent(1209/4999): loss=0.5792045923190784\n",
      "Gradient Descent(1210/4999): loss=0.5792023633057852\n",
      "Gradient Descent(1211/4999): loss=0.5792001411258321\n",
      "Gradient Descent(1212/4999): loss=0.579197925753128\n",
      "Gradient Descent(1213/4999): loss=0.5791957171616846\n",
      "Gradient Descent(1214/4999): loss=0.5791935153256165\n",
      "Gradient Descent(1215/4999): loss=0.5791913202191394\n",
      "Gradient Descent(1216/4999): loss=0.579189131816571\n",
      "Gradient Descent(1217/4999): loss=0.5791869500923297\n",
      "Gradient Descent(1218/4999): loss=0.5791847750209347\n",
      "Gradient Descent(1219/4999): loss=0.5791826065770055\n",
      "Gradient Descent(1220/4999): loss=0.5791804447352615\n",
      "Gradient Descent(1221/4999): loss=0.5791782894705215\n",
      "Gradient Descent(1222/4999): loss=0.5791761407577034\n",
      "Gradient Descent(1223/4999): loss=0.5791739985718238\n",
      "Gradient Descent(1224/4999): loss=0.5791718628879975\n",
      "Gradient Descent(1225/4999): loss=0.5791697336814373\n",
      "Gradient Descent(1226/4999): loss=0.5791676109274535\n",
      "Gradient Descent(1227/4999): loss=0.5791654946014535\n",
      "Gradient Descent(1228/4999): loss=0.5791633846789414\n",
      "Gradient Descent(1229/4999): loss=0.5791612811355175\n",
      "Gradient Descent(1230/4999): loss=0.5791591839468784\n",
      "Gradient Descent(1231/4999): loss=0.5791570930888164\n",
      "Gradient Descent(1232/4999): loss=0.5791550085372185\n",
      "Gradient Descent(1233/4999): loss=0.5791529302680667\n",
      "Gradient Descent(1234/4999): loss=0.5791508582574376\n",
      "Gradient Descent(1235/4999): loss=0.579148792481502\n",
      "Gradient Descent(1236/4999): loss=0.5791467329165242\n",
      "Gradient Descent(1237/4999): loss=0.5791446795388614\n",
      "Gradient Descent(1238/4999): loss=0.5791426323249649\n",
      "Gradient Descent(1239/4999): loss=0.5791405912513775\n",
      "Gradient Descent(1240/4999): loss=0.5791385562947348\n",
      "Gradient Descent(1241/4999): loss=0.5791365274317638\n",
      "Gradient Descent(1242/4999): loss=0.5791345046392835\n",
      "Gradient Descent(1243/4999): loss=0.5791324878942041\n",
      "Gradient Descent(1244/4999): loss=0.5791304771735261\n",
      "Gradient Descent(1245/4999): loss=0.5791284724543404\n",
      "Gradient Descent(1246/4999): loss=0.5791264737138284\n",
      "Gradient Descent(1247/4999): loss=0.579124480929261\n",
      "Gradient Descent(1248/4999): loss=0.5791224940779982\n",
      "Gradient Descent(1249/4999): loss=0.5791205131374895\n",
      "Gradient Descent(1250/4999): loss=0.5791185380852722\n",
      "Gradient Descent(1251/4999): loss=0.5791165688989728\n",
      "Gradient Descent(1252/4999): loss=0.5791146055563052\n",
      "Gradient Descent(1253/4999): loss=0.579112648035071\n",
      "Gradient Descent(1254/4999): loss=0.5791106963131589\n",
      "Gradient Descent(1255/4999): loss=0.5791087503685451\n",
      "Gradient Descent(1256/4999): loss=0.5791068101792913\n",
      "Gradient Descent(1257/4999): loss=0.5791048757235461\n",
      "Gradient Descent(1258/4999): loss=0.5791029469795441\n",
      "Gradient Descent(1259/4999): loss=0.5791010239256047\n",
      "Gradient Descent(1260/4999): loss=0.5790991065401332\n",
      "Gradient Descent(1261/4999): loss=0.5790971948016193\n",
      "Gradient Descent(1262/4999): loss=0.5790952886886374\n",
      "Gradient Descent(1263/4999): loss=0.5790933881798459\n",
      "Gradient Descent(1264/4999): loss=0.5790914932539873\n",
      "Gradient Descent(1265/4999): loss=0.5790896038898873\n",
      "Gradient Descent(1266/4999): loss=0.5790877200664551\n",
      "Gradient Descent(1267/4999): loss=0.5790858417626824\n",
      "Gradient Descent(1268/4999): loss=0.5790839689576435\n",
      "Gradient Descent(1269/4999): loss=0.579082101630495\n",
      "Gradient Descent(1270/4999): loss=0.5790802397604753\n",
      "Gradient Descent(1271/4999): loss=0.5790783833269044\n",
      "Gradient Descent(1272/4999): loss=0.5790765323091832\n",
      "Gradient Descent(1273/4999): loss=0.5790746866867941\n",
      "Gradient Descent(1274/4999): loss=0.5790728464392992\n",
      "Gradient Descent(1275/4999): loss=0.5790710115463418\n",
      "Gradient Descent(1276/4999): loss=0.5790691819876442\n",
      "Gradient Descent(1277/4999): loss=0.5790673577430092\n",
      "Gradient Descent(1278/4999): loss=0.5790655387923184\n",
      "Gradient Descent(1279/4999): loss=0.5790637251155321\n",
      "Gradient Descent(1280/4999): loss=0.57906191669269\n",
      "Gradient Descent(1281/4999): loss=0.5790601135039094\n",
      "Gradient Descent(1282/4999): loss=0.5790583155293862\n",
      "Gradient Descent(1283/4999): loss=0.579056522749394\n",
      "Gradient Descent(1284/4999): loss=0.5790547351442836\n",
      "Gradient Descent(1285/4999): loss=0.579052952694483\n",
      "Gradient Descent(1286/4999): loss=0.5790511753804967\n",
      "Gradient Descent(1287/4999): loss=0.5790494031829063\n",
      "Gradient Descent(1288/4999): loss=0.5790476360823692\n",
      "Gradient Descent(1289/4999): loss=0.5790458740596192\n",
      "Gradient Descent(1290/4999): loss=0.5790441170954649\n",
      "Gradient Descent(1291/4999): loss=0.5790423651707907\n",
      "Gradient Descent(1292/4999): loss=0.5790406182665561\n",
      "Gradient Descent(1293/4999): loss=0.579038876363795\n",
      "Gradient Descent(1294/4999): loss=0.5790371394436161\n",
      "Gradient Descent(1295/4999): loss=0.5790354074872017\n",
      "Gradient Descent(1296/4999): loss=0.5790336804758084\n",
      "Gradient Descent(1297/4999): loss=0.579031958390766\n",
      "Gradient Descent(1298/4999): loss=0.5790302412134779\n",
      "Gradient Descent(1299/4999): loss=0.57902852892542\n",
      "Gradient Descent(1300/4999): loss=0.5790268215081409\n",
      "Gradient Descent(1301/4999): loss=0.5790251189432623\n",
      "Gradient Descent(1302/4999): loss=0.579023421212477\n",
      "Gradient Descent(1303/4999): loss=0.57902172829755\n",
      "Gradient Descent(1304/4999): loss=0.5790200401803183\n",
      "Gradient Descent(1305/4999): loss=0.5790183568426892\n",
      "Gradient Descent(1306/4999): loss=0.5790166782666417\n",
      "Gradient Descent(1307/4999): loss=0.5790150044342252\n",
      "Gradient Descent(1308/4999): loss=0.5790133353275593\n",
      "Gradient Descent(1309/4999): loss=0.5790116709288341\n",
      "Gradient Descent(1310/4999): loss=0.5790100112203094\n",
      "Gradient Descent(1311/4999): loss=0.5790083561843141\n",
      "Gradient Descent(1312/4999): loss=0.5790067058032471\n",
      "Gradient Descent(1313/4999): loss=0.579005060059576\n",
      "Gradient Descent(1314/4999): loss=0.5790034189358368\n",
      "Gradient Descent(1315/4999): loss=0.5790017824146344\n",
      "Gradient Descent(1316/4999): loss=0.579000150478642\n",
      "Gradient Descent(1317/4999): loss=0.5789985231106002\n",
      "Gradient Descent(1318/4999): loss=0.5789969002933175\n",
      "Gradient Descent(1319/4999): loss=0.5789952820096699\n",
      "Gradient Descent(1320/4999): loss=0.5789936682426002\n",
      "Gradient Descent(1321/4999): loss=0.5789920589751186\n",
      "Gradient Descent(1322/4999): loss=0.5789904541903014\n",
      "Gradient Descent(1323/4999): loss=0.5789888538712913\n",
      "Gradient Descent(1324/4999): loss=0.578987258001297\n",
      "Gradient Descent(1325/4999): loss=0.5789856665635933\n",
      "Gradient Descent(1326/4999): loss=0.5789840795415205\n",
      "Gradient Descent(1327/4999): loss=0.5789824969184836\n",
      "Gradient Descent(1328/4999): loss=0.5789809186779534\n",
      "Gradient Descent(1329/4999): loss=0.578979344803465\n",
      "Gradient Descent(1330/4999): loss=0.578977775278618\n",
      "Gradient Descent(1331/4999): loss=0.5789762100870766\n",
      "Gradient Descent(1332/4999): loss=0.5789746492125682\n",
      "Gradient Descent(1333/4999): loss=0.5789730926388851\n",
      "Gradient Descent(1334/4999): loss=0.5789715403498822\n",
      "Gradient Descent(1335/4999): loss=0.5789699923294778\n",
      "Gradient Descent(1336/4999): loss=0.5789684485616535\n",
      "Gradient Descent(1337/4999): loss=0.5789669090304532\n",
      "Gradient Descent(1338/4999): loss=0.5789653737199839\n",
      "Gradient Descent(1339/4999): loss=0.578963842614414\n",
      "Gradient Descent(1340/4999): loss=0.5789623156979746\n",
      "Gradient Descent(1341/4999): loss=0.5789607929549582\n",
      "Gradient Descent(1342/4999): loss=0.5789592743697191\n",
      "Gradient Descent(1343/4999): loss=0.5789577599266723\n",
      "Gradient Descent(1344/4999): loss=0.5789562496102945\n",
      "Gradient Descent(1345/4999): loss=0.5789547434051228\n",
      "Gradient Descent(1346/4999): loss=0.5789532412957548\n",
      "Gradient Descent(1347/4999): loss=0.5789517432668484\n",
      "Gradient Descent(1348/4999): loss=0.578950249303122\n",
      "Gradient Descent(1349/4999): loss=0.578948759389353\n",
      "Gradient Descent(1350/4999): loss=0.5789472735103793\n",
      "Gradient Descent(1351/4999): loss=0.5789457916510973\n",
      "Gradient Descent(1352/4999): loss=0.5789443137964632\n",
      "Gradient Descent(1353/4999): loss=0.5789428399314918\n",
      "Gradient Descent(1354/4999): loss=0.5789413700412562\n",
      "Gradient Descent(1355/4999): loss=0.5789399041108887\n",
      "Gradient Descent(1356/4999): loss=0.5789384421255791\n",
      "Gradient Descent(1357/4999): loss=0.5789369840705751\n",
      "Gradient Descent(1358/4999): loss=0.5789355299311829\n",
      "Gradient Descent(1359/4999): loss=0.5789340796927657\n",
      "Gradient Descent(1360/4999): loss=0.5789326333407435\n",
      "Gradient Descent(1361/4999): loss=0.5789311908605942\n",
      "Gradient Descent(1362/4999): loss=0.5789297522378519\n",
      "Gradient Descent(1363/4999): loss=0.5789283174581076\n",
      "Gradient Descent(1364/4999): loss=0.5789268865070087\n",
      "Gradient Descent(1365/4999): loss=0.5789254593702586\n",
      "Gradient Descent(1366/4999): loss=0.5789240360336163\n",
      "Gradient Descent(1367/4999): loss=0.5789226164828972\n",
      "Gradient Descent(1368/4999): loss=0.5789212007039718\n",
      "Gradient Descent(1369/4999): loss=0.5789197886827658\n",
      "Gradient Descent(1370/4999): loss=0.5789183804052601\n",
      "Gradient Descent(1371/4999): loss=0.57891697585749\n",
      "Gradient Descent(1372/4999): loss=0.5789155750255464\n",
      "Gradient Descent(1373/4999): loss=0.5789141778955733\n",
      "Gradient Descent(1374/4999): loss=0.5789127844537699\n",
      "Gradient Descent(1375/4999): loss=0.5789113946863889\n",
      "Gradient Descent(1376/4999): loss=0.5789100085797368\n",
      "Gradient Descent(1377/4999): loss=0.5789086261201736\n",
      "Gradient Descent(1378/4999): loss=0.5789072472941129\n",
      "Gradient Descent(1379/4999): loss=0.578905872088021\n",
      "Gradient Descent(1380/4999): loss=0.5789045004884174\n",
      "Gradient Descent(1381/4999): loss=0.5789031324818741\n",
      "Gradient Descent(1382/4999): loss=0.5789017680550157\n",
      "Gradient Descent(1383/4999): loss=0.5789004071945194\n",
      "Gradient Descent(1384/4999): loss=0.5788990498871135\n",
      "Gradient Descent(1385/4999): loss=0.5788976961195793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1386/4999): loss=0.578896345878749\n",
      "Gradient Descent(1387/4999): loss=0.5788949991515068\n",
      "Gradient Descent(1388/4999): loss=0.5788936559247875\n",
      "Gradient Descent(1389/4999): loss=0.5788923161855775\n",
      "Gradient Descent(1390/4999): loss=0.5788909799209139\n",
      "Gradient Descent(1391/4999): loss=0.5788896471178843\n",
      "Gradient Descent(1392/4999): loss=0.5788883177636268\n",
      "Gradient Descent(1393/4999): loss=0.5788869918453301\n",
      "Gradient Descent(1394/4999): loss=0.5788856693502323\n",
      "Gradient Descent(1395/4999): loss=0.578884350265622\n",
      "Gradient Descent(1396/4999): loss=0.5788830345788369\n",
      "Gradient Descent(1397/4999): loss=0.5788817222772645\n",
      "Gradient Descent(1398/4999): loss=0.5788804133483415\n",
      "Gradient Descent(1399/4999): loss=0.5788791077795536\n",
      "Gradient Descent(1400/4999): loss=0.5788778055584356\n",
      "Gradient Descent(1401/4999): loss=0.5788765066725704\n",
      "Gradient Descent(1402/4999): loss=0.5788752111095901\n",
      "Gradient Descent(1403/4999): loss=0.5788739188571747\n",
      "Gradient Descent(1404/4999): loss=0.5788726299030522\n",
      "Gradient Descent(1405/4999): loss=0.5788713442349991\n",
      "Gradient Descent(1406/4999): loss=0.5788700618408386\n",
      "Gradient Descent(1407/4999): loss=0.5788687827084427\n",
      "Gradient Descent(1408/4999): loss=0.5788675068257297\n",
      "Gradient Descent(1409/4999): loss=0.5788662341806656\n",
      "Gradient Descent(1410/4999): loss=0.5788649647612634\n",
      "Gradient Descent(1411/4999): loss=0.5788636985555824\n",
      "Gradient Descent(1412/4999): loss=0.5788624355517289\n",
      "Gradient Descent(1413/4999): loss=0.5788611757378557\n",
      "Gradient Descent(1414/4999): loss=0.5788599191021617\n",
      "Gradient Descent(1415/4999): loss=0.5788586656328918\n",
      "Gradient Descent(1416/4999): loss=0.5788574153183367\n",
      "Gradient Descent(1417/4999): loss=0.5788561681468332\n",
      "Gradient Descent(1418/4999): loss=0.5788549241067629\n",
      "Gradient Descent(1419/4999): loss=0.5788536831865535\n",
      "Gradient Descent(1420/4999): loss=0.5788524453746774\n",
      "Gradient Descent(1421/4999): loss=0.5788512106596521\n",
      "Gradient Descent(1422/4999): loss=0.5788499790300398\n",
      "Gradient Descent(1423/4999): loss=0.5788487504744474\n",
      "Gradient Descent(1424/4999): loss=0.5788475249815265\n",
      "Gradient Descent(1425/4999): loss=0.5788463025399722\n",
      "Gradient Descent(1426/4999): loss=0.5788450831385246\n",
      "Gradient Descent(1427/4999): loss=0.5788438667659671\n",
      "Gradient Descent(1428/4999): loss=0.5788426534111273\n",
      "Gradient Descent(1429/4999): loss=0.5788414430628759\n",
      "Gradient Descent(1430/4999): loss=0.5788402357101273\n",
      "Gradient Descent(1431/4999): loss=0.578839031341839\n",
      "Gradient Descent(1432/4999): loss=0.5788378299470118\n",
      "Gradient Descent(1433/4999): loss=0.5788366315146889\n",
      "Gradient Descent(1434/4999): loss=0.5788354360339569\n",
      "Gradient Descent(1435/4999): loss=0.5788342434939443\n",
      "Gradient Descent(1436/4999): loss=0.5788330538838222\n",
      "Gradient Descent(1437/4999): loss=0.5788318671928043\n",
      "Gradient Descent(1438/4999): loss=0.5788306834101455\n",
      "Gradient Descent(1439/4999): loss=0.5788295025251436\n",
      "Gradient Descent(1440/4999): loss=0.5788283245271374\n",
      "Gradient Descent(1441/4999): loss=0.5788271494055073\n",
      "Gradient Descent(1442/4999): loss=0.5788259771496755\n",
      "Gradient Descent(1443/4999): loss=0.5788248077491052\n",
      "Gradient Descent(1444/4999): loss=0.5788236411933005\n",
      "Gradient Descent(1445/4999): loss=0.5788224774718064\n",
      "Gradient Descent(1446/4999): loss=0.5788213165742089\n",
      "Gradient Descent(1447/4999): loss=0.5788201584901346\n",
      "Gradient Descent(1448/4999): loss=0.57881900320925\n",
      "Gradient Descent(1449/4999): loss=0.5788178507212626\n",
      "Gradient Descent(1450/4999): loss=0.5788167010159194\n",
      "Gradient Descent(1451/4999): loss=0.5788155540830074\n",
      "Gradient Descent(1452/4999): loss=0.5788144099123541\n",
      "Gradient Descent(1453/4999): loss=0.5788132684938254\n",
      "Gradient Descent(1454/4999): loss=0.5788121298173279\n",
      "Gradient Descent(1455/4999): loss=0.5788109938728063\n",
      "Gradient Descent(1456/4999): loss=0.5788098606502458\n",
      "Gradient Descent(1457/4999): loss=0.5788087301396696\n",
      "Gradient Descent(1458/4999): loss=0.5788076023311399\n",
      "Gradient Descent(1459/4999): loss=0.5788064772147578\n",
      "Gradient Descent(1460/4999): loss=0.578805354780663\n",
      "Gradient Descent(1461/4999): loss=0.5788042350190334\n",
      "Gradient Descent(1462/4999): loss=0.5788031179200854\n",
      "Gradient Descent(1463/4999): loss=0.5788020034740732\n",
      "Gradient Descent(1464/4999): loss=0.5788008916712889\n",
      "Gradient Descent(1465/4999): loss=0.5787997825020627\n",
      "Gradient Descent(1466/4999): loss=0.5787986759567622\n",
      "Gradient Descent(1467/4999): loss=0.5787975720257926\n",
      "Gradient Descent(1468/4999): loss=0.5787964706995963\n",
      "Gradient Descent(1469/4999): loss=0.5787953719686532\n",
      "Gradient Descent(1470/4999): loss=0.57879427582348\n",
      "Gradient Descent(1471/4999): loss=0.5787931822546304\n",
      "Gradient Descent(1472/4999): loss=0.5787920912526944\n",
      "Gradient Descent(1473/4999): loss=0.5787910028082995\n",
      "Gradient Descent(1474/4999): loss=0.5787899169121091\n",
      "Gradient Descent(1475/4999): loss=0.5787888335548228\n",
      "Gradient Descent(1476/4999): loss=0.578787752727177\n",
      "Gradient Descent(1477/4999): loss=0.5787866744199432\n",
      "Gradient Descent(1478/4999): loss=0.5787855986239298\n",
      "Gradient Descent(1479/4999): loss=0.57878452532998\n",
      "Gradient Descent(1480/4999): loss=0.5787834545289735\n",
      "Gradient Descent(1481/4999): loss=0.578782386211825\n",
      "Gradient Descent(1482/4999): loss=0.5787813203694846\n",
      "Gradient Descent(1483/4999): loss=0.5787802569929373\n",
      "Gradient Descent(1484/4999): loss=0.5787791960732037\n",
      "Gradient Descent(1485/4999): loss=0.5787781376013391\n",
      "Gradient Descent(1486/4999): loss=0.5787770815684335\n",
      "Gradient Descent(1487/4999): loss=0.5787760279656116\n",
      "Gradient Descent(1488/4999): loss=0.5787749767840322\n",
      "Gradient Descent(1489/4999): loss=0.5787739280148895\n",
      "Gradient Descent(1490/4999): loss=0.5787728816494109\n",
      "Gradient Descent(1491/4999): loss=0.5787718376788582\n",
      "Gradient Descent(1492/4999): loss=0.5787707960945273\n",
      "Gradient Descent(1493/4999): loss=0.5787697568877482\n",
      "Gradient Descent(1494/4999): loss=0.5787687200498839\n",
      "Gradient Descent(1495/4999): loss=0.5787676855723316\n",
      "Gradient Descent(1496/4999): loss=0.5787666534465213\n",
      "Gradient Descent(1497/4999): loss=0.578765623663917\n",
      "Gradient Descent(1498/4999): loss=0.5787645962160154\n",
      "Gradient Descent(1499/4999): loss=0.5787635710943464\n",
      "Gradient Descent(1500/4999): loss=0.5787625482904728\n",
      "Gradient Descent(1501/4999): loss=0.57876152779599\n",
      "Gradient Descent(1502/4999): loss=0.5787605096025263\n",
      "Gradient Descent(1503/4999): loss=0.5787594937017425\n",
      "Gradient Descent(1504/4999): loss=0.5787584800853315\n",
      "Gradient Descent(1505/4999): loss=0.578757468745019\n",
      "Gradient Descent(1506/4999): loss=0.5787564596725623\n",
      "Gradient Descent(1507/4999): loss=0.5787554528597508\n",
      "Gradient Descent(1508/4999): loss=0.578754448298406\n",
      "Gradient Descent(1509/4999): loss=0.5787534459803813\n",
      "Gradient Descent(1510/4999): loss=0.5787524458975609\n",
      "Gradient Descent(1511/4999): loss=0.5787514480418616\n",
      "Gradient Descent(1512/4999): loss=0.5787504524052312\n",
      "Gradient Descent(1513/4999): loss=0.5787494589796481\n",
      "Gradient Descent(1514/4999): loss=0.5787484677571227\n",
      "Gradient Descent(1515/4999): loss=0.578747478729696\n",
      "Gradient Descent(1516/4999): loss=0.57874649188944\n",
      "Gradient Descent(1517/4999): loss=0.5787455072284576\n",
      "Gradient Descent(1518/4999): loss=0.578744524738882\n",
      "Gradient Descent(1519/4999): loss=0.5787435444128772\n",
      "Gradient Descent(1520/4999): loss=0.5787425662426374\n",
      "Gradient Descent(1521/4999): loss=0.5787415902203876\n",
      "Gradient Descent(1522/4999): loss=0.5787406163383821\n",
      "Gradient Descent(1523/4999): loss=0.578739644588906\n",
      "Gradient Descent(1524/4999): loss=0.5787386749642742\n",
      "Gradient Descent(1525/4999): loss=0.5787377074568307\n",
      "Gradient Descent(1526/4999): loss=0.5787367420589505\n",
      "Gradient Descent(1527/4999): loss=0.5787357787630372\n",
      "Gradient Descent(1528/4999): loss=0.578734817561524\n",
      "Gradient Descent(1529/4999): loss=0.5787338584468736\n",
      "Gradient Descent(1530/4999): loss=0.578732901411578\n",
      "Gradient Descent(1531/4999): loss=0.5787319464481578\n",
      "Gradient Descent(1532/4999): loss=0.5787309935491637\n",
      "Gradient Descent(1533/4999): loss=0.5787300427071741\n",
      "Gradient Descent(1534/4999): loss=0.5787290939147968\n",
      "Gradient Descent(1535/4999): loss=0.5787281471646681\n",
      "Gradient Descent(1536/4999): loss=0.578727202449453\n",
      "Gradient Descent(1537/4999): loss=0.5787262597618447\n",
      "Gradient Descent(1538/4999): loss=0.5787253190945647\n",
      "Gradient Descent(1539/4999): loss=0.578724380440363\n",
      "Gradient Descent(1540/4999): loss=0.5787234437920175\n",
      "Gradient Descent(1541/4999): loss=0.5787225091423343\n",
      "Gradient Descent(1542/4999): loss=0.5787215764841468\n",
      "Gradient Descent(1543/4999): loss=0.5787206458103169\n",
      "Gradient Descent(1544/4999): loss=0.5787197171137338\n",
      "Gradient Descent(1545/4999): loss=0.578718790387314\n",
      "Gradient Descent(1546/4999): loss=0.5787178656240022\n",
      "Gradient Descent(1547/4999): loss=0.5787169428167697\n",
      "Gradient Descent(1548/4999): loss=0.578716021958615\n",
      "Gradient Descent(1549/4999): loss=0.5787151030425646\n",
      "Gradient Descent(1550/4999): loss=0.5787141860616709\n",
      "Gradient Descent(1551/4999): loss=0.5787132710090139\n",
      "Gradient Descent(1552/4999): loss=0.5787123578777004\n",
      "Gradient Descent(1553/4999): loss=0.5787114466608634\n",
      "Gradient Descent(1554/4999): loss=0.578710537351663\n",
      "Gradient Descent(1555/4999): loss=0.5787096299432856\n",
      "Gradient Descent(1556/4999): loss=0.5787087244289437\n",
      "Gradient Descent(1557/4999): loss=0.5787078208018769\n",
      "Gradient Descent(1558/4999): loss=0.5787069190553499\n",
      "Gradient Descent(1559/4999): loss=0.578706019182654\n",
      "Gradient Descent(1560/4999): loss=0.5787051211771066\n",
      "Gradient Descent(1561/4999): loss=0.578704225032051\n",
      "Gradient Descent(1562/4999): loss=0.5787033307408558\n",
      "Gradient Descent(1563/4999): loss=0.5787024382969156\n",
      "Gradient Descent(1564/4999): loss=0.5787015476936505\n",
      "Gradient Descent(1565/4999): loss=0.5787006589245062\n",
      "Gradient Descent(1566/4999): loss=0.5786997719829534\n",
      "Gradient Descent(1567/4999): loss=0.5786988868624885\n",
      "Gradient Descent(1568/4999): loss=0.5786980035566327\n",
      "Gradient Descent(1569/4999): loss=0.5786971220589326\n",
      "Gradient Descent(1570/4999): loss=0.5786962423629591\n",
      "Gradient Descent(1571/4999): loss=0.5786953644623092\n",
      "Gradient Descent(1572/4999): loss=0.5786944883506032\n",
      "Gradient Descent(1573/4999): loss=0.5786936140214869\n",
      "Gradient Descent(1574/4999): loss=0.578692741468631\n",
      "Gradient Descent(1575/4999): loss=0.5786918706857296\n",
      "Gradient Descent(1576/4999): loss=0.5786910016665022\n",
      "Gradient Descent(1577/4999): loss=0.5786901344046921\n",
      "Gradient Descent(1578/4999): loss=0.5786892688940667\n",
      "Gradient Descent(1579/4999): loss=0.578688405128418\n",
      "Gradient Descent(1580/4999): loss=0.5786875431015613\n",
      "Gradient Descent(1581/4999): loss=0.5786866828073364\n",
      "Gradient Descent(1582/4999): loss=0.5786858242396065\n",
      "Gradient Descent(1583/4999): loss=0.5786849673922588\n",
      "Gradient Descent(1584/4999): loss=0.5786841122592039\n",
      "Gradient Descent(1585/4999): loss=0.5786832588343763\n",
      "Gradient Descent(1586/4999): loss=0.5786824071117335\n",
      "Gradient Descent(1587/4999): loss=0.5786815570852566\n",
      "Gradient Descent(1588/4999): loss=0.5786807087489497\n",
      "Gradient Descent(1589/4999): loss=0.5786798620968403\n",
      "Gradient Descent(1590/4999): loss=0.578679017122979\n",
      "Gradient Descent(1591/4999): loss=0.5786781738214393\n",
      "Gradient Descent(1592/4999): loss=0.5786773321863175\n",
      "Gradient Descent(1593/4999): loss=0.5786764922117328\n",
      "Gradient Descent(1594/4999): loss=0.5786756538918271\n",
      "Gradient Descent(1595/4999): loss=0.5786748172207647\n",
      "Gradient Descent(1596/4999): loss=0.5786739821927331\n",
      "Gradient Descent(1597/4999): loss=0.5786731488019414\n",
      "Gradient Descent(1598/4999): loss=0.5786723170426218\n",
      "Gradient Descent(1599/4999): loss=0.5786714869090278\n",
      "Gradient Descent(1600/4999): loss=0.5786706583954361\n",
      "Gradient Descent(1601/4999): loss=0.5786698314961451\n",
      "Gradient Descent(1602/4999): loss=0.5786690062054752\n",
      "Gradient Descent(1603/4999): loss=0.5786681825177683\n",
      "Gradient Descent(1604/4999): loss=0.5786673604273891\n",
      "Gradient Descent(1605/4999): loss=0.5786665399287231\n",
      "Gradient Descent(1606/4999): loss=0.578665721016178\n",
      "Gradient Descent(1607/4999): loss=0.5786649036841828\n",
      "Gradient Descent(1608/4999): loss=0.578664087927188\n",
      "Gradient Descent(1609/4999): loss=0.5786632737396661\n",
      "Gradient Descent(1610/4999): loss=0.57866246111611\n",
      "Gradient Descent(1611/4999): loss=0.5786616500510342\n",
      "Gradient Descent(1612/4999): loss=0.5786608405389747\n",
      "Gradient Descent(1613/4999): loss=0.5786600325744883\n",
      "Gradient Descent(1614/4999): loss=0.5786592261521526\n",
      "Gradient Descent(1615/4999): loss=0.5786584212665665\n",
      "Gradient Descent(1616/4999): loss=0.5786576179123494\n",
      "Gradient Descent(1617/4999): loss=0.5786568160841414\n",
      "Gradient Descent(1618/4999): loss=0.5786560157766037\n",
      "Gradient Descent(1619/4999): loss=0.5786552169844179\n",
      "Gradient Descent(1620/4999): loss=0.5786544197022857\n",
      "Gradient Descent(1621/4999): loss=0.5786536239249297\n",
      "Gradient Descent(1622/4999): loss=0.5786528296470926\n",
      "Gradient Descent(1623/4999): loss=0.5786520368635374\n",
      "Gradient Descent(1624/4999): loss=0.5786512455690473\n",
      "Gradient Descent(1625/4999): loss=0.5786504557584256\n",
      "Gradient Descent(1626/4999): loss=0.5786496674264957\n",
      "Gradient Descent(1627/4999): loss=0.5786488805681008\n",
      "Gradient Descent(1628/4999): loss=0.578648095178104\n",
      "Gradient Descent(1629/4999): loss=0.5786473112513881\n",
      "Gradient Descent(1630/4999): loss=0.578646528782856\n",
      "Gradient Descent(1631/4999): loss=0.5786457477674298\n",
      "Gradient Descent(1632/4999): loss=0.5786449682000513\n",
      "Gradient Descent(1633/4999): loss=0.5786441900756819\n",
      "Gradient Descent(1634/4999): loss=0.5786434133893019\n",
      "Gradient Descent(1635/4999): loss=0.5786426381359118\n",
      "Gradient Descent(1636/4999): loss=0.5786418643105304\n",
      "Gradient Descent(1637/4999): loss=0.5786410919081967\n",
      "Gradient Descent(1638/4999): loss=0.5786403209239674\n",
      "Gradient Descent(1639/4999): loss=0.5786395513529199\n",
      "Gradient Descent(1640/4999): loss=0.5786387831901489\n",
      "Gradient Descent(1641/4999): loss=0.5786380164307694\n",
      "Gradient Descent(1642/4999): loss=0.5786372510699141\n",
      "Gradient Descent(1643/4999): loss=0.5786364871027352\n",
      "Gradient Descent(1644/4999): loss=0.5786357245244033\n",
      "Gradient Descent(1645/4999): loss=0.5786349633301071\n",
      "Gradient Descent(1646/4999): loss=0.5786342035150546\n",
      "Gradient Descent(1647/4999): loss=0.5786334450744716\n",
      "Gradient Descent(1648/4999): loss=0.5786326880036027\n",
      "Gradient Descent(1649/4999): loss=0.5786319322977104\n",
      "Gradient Descent(1650/4999): loss=0.5786311779520759\n",
      "Gradient Descent(1651/4999): loss=0.5786304249619979\n",
      "Gradient Descent(1652/4999): loss=0.5786296733227938\n",
      "Gradient Descent(1653/4999): loss=0.5786289230297987\n",
      "Gradient Descent(1654/4999): loss=0.5786281740783655\n",
      "Gradient Descent(1655/4999): loss=0.5786274264638654\n",
      "Gradient Descent(1656/4999): loss=0.578626680181687\n",
      "Gradient Descent(1657/4999): loss=0.5786259352272366\n",
      "Gradient Descent(1658/4999): loss=0.5786251915959386\n",
      "Gradient Descent(1659/4999): loss=0.5786244492832345\n",
      "Gradient Descent(1660/4999): loss=0.5786237082845833\n",
      "Gradient Descent(1661/4999): loss=0.5786229685954619\n",
      "Gradient Descent(1662/4999): loss=0.5786222302113643\n",
      "Gradient Descent(1663/4999): loss=0.5786214931278016\n",
      "Gradient Descent(1664/4999): loss=0.5786207573403027\n",
      "Gradient Descent(1665/4999): loss=0.578620022844413\n",
      "Gradient Descent(1666/4999): loss=0.5786192896356956\n",
      "Gradient Descent(1667/4999): loss=0.5786185577097304\n",
      "Gradient Descent(1668/4999): loss=0.5786178270621138\n",
      "Gradient Descent(1669/4999): loss=0.57861709768846\n",
      "Gradient Descent(1670/4999): loss=0.5786163695843995\n",
      "Gradient Descent(1671/4999): loss=0.5786156427455796\n",
      "Gradient Descent(1672/4999): loss=0.5786149171676643\n",
      "Gradient Descent(1673/4999): loss=0.5786141928463344\n",
      "Gradient Descent(1674/4999): loss=0.5786134697772873\n",
      "Gradient Descent(1675/4999): loss=0.5786127479562367\n",
      "Gradient Descent(1676/4999): loss=0.5786120273789128\n",
      "Gradient Descent(1677/4999): loss=0.5786113080410622\n",
      "Gradient Descent(1678/4999): loss=0.5786105899384479\n",
      "Gradient Descent(1679/4999): loss=0.5786098730668492\n",
      "Gradient Descent(1680/4999): loss=0.5786091574220613\n",
      "Gradient Descent(1681/4999): loss=0.5786084429998957\n",
      "Gradient Descent(1682/4999): loss=0.5786077297961801\n",
      "Gradient Descent(1683/4999): loss=0.578607017806758\n",
      "Gradient Descent(1684/4999): loss=0.5786063070274889\n",
      "Gradient Descent(1685/4999): loss=0.5786055974542482\n",
      "Gradient Descent(1686/4999): loss=0.5786048890829273\n",
      "Gradient Descent(1687/4999): loss=0.5786041819094326\n",
      "Gradient Descent(1688/4999): loss=0.5786034759296875\n",
      "Gradient Descent(1689/4999): loss=0.5786027711396294\n",
      "Gradient Descent(1690/4999): loss=0.5786020675352128\n",
      "Gradient Descent(1691/4999): loss=0.5786013651124068\n",
      "Gradient Descent(1692/4999): loss=0.5786006638671962\n",
      "Gradient Descent(1693/4999): loss=0.5785999637955812\n",
      "Gradient Descent(1694/4999): loss=0.578599264893577\n",
      "Gradient Descent(1695/4999): loss=0.5785985671572148\n",
      "Gradient Descent(1696/4999): loss=0.57859787058254\n",
      "Gradient Descent(1697/4999): loss=0.5785971751656142\n",
      "Gradient Descent(1698/4999): loss=0.578596480902513\n",
      "Gradient Descent(1699/4999): loss=0.5785957877893283\n",
      "Gradient Descent(1700/4999): loss=0.5785950958221658\n",
      "Gradient Descent(1701/4999): loss=0.5785944049971468\n",
      "Gradient Descent(1702/4999): loss=0.5785937153104067\n",
      "Gradient Descent(1703/4999): loss=0.5785930267580969\n",
      "Gradient Descent(1704/4999): loss=0.5785923393363824\n",
      "Gradient Descent(1705/4999): loss=0.5785916530414437\n",
      "Gradient Descent(1706/4999): loss=0.578590967869475\n",
      "Gradient Descent(1707/4999): loss=0.5785902838166861\n",
      "Gradient Descent(1708/4999): loss=0.5785896008793003\n",
      "Gradient Descent(1709/4999): loss=0.5785889190535562\n",
      "Gradient Descent(1710/4999): loss=0.5785882383357066\n",
      "Gradient Descent(1711/4999): loss=0.5785875587220182\n",
      "Gradient Descent(1712/4999): loss=0.578586880208772\n",
      "Gradient Descent(1713/4999): loss=0.5785862027922638\n",
      "Gradient Descent(1714/4999): loss=0.5785855264688031\n",
      "Gradient Descent(1715/4999): loss=0.5785848512347137\n",
      "Gradient Descent(1716/4999): loss=0.5785841770863336\n",
      "Gradient Descent(1717/4999): loss=0.5785835040200141\n",
      "Gradient Descent(1718/4999): loss=0.5785828320321212\n",
      "Gradient Descent(1719/4999): loss=0.5785821611190346\n",
      "Gradient Descent(1720/4999): loss=0.5785814912771479\n",
      "Gradient Descent(1721/4999): loss=0.5785808225028679\n",
      "Gradient Descent(1722/4999): loss=0.5785801547926158\n",
      "Gradient Descent(1723/4999): loss=0.5785794881428264\n",
      "Gradient Descent(1724/4999): loss=0.5785788225499476\n",
      "Gradient Descent(1725/4999): loss=0.5785781580104418\n",
      "Gradient Descent(1726/4999): loss=0.5785774945207836\n",
      "Gradient Descent(1727/4999): loss=0.5785768320774622\n",
      "Gradient Descent(1728/4999): loss=0.5785761706769801\n",
      "Gradient Descent(1729/4999): loss=0.5785755103158523\n",
      "Gradient Descent(1730/4999): loss=0.5785748509906081\n",
      "Gradient Descent(1731/4999): loss=0.5785741926977894\n",
      "Gradient Descent(1732/4999): loss=0.5785735354339516\n",
      "Gradient Descent(1733/4999): loss=0.5785728791956631\n",
      "Gradient Descent(1734/4999): loss=0.5785722239795055\n",
      "Gradient Descent(1735/4999): loss=0.5785715697820736\n",
      "Gradient Descent(1736/4999): loss=0.5785709165999748\n",
      "Gradient Descent(1737/4999): loss=0.5785702644298295\n",
      "Gradient Descent(1738/4999): loss=0.5785696132682715\n",
      "Gradient Descent(1739/4999): loss=0.5785689631119468\n",
      "Gradient Descent(1740/4999): loss=0.5785683139575146\n",
      "Gradient Descent(1741/4999): loss=0.5785676658016469\n",
      "Gradient Descent(1742/4999): loss=0.5785670186410276\n",
      "Gradient Descent(1743/4999): loss=0.5785663724723543\n",
      "Gradient Descent(1744/4999): loss=0.5785657272923367\n",
      "Gradient Descent(1745/4999): loss=0.5785650830976972\n",
      "Gradient Descent(1746/4999): loss=0.5785644398851704\n",
      "Gradient Descent(1747/4999): loss=0.5785637976515033\n",
      "Gradient Descent(1748/4999): loss=0.5785631563934559\n",
      "Gradient Descent(1749/4999): loss=0.5785625161078001\n",
      "Gradient Descent(1750/4999): loss=0.57856187679132\n",
      "Gradient Descent(1751/4999): loss=0.5785612384408122\n",
      "Gradient Descent(1752/4999): loss=0.5785606010530853\n",
      "Gradient Descent(1753/4999): loss=0.5785599646249604\n",
      "Gradient Descent(1754/4999): loss=0.5785593291532704\n",
      "Gradient Descent(1755/4999): loss=0.5785586946348601\n",
      "Gradient Descent(1756/4999): loss=0.578558061066587\n",
      "Gradient Descent(1757/4999): loss=0.5785574284453197\n",
      "Gradient Descent(1758/4999): loss=0.5785567967679393\n",
      "Gradient Descent(1759/4999): loss=0.5785561660313387\n",
      "Gradient Descent(1760/4999): loss=0.5785555362324224\n",
      "Gradient Descent(1761/4999): loss=0.5785549073681071\n",
      "Gradient Descent(1762/4999): loss=0.5785542794353206\n",
      "Gradient Descent(1763/4999): loss=0.5785536524310027\n",
      "Gradient Descent(1764/4999): loss=0.5785530263521054\n",
      "Gradient Descent(1765/4999): loss=0.5785524011955913\n",
      "Gradient Descent(1766/4999): loss=0.5785517769584352\n",
      "Gradient Descent(1767/4999): loss=0.5785511536376231\n",
      "Gradient Descent(1768/4999): loss=0.5785505312301528\n",
      "Gradient Descent(1769/4999): loss=0.5785499097330332\n",
      "Gradient Descent(1770/4999): loss=0.578549289143285\n",
      "Gradient Descent(1771/4999): loss=0.5785486694579395\n",
      "Gradient Descent(1772/4999): loss=0.5785480506740396\n",
      "Gradient Descent(1773/4999): loss=0.5785474327886402\n",
      "Gradient Descent(1774/4999): loss=0.578546815798806\n",
      "Gradient Descent(1775/4999): loss=0.5785461997016141\n",
      "Gradient Descent(1776/4999): loss=0.5785455844941519\n",
      "Gradient Descent(1777/4999): loss=0.5785449701735184\n",
      "Gradient Descent(1778/4999): loss=0.5785443567368231\n",
      "Gradient Descent(1779/4999): loss=0.5785437441811871\n",
      "Gradient Descent(1780/4999): loss=0.5785431325037418\n",
      "Gradient Descent(1781/4999): loss=0.5785425217016299\n",
      "Gradient Descent(1782/4999): loss=0.5785419117720049\n",
      "Gradient Descent(1783/4999): loss=0.5785413027120309\n",
      "Gradient Descent(1784/4999): loss=0.5785406945188828\n",
      "Gradient Descent(1785/4999): loss=0.5785400871897468\n",
      "Gradient Descent(1786/4999): loss=0.5785394807218188\n",
      "Gradient Descent(1787/4999): loss=0.5785388751123063\n",
      "Gradient Descent(1788/4999): loss=0.5785382703584264\n",
      "Gradient Descent(1789/4999): loss=0.5785376664574079\n",
      "Gradient Descent(1790/4999): loss=0.578537063406489\n",
      "Gradient Descent(1791/4999): loss=0.5785364612029193\n",
      "Gradient Descent(1792/4999): loss=0.5785358598439583\n",
      "Gradient Descent(1793/4999): loss=0.5785352593268759\n",
      "Gradient Descent(1794/4999): loss=0.5785346596489527\n",
      "Gradient Descent(1795/4999): loss=0.5785340608074792\n",
      "Gradient Descent(1796/4999): loss=0.5785334627997564\n",
      "Gradient Descent(1797/4999): loss=0.5785328656230954\n",
      "Gradient Descent(1798/4999): loss=0.5785322692748177\n",
      "Gradient Descent(1799/4999): loss=0.5785316737522549\n",
      "Gradient Descent(1800/4999): loss=0.5785310790527485\n",
      "Gradient Descent(1801/4999): loss=0.57853048517365\n",
      "Gradient Descent(1802/4999): loss=0.5785298921123216\n",
      "Gradient Descent(1803/4999): loss=0.5785292998661348\n",
      "Gradient Descent(1804/4999): loss=0.5785287084324714\n",
      "Gradient Descent(1805/4999): loss=0.5785281178087229\n",
      "Gradient Descent(1806/4999): loss=0.5785275279922908\n",
      "Gradient Descent(1807/4999): loss=0.5785269389805864\n",
      "Gradient Descent(1808/4999): loss=0.5785263507710309\n",
      "Gradient Descent(1809/4999): loss=0.5785257633610551\n",
      "Gradient Descent(1810/4999): loss=0.5785251767480997\n",
      "Gradient Descent(1811/4999): loss=0.5785245909296148\n",
      "Gradient Descent(1812/4999): loss=0.5785240059030606\n",
      "Gradient Descent(1813/4999): loss=0.5785234216659065\n",
      "Gradient Descent(1814/4999): loss=0.5785228382156318\n",
      "Gradient Descent(1815/4999): loss=0.5785222555497249\n",
      "Gradient Descent(1816/4999): loss=0.5785216736656841\n",
      "Gradient Descent(1817/4999): loss=0.578521092561017\n",
      "Gradient Descent(1818/4999): loss=0.5785205122332405\n",
      "Gradient Descent(1819/4999): loss=0.5785199326798813\n",
      "Gradient Descent(1820/4999): loss=0.5785193538984751\n",
      "Gradient Descent(1821/4999): loss=0.5785187758865669\n",
      "Gradient Descent(1822/4999): loss=0.578518198641711\n",
      "Gradient Descent(1823/4999): loss=0.5785176221614712\n",
      "Gradient Descent(1824/4999): loss=0.5785170464434203\n",
      "Gradient Descent(1825/4999): loss=0.5785164714851401\n",
      "Gradient Descent(1826/4999): loss=0.5785158972842218\n",
      "Gradient Descent(1827/4999): loss=0.5785153238382656\n",
      "Gradient Descent(1828/4999): loss=0.578514751144881\n",
      "Gradient Descent(1829/4999): loss=0.5785141792016862\n",
      "Gradient Descent(1830/4999): loss=0.5785136080063086\n",
      "Gradient Descent(1831/4999): loss=0.5785130375563841\n",
      "Gradient Descent(1832/4999): loss=0.578512467849558\n",
      "Gradient Descent(1833/4999): loss=0.5785118988834844\n",
      "Gradient Descent(1834/4999): loss=0.5785113306558265\n",
      "Gradient Descent(1835/4999): loss=0.5785107631642556\n",
      "Gradient Descent(1836/4999): loss=0.5785101964064526\n",
      "Gradient Descent(1837/4999): loss=0.5785096303801066\n",
      "Gradient Descent(1838/4999): loss=0.5785090650829156\n",
      "Gradient Descent(1839/4999): loss=0.5785085005125861\n",
      "Gradient Descent(1840/4999): loss=0.5785079366668336\n",
      "Gradient Descent(1841/4999): loss=0.578507373543382\n",
      "Gradient Descent(1842/4999): loss=0.5785068111399639\n",
      "Gradient Descent(1843/4999): loss=0.57850624945432\n",
      "Gradient Descent(1844/4999): loss=0.5785056884842003\n",
      "Gradient Descent(1845/4999): loss=0.5785051282273627\n",
      "Gradient Descent(1846/4999): loss=0.5785045686815735\n",
      "Gradient Descent(1847/4999): loss=0.5785040098446078\n",
      "Gradient Descent(1848/4999): loss=0.5785034517142489\n",
      "Gradient Descent(1849/4999): loss=0.5785028942882882\n",
      "Gradient Descent(1850/4999): loss=0.5785023375645261\n",
      "Gradient Descent(1851/4999): loss=0.5785017815407705\n",
      "Gradient Descent(1852/4999): loss=0.5785012262148379\n",
      "Gradient Descent(1853/4999): loss=0.578500671584553\n",
      "Gradient Descent(1854/4999): loss=0.5785001176477488\n",
      "Gradient Descent(1855/4999): loss=0.5784995644022662\n",
      "Gradient Descent(1856/4999): loss=0.5784990118459546\n",
      "Gradient Descent(1857/4999): loss=0.5784984599766712\n",
      "Gradient Descent(1858/4999): loss=0.5784979087922811\n",
      "Gradient Descent(1859/4999): loss=0.5784973582906578\n",
      "Gradient Descent(1860/4999): loss=0.5784968084696829\n",
      "Gradient Descent(1861/4999): loss=0.5784962593272452\n",
      "Gradient Descent(1862/4999): loss=0.5784957108612425\n",
      "Gradient Descent(1863/4999): loss=0.5784951630695794\n",
      "Gradient Descent(1864/4999): loss=0.5784946159501694\n",
      "Gradient Descent(1865/4999): loss=0.5784940695009332\n",
      "Gradient Descent(1866/4999): loss=0.5784935237197995\n",
      "Gradient Descent(1867/4999): loss=0.5784929786047045\n",
      "Gradient Descent(1868/4999): loss=0.5784924341535929\n",
      "Gradient Descent(1869/4999): loss=0.5784918903644162\n",
      "Gradient Descent(1870/4999): loss=0.5784913472351345\n",
      "Gradient Descent(1871/4999): loss=0.5784908047637144\n",
      "Gradient Descent(1872/4999): loss=0.5784902629481317\n",
      "Gradient Descent(1873/4999): loss=0.5784897217863681\n",
      "Gradient Descent(1874/4999): loss=0.5784891812764142\n",
      "Gradient Descent(1875/4999): loss=0.5784886414162677\n",
      "Gradient Descent(1876/4999): loss=0.5784881022039332\n",
      "Gradient Descent(1877/4999): loss=0.5784875636374238\n",
      "Gradient Descent(1878/4999): loss=0.5784870257147597\n",
      "Gradient Descent(1879/4999): loss=0.578486488433968\n",
      "Gradient Descent(1880/4999): loss=0.5784859517930839\n",
      "Gradient Descent(1881/4999): loss=0.5784854157901494\n",
      "Gradient Descent(1882/4999): loss=0.5784848804232146\n",
      "Gradient Descent(1883/4999): loss=0.5784843456903361\n",
      "Gradient Descent(1884/4999): loss=0.5784838115895782\n",
      "Gradient Descent(1885/4999): loss=0.5784832781190122\n",
      "Gradient Descent(1886/4999): loss=0.578482745276717\n",
      "Gradient Descent(1887/4999): loss=0.5784822130607785\n",
      "Gradient Descent(1888/4999): loss=0.5784816814692898\n",
      "Gradient Descent(1889/4999): loss=0.5784811505003508\n",
      "Gradient Descent(1890/4999): loss=0.5784806201520689\n",
      "Gradient Descent(1891/4999): loss=0.578480090422559\n",
      "Gradient Descent(1892/4999): loss=0.5784795613099419\n",
      "Gradient Descent(1893/4999): loss=0.5784790328123466\n",
      "Gradient Descent(1894/4999): loss=0.5784785049279083\n",
      "Gradient Descent(1895/4999): loss=0.5784779776547696\n",
      "Gradient Descent(1896/4999): loss=0.5784774509910798\n",
      "Gradient Descent(1897/4999): loss=0.5784769249349955\n",
      "Gradient Descent(1898/4999): loss=0.5784763994846797\n",
      "Gradient Descent(1899/4999): loss=0.5784758746383026\n",
      "Gradient Descent(1900/4999): loss=0.5784753503940411\n",
      "Gradient Descent(1901/4999): loss=0.5784748267500791\n",
      "Gradient Descent(1902/4999): loss=0.5784743037046071\n",
      "Gradient Descent(1903/4999): loss=0.5784737812558222\n",
      "Gradient Descent(1904/4999): loss=0.578473259401929\n",
      "Gradient Descent(1905/4999): loss=0.5784727381411378\n",
      "Gradient Descent(1906/4999): loss=0.5784722174716661\n",
      "Gradient Descent(1907/4999): loss=0.5784716973917383\n",
      "Gradient Descent(1908/4999): loss=0.5784711778995847\n",
      "Gradient Descent(1909/4999): loss=0.578470658993443\n",
      "Gradient Descent(1910/4999): loss=0.578470140671557\n",
      "Gradient Descent(1911/4999): loss=0.5784696229321772\n",
      "Gradient Descent(1912/4999): loss=0.5784691057735607\n",
      "Gradient Descent(1913/4999): loss=0.5784685891939708\n",
      "Gradient Descent(1914/4999): loss=0.5784680731916777\n",
      "Gradient Descent(1915/4999): loss=0.578467557764958\n",
      "Gradient Descent(1916/4999): loss=0.5784670429120939\n",
      "Gradient Descent(1917/4999): loss=0.5784665286313754\n",
      "Gradient Descent(1918/4999): loss=0.578466014921098\n",
      "Gradient Descent(1919/4999): loss=0.5784655017795635\n",
      "Gradient Descent(1920/4999): loss=0.5784649892050804\n",
      "Gradient Descent(1921/4999): loss=0.5784644771959632\n",
      "Gradient Descent(1922/4999): loss=0.5784639657505329\n",
      "Gradient Descent(1923/4999): loss=0.578463454867117\n",
      "Gradient Descent(1924/4999): loss=0.5784629445440486\n",
      "Gradient Descent(1925/4999): loss=0.5784624347796671\n",
      "Gradient Descent(1926/4999): loss=0.5784619255723189\n",
      "Gradient Descent(1927/4999): loss=0.5784614169203555\n",
      "Gradient Descent(1928/4999): loss=0.578460908822135\n",
      "Gradient Descent(1929/4999): loss=0.5784604012760218\n",
      "Gradient Descent(1930/4999): loss=0.5784598942803862\n",
      "Gradient Descent(1931/4999): loss=0.5784593878336046\n",
      "Gradient Descent(1932/4999): loss=0.5784588819340593\n",
      "Gradient Descent(1933/4999): loss=0.5784583765801387\n",
      "Gradient Descent(1934/4999): loss=0.5784578717702371\n",
      "Gradient Descent(1935/4999): loss=0.5784573675027551\n",
      "Gradient Descent(1936/4999): loss=0.5784568637760993\n",
      "Gradient Descent(1937/4999): loss=0.5784563605886812\n",
      "Gradient Descent(1938/4999): loss=0.5784558579389196\n",
      "Gradient Descent(1939/4999): loss=0.5784553558252383\n",
      "Gradient Descent(1940/4999): loss=0.5784548542460672\n",
      "Gradient Descent(1941/4999): loss=0.5784543531998418\n",
      "Gradient Descent(1942/4999): loss=0.5784538526850043\n",
      "Gradient Descent(1943/4999): loss=0.5784533527000013\n",
      "Gradient Descent(1944/4999): loss=0.5784528532432862\n",
      "Gradient Descent(1945/4999): loss=0.5784523543133178\n",
      "Gradient Descent(1946/4999): loss=0.5784518559085607\n",
      "Gradient Descent(1947/4999): loss=0.578451358027485\n",
      "Gradient Descent(1948/4999): loss=0.5784508606685668\n",
      "Gradient Descent(1949/4999): loss=0.5784503638302874\n",
      "Gradient Descent(1950/4999): loss=0.5784498675111344\n",
      "Gradient Descent(1951/4999): loss=0.5784493717096002\n",
      "Gradient Descent(1952/4999): loss=0.5784488764241837\n",
      "Gradient Descent(1953/4999): loss=0.5784483816533883\n",
      "Gradient Descent(1954/4999): loss=0.578447887395724\n",
      "Gradient Descent(1955/4999): loss=0.5784473936497054\n",
      "Gradient Descent(1956/4999): loss=0.5784469004138534\n",
      "Gradient Descent(1957/4999): loss=0.5784464076866939\n",
      "Gradient Descent(1958/4999): loss=0.5784459154667585\n",
      "Gradient Descent(1959/4999): loss=0.5784454237525839\n",
      "Gradient Descent(1960/4999): loss=0.5784449325427126\n",
      "Gradient Descent(1961/4999): loss=0.5784444418356922\n",
      "Gradient Descent(1962/4999): loss=0.5784439516300759\n",
      "Gradient Descent(1963/4999): loss=0.5784434619244222\n",
      "Gradient Descent(1964/4999): loss=0.5784429727172948\n",
      "Gradient Descent(1965/4999): loss=0.5784424840072628\n",
      "Gradient Descent(1966/4999): loss=0.5784419957929008\n",
      "Gradient Descent(1967/4999): loss=0.5784415080727883\n",
      "Gradient Descent(1968/4999): loss=0.5784410208455103\n",
      "Gradient Descent(1969/4999): loss=0.5784405341096569\n",
      "Gradient Descent(1970/4999): loss=0.5784400478638234\n",
      "Gradient Descent(1971/4999): loss=0.5784395621066105\n",
      "Gradient Descent(1972/4999): loss=0.578439076836624\n",
      "Gradient Descent(1973/4999): loss=0.5784385920524744\n",
      "Gradient Descent(1974/4999): loss=0.5784381077527783\n",
      "Gradient Descent(1975/4999): loss=0.5784376239361563\n",
      "Gradient Descent(1976/4999): loss=0.5784371406012349\n",
      "Gradient Descent(1977/4999): loss=0.578436657746645\n",
      "Gradient Descent(1978/4999): loss=0.5784361753710234\n",
      "Gradient Descent(1979/4999): loss=0.5784356934730114\n",
      "Gradient Descent(1980/4999): loss=0.5784352120512551\n",
      "Gradient Descent(1981/4999): loss=0.5784347311044061\n",
      "Gradient Descent(1982/4999): loss=0.5784342506311206\n",
      "Gradient Descent(1983/4999): loss=0.5784337706300601\n",
      "Gradient Descent(1984/4999): loss=0.5784332910998906\n",
      "Gradient Descent(1985/4999): loss=0.5784328120392834\n",
      "Gradient Descent(1986/4999): loss=0.5784323334469144\n",
      "Gradient Descent(1987/4999): loss=0.5784318553214648\n",
      "Gradient Descent(1988/4999): loss=0.5784313776616201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1989/4999): loss=0.5784309004660712\n",
      "Gradient Descent(1990/4999): loss=0.5784304237335132\n",
      "Gradient Descent(1991/4999): loss=0.5784299474626464\n",
      "Gradient Descent(1992/4999): loss=0.5784294716521762\n",
      "Gradient Descent(1993/4999): loss=0.578428996300812\n",
      "Gradient Descent(1994/4999): loss=0.5784285214072686\n",
      "Gradient Descent(1995/4999): loss=0.5784280469702653\n",
      "Gradient Descent(1996/4999): loss=0.5784275729885258\n",
      "Gradient Descent(1997/4999): loss=0.578427099460779\n",
      "Gradient Descent(1998/4999): loss=0.5784266263857583\n",
      "Gradient Descent(1999/4999): loss=0.5784261537622013\n",
      "Gradient Descent(2000/4999): loss=0.5784256815888511\n",
      "Gradient Descent(2001/4999): loss=0.5784252098644548\n",
      "Gradient Descent(2002/4999): loss=0.5784247385877643\n",
      "Gradient Descent(2003/4999): loss=0.5784242677575361\n",
      "Gradient Descent(2004/4999): loss=0.5784237973725311\n",
      "Gradient Descent(2005/4999): loss=0.5784233274315149\n",
      "Gradient Descent(2006/4999): loss=0.5784228579332577\n",
      "Gradient Descent(2007/4999): loss=0.578422388876534\n",
      "Gradient Descent(2008/4999): loss=0.5784219202601228\n",
      "Gradient Descent(2009/4999): loss=0.578421452082808\n",
      "Gradient Descent(2010/4999): loss=0.5784209843433775\n",
      "Gradient Descent(2011/4999): loss=0.5784205170406237\n",
      "Gradient Descent(2012/4999): loss=0.5784200501733436\n",
      "Gradient Descent(2013/4999): loss=0.5784195837403386\n",
      "Gradient Descent(2014/4999): loss=0.5784191177404144\n",
      "Gradient Descent(2015/4999): loss=0.5784186521723811\n",
      "Gradient Descent(2016/4999): loss=0.5784181870350529\n",
      "Gradient Descent(2017/4999): loss=0.578417722327249\n",
      "Gradient Descent(2018/4999): loss=0.5784172580477923\n",
      "Gradient Descent(2019/4999): loss=0.5784167941955104\n",
      "Gradient Descent(2020/4999): loss=0.5784163307692348\n",
      "Gradient Descent(2021/4999): loss=0.5784158677678016\n",
      "Gradient Descent(2022/4999): loss=0.5784154051900512\n",
      "Gradient Descent(2023/4999): loss=0.5784149430348279\n",
      "Gradient Descent(2024/4999): loss=0.5784144813009807\n",
      "Gradient Descent(2025/4999): loss=0.5784140199873625\n",
      "Gradient Descent(2026/4999): loss=0.5784135590928299\n",
      "Gradient Descent(2027/4999): loss=0.5784130986162449\n",
      "Gradient Descent(2028/4999): loss=0.5784126385564728\n",
      "Gradient Descent(2029/4999): loss=0.578412178912383\n",
      "Gradient Descent(2030/4999): loss=0.5784117196828493\n",
      "Gradient Descent(2031/4999): loss=0.5784112608667497\n",
      "Gradient Descent(2032/4999): loss=0.5784108024629662\n",
      "Gradient Descent(2033/4999): loss=0.5784103444703845\n",
      "Gradient Descent(2034/4999): loss=0.5784098868878953\n",
      "Gradient Descent(2035/4999): loss=0.5784094297143924\n",
      "Gradient Descent(2036/4999): loss=0.5784089729487738\n",
      "Gradient Descent(2037/4999): loss=0.578408516589942\n",
      "Gradient Descent(2038/4999): loss=0.5784080606368032\n",
      "Gradient Descent(2039/4999): loss=0.5784076050882674\n",
      "Gradient Descent(2040/4999): loss=0.5784071499432489\n",
      "Gradient Descent(2041/4999): loss=0.5784066952006659\n",
      "Gradient Descent(2042/4999): loss=0.5784062408594404\n",
      "Gradient Descent(2043/4999): loss=0.5784057869184984\n",
      "Gradient Descent(2044/4999): loss=0.5784053333767699\n",
      "Gradient Descent(2045/4999): loss=0.5784048802331885\n",
      "Gradient Descent(2046/4999): loss=0.5784044274866922\n",
      "Gradient Descent(2047/4999): loss=0.5784039751362222\n",
      "Gradient Descent(2048/4999): loss=0.5784035231807242\n",
      "Gradient Descent(2049/4999): loss=0.5784030716191473\n",
      "Gradient Descent(2050/4999): loss=0.5784026204504445\n",
      "Gradient Descent(2051/4999): loss=0.5784021696735728\n",
      "Gradient Descent(2052/4999): loss=0.5784017192874928\n",
      "Gradient Descent(2053/4999): loss=0.5784012692911691\n",
      "Gradient Descent(2054/4999): loss=0.5784008196835697\n",
      "Gradient Descent(2055/4999): loss=0.5784003704636665\n",
      "Gradient Descent(2056/4999): loss=0.5783999216304355\n",
      "Gradient Descent(2057/4999): loss=0.5783994731828558\n",
      "Gradient Descent(2058/4999): loss=0.5783990251199106\n",
      "Gradient Descent(2059/4999): loss=0.578398577440587\n",
      "Gradient Descent(2060/4999): loss=0.5783981301438749\n",
      "Gradient Descent(2061/4999): loss=0.5783976832287688\n",
      "Gradient Descent(2062/4999): loss=0.5783972366942666\n",
      "Gradient Descent(2063/4999): loss=0.5783967905393695\n",
      "Gradient Descent(2064/4999): loss=0.5783963447630825\n",
      "Gradient Descent(2065/4999): loss=0.5783958993644146\n",
      "Gradient Descent(2066/4999): loss=0.5783954543423777\n",
      "Gradient Descent(2067/4999): loss=0.5783950096959878\n",
      "Gradient Descent(2068/4999): loss=0.578394565424264\n",
      "Gradient Descent(2069/4999): loss=0.5783941215262298\n",
      "Gradient Descent(2070/4999): loss=0.5783936780009112\n",
      "Gradient Descent(2071/4999): loss=0.5783932348473384\n",
      "Gradient Descent(2072/4999): loss=0.5783927920645447\n",
      "Gradient Descent(2073/4999): loss=0.5783923496515674\n",
      "Gradient Descent(2074/4999): loss=0.5783919076074469\n",
      "Gradient Descent(2075/4999): loss=0.578391465931227\n",
      "Gradient Descent(2076/4999): loss=0.5783910246219554\n",
      "Gradient Descent(2077/4999): loss=0.5783905836786827\n",
      "Gradient Descent(2078/4999): loss=0.5783901431004632\n",
      "Gradient Descent(2079/4999): loss=0.5783897028863549\n",
      "Gradient Descent(2080/4999): loss=0.5783892630354185\n",
      "Gradient Descent(2081/4999): loss=0.5783888235467189\n",
      "Gradient Descent(2082/4999): loss=0.5783883844193238\n",
      "Gradient Descent(2083/4999): loss=0.5783879456523042\n",
      "Gradient Descent(2084/4999): loss=0.5783875072447352\n",
      "Gradient Descent(2085/4999): loss=0.5783870691956945\n",
      "Gradient Descent(2086/4999): loss=0.5783866315042631\n",
      "Gradient Descent(2087/4999): loss=0.5783861941695257\n",
      "Gradient Descent(2088/4999): loss=0.5783857571905705\n",
      "Gradient Descent(2089/4999): loss=0.5783853205664884\n",
      "Gradient Descent(2090/4999): loss=0.5783848842963739\n",
      "Gradient Descent(2091/4999): loss=0.5783844483793247\n",
      "Gradient Descent(2092/4999): loss=0.5783840128144416\n",
      "Gradient Descent(2093/4999): loss=0.5783835776008289\n",
      "Gradient Descent(2094/4999): loss=0.578383142737594\n",
      "Gradient Descent(2095/4999): loss=0.5783827082238474\n",
      "Gradient Descent(2096/4999): loss=0.5783822740587031\n",
      "Gradient Descent(2097/4999): loss=0.5783818402412783\n",
      "Gradient Descent(2098/4999): loss=0.5783814067706927\n",
      "Gradient Descent(2099/4999): loss=0.5783809736460699\n",
      "Gradient Descent(2100/4999): loss=0.5783805408665363\n",
      "Gradient Descent(2101/4999): loss=0.5783801084312216\n",
      "Gradient Descent(2102/4999): loss=0.5783796763392587\n",
      "Gradient Descent(2103/4999): loss=0.5783792445897833\n",
      "Gradient Descent(2104/4999): loss=0.5783788131819345\n",
      "Gradient Descent(2105/4999): loss=0.578378382114854\n",
      "Gradient Descent(2106/4999): loss=0.5783779513876875\n",
      "Gradient Descent(2107/4999): loss=0.578377520999583\n",
      "Gradient Descent(2108/4999): loss=0.5783770909496916\n",
      "Gradient Descent(2109/4999): loss=0.5783766612371678\n",
      "Gradient Descent(2110/4999): loss=0.5783762318611688\n",
      "Gradient Descent(2111/4999): loss=0.5783758028208552\n",
      "Gradient Descent(2112/4999): loss=0.5783753741153902\n",
      "Gradient Descent(2113/4999): loss=0.5783749457439401\n",
      "Gradient Descent(2114/4999): loss=0.5783745177056745\n",
      "Gradient Descent(2115/4999): loss=0.5783740899997655\n",
      "Gradient Descent(2116/4999): loss=0.5783736626253885\n",
      "Gradient Descent(2117/4999): loss=0.5783732355817218\n",
      "Gradient Descent(2118/4999): loss=0.5783728088679464\n",
      "Gradient Descent(2119/4999): loss=0.5783723824832466\n",
      "Gradient Descent(2120/4999): loss=0.5783719564268094\n",
      "Gradient Descent(2121/4999): loss=0.5783715306978248\n",
      "Gradient Descent(2122/4999): loss=0.5783711052954853\n",
      "Gradient Descent(2123/4999): loss=0.5783706802189871\n",
      "Gradient Descent(2124/4999): loss=0.5783702554675284\n",
      "Gradient Descent(2125/4999): loss=0.5783698310403108\n",
      "Gradient Descent(2126/4999): loss=0.5783694069365386\n",
      "Gradient Descent(2127/4999): loss=0.578368983155419\n",
      "Gradient Descent(2128/4999): loss=0.5783685596961619\n",
      "Gradient Descent(2129/4999): loss=0.57836813655798\n",
      "Gradient Descent(2130/4999): loss=0.5783677137400891\n",
      "Gradient Descent(2131/4999): loss=0.5783672912417075\n",
      "Gradient Descent(2132/4999): loss=0.5783668690620564\n",
      "Gradient Descent(2133/4999): loss=0.5783664472003595\n",
      "Gradient Descent(2134/4999): loss=0.5783660256558439\n",
      "Gradient Descent(2135/4999): loss=0.578365604427739\n",
      "Gradient Descent(2136/4999): loss=0.5783651835152767\n",
      "Gradient Descent(2137/4999): loss=0.5783647629176923\n",
      "Gradient Descent(2138/4999): loss=0.5783643426342233\n",
      "Gradient Descent(2139/4999): loss=0.5783639226641102\n",
      "Gradient Descent(2140/4999): loss=0.578363503006596\n",
      "Gradient Descent(2141/4999): loss=0.5783630836609261\n",
      "Gradient Descent(2142/4999): loss=0.5783626646263498\n",
      "Gradient Descent(2143/4999): loss=0.5783622459021176\n",
      "Gradient Descent(2144/4999): loss=0.5783618274874833\n",
      "Gradient Descent(2145/4999): loss=0.5783614093817037\n",
      "Gradient Descent(2146/4999): loss=0.5783609915840374\n",
      "Gradient Descent(2147/4999): loss=0.5783605740937466\n",
      "Gradient Descent(2148/4999): loss=0.5783601569100952\n",
      "Gradient Descent(2149/4999): loss=0.5783597400323504\n",
      "Gradient Descent(2150/4999): loss=0.5783593234597816\n",
      "Gradient Descent(2151/4999): loss=0.5783589071916609\n",
      "Gradient Descent(2152/4999): loss=0.5783584912272632\n",
      "Gradient Descent(2153/4999): loss=0.5783580755658655\n",
      "Gradient Descent(2154/4999): loss=0.5783576602067477\n",
      "Gradient Descent(2155/4999): loss=0.5783572451491925\n",
      "Gradient Descent(2156/4999): loss=0.5783568303924844\n",
      "Gradient Descent(2157/4999): loss=0.578356415935911\n",
      "Gradient Descent(2158/4999): loss=0.5783560017787625\n",
      "Gradient Descent(2159/4999): loss=0.5783555879203309\n",
      "Gradient Descent(2160/4999): loss=0.5783551743599117\n",
      "Gradient Descent(2161/4999): loss=0.5783547610968021\n",
      "Gradient Descent(2162/4999): loss=0.578354348130302\n",
      "Gradient Descent(2163/4999): loss=0.5783539354597138\n",
      "Gradient Descent(2164/4999): loss=0.5783535230843428\n",
      "Gradient Descent(2165/4999): loss=0.5783531110034961\n",
      "Gradient Descent(2166/4999): loss=0.5783526992164835\n",
      "Gradient Descent(2167/4999): loss=0.5783522877226169\n",
      "Gradient Descent(2168/4999): loss=0.5783518765212117\n",
      "Gradient Descent(2169/4999): loss=0.5783514656115843\n",
      "Gradient Descent(2170/4999): loss=0.5783510549930544\n",
      "Gradient Descent(2171/4999): loss=0.5783506446649441\n",
      "Gradient Descent(2172/4999): loss=0.5783502346265774\n",
      "Gradient Descent(2173/4999): loss=0.578349824877281\n",
      "Gradient Descent(2174/4999): loss=0.578349415416384\n",
      "Gradient Descent(2175/4999): loss=0.5783490062432176\n",
      "Gradient Descent(2176/4999): loss=0.578348597357116\n",
      "Gradient Descent(2177/4999): loss=0.5783481887574147\n",
      "Gradient Descent(2178/4999): loss=0.5783477804434525\n",
      "Gradient Descent(2179/4999): loss=0.5783473724145698\n",
      "Gradient Descent(2180/4999): loss=0.57834696467011\n",
      "Gradient Descent(2181/4999): loss=0.5783465572094183\n",
      "Gradient Descent(2182/4999): loss=0.5783461500318425\n",
      "Gradient Descent(2183/4999): loss=0.5783457431367324\n",
      "Gradient Descent(2184/4999): loss=0.5783453365234401\n",
      "Gradient Descent(2185/4999): loss=0.5783449301913205\n",
      "Gradient Descent(2186/4999): loss=0.57834452413973\n",
      "Gradient Descent(2187/4999): loss=0.5783441183680281\n",
      "Gradient Descent(2188/4999): loss=0.5783437128755756\n",
      "Gradient Descent(2189/4999): loss=0.5783433076617361\n",
      "Gradient Descent(2190/4999): loss=0.5783429027258755\n",
      "Gradient Descent(2191/4999): loss=0.5783424980673617\n",
      "Gradient Descent(2192/4999): loss=0.578342093685565\n",
      "Gradient Descent(2193/4999): loss=0.5783416895798577\n",
      "Gradient Descent(2194/4999): loss=0.5783412857496141\n",
      "Gradient Descent(2195/4999): loss=0.5783408821942113\n",
      "Gradient Descent(2196/4999): loss=0.5783404789130284\n",
      "Gradient Descent(2197/4999): loss=0.5783400759054459\n",
      "Gradient Descent(2198/4999): loss=0.5783396731708477\n",
      "Gradient Descent(2199/4999): loss=0.5783392707086189\n",
      "Gradient Descent(2200/4999): loss=0.5783388685181473\n",
      "Gradient Descent(2201/4999): loss=0.5783384665988224\n",
      "Gradient Descent(2202/4999): loss=0.5783380649500361\n",
      "Gradient Descent(2203/4999): loss=0.5783376635711825\n",
      "Gradient Descent(2204/4999): loss=0.5783372624616577\n",
      "Gradient Descent(2205/4999): loss=0.5783368616208597\n",
      "Gradient Descent(2206/4999): loss=0.5783364610481889\n",
      "Gradient Descent(2207/4999): loss=0.5783360607430477\n",
      "Gradient Descent(2208/4999): loss=0.5783356607048404\n",
      "Gradient Descent(2209/4999): loss=0.5783352609329737\n",
      "Gradient Descent(2210/4999): loss=0.5783348614268562\n",
      "Gradient Descent(2211/4999): loss=0.5783344621858985\n",
      "Gradient Descent(2212/4999): loss=0.5783340632095132\n",
      "Gradient Descent(2213/4999): loss=0.5783336644971154\n",
      "Gradient Descent(2214/4999): loss=0.5783332660481215\n",
      "Gradient Descent(2215/4999): loss=0.5783328678619506\n",
      "Gradient Descent(2216/4999): loss=0.5783324699380232\n",
      "Gradient Descent(2217/4999): loss=0.5783320722757626\n",
      "Gradient Descent(2218/4999): loss=0.578331674874593\n",
      "Gradient Descent(2219/4999): loss=0.5783312777339419\n",
      "Gradient Descent(2220/4999): loss=0.5783308808532378\n",
      "Gradient Descent(2221/4999): loss=0.5783304842319115\n",
      "Gradient Descent(2222/4999): loss=0.5783300878693958\n",
      "Gradient Descent(2223/4999): loss=0.5783296917651254\n",
      "Gradient Descent(2224/4999): loss=0.5783292959185372\n",
      "Gradient Descent(2225/4999): loss=0.5783289003290698\n",
      "Gradient Descent(2226/4999): loss=0.5783285049961636\n",
      "Gradient Descent(2227/4999): loss=0.5783281099192613\n",
      "Gradient Descent(2228/4999): loss=0.5783277150978076\n",
      "Gradient Descent(2229/4999): loss=0.5783273205312484\n",
      "Gradient Descent(2230/4999): loss=0.5783269262190323\n",
      "Gradient Descent(2231/4999): loss=0.5783265321606094\n",
      "Gradient Descent(2232/4999): loss=0.5783261383554318\n",
      "Gradient Descent(2233/4999): loss=0.5783257448029536\n",
      "Gradient Descent(2234/4999): loss=0.5783253515026306\n",
      "Gradient Descent(2235/4999): loss=0.5783249584539205\n",
      "Gradient Descent(2236/4999): loss=0.5783245656562831\n",
      "Gradient Descent(2237/4999): loss=0.5783241731091797\n",
      "Gradient Descent(2238/4999): loss=0.5783237808120738\n",
      "Gradient Descent(2239/4999): loss=0.5783233887644306\n",
      "Gradient Descent(2240/4999): loss=0.5783229969657172\n",
      "Gradient Descent(2241/4999): loss=0.5783226054154021\n",
      "Gradient Descent(2242/4999): loss=0.5783222141129566\n",
      "Gradient Descent(2243/4999): loss=0.5783218230578526\n",
      "Gradient Descent(2244/4999): loss=0.5783214322495648\n",
      "Gradient Descent(2245/4999): loss=0.5783210416875695\n",
      "Gradient Descent(2246/4999): loss=0.5783206513713443\n",
      "Gradient Descent(2247/4999): loss=0.5783202613003691\n",
      "Gradient Descent(2248/4999): loss=0.5783198714741253\n",
      "Gradient Descent(2249/4999): loss=0.5783194818920965\n",
      "Gradient Descent(2250/4999): loss=0.5783190925537677\n",
      "Gradient Descent(2251/4999): loss=0.5783187034586255\n",
      "Gradient Descent(2252/4999): loss=0.5783183146061586\n",
      "Gradient Descent(2253/4999): loss=0.5783179259958575\n",
      "Gradient Descent(2254/4999): loss=0.5783175376272143\n",
      "Gradient Descent(2255/4999): loss=0.5783171494997227\n",
      "Gradient Descent(2256/4999): loss=0.5783167616128786\n",
      "Gradient Descent(2257/4999): loss=0.5783163739661789\n",
      "Gradient Descent(2258/4999): loss=0.5783159865591229\n",
      "Gradient Descent(2259/4999): loss=0.5783155993912111\n",
      "Gradient Descent(2260/4999): loss=0.5783152124619462\n",
      "Gradient Descent(2261/4999): loss=0.5783148257708325\n",
      "Gradient Descent(2262/4999): loss=0.5783144393173755\n",
      "Gradient Descent(2263/4999): loss=0.5783140531010826\n",
      "Gradient Descent(2264/4999): loss=0.5783136671214635\n",
      "Gradient Descent(2265/4999): loss=0.5783132813780288\n",
      "Gradient Descent(2266/4999): loss=0.5783128958702911\n",
      "Gradient Descent(2267/4999): loss=0.5783125105977648\n",
      "Gradient Descent(2268/4999): loss=0.5783121255599655\n",
      "Gradient Descent(2269/4999): loss=0.5783117407564107\n",
      "Gradient Descent(2270/4999): loss=0.57831135618662\n",
      "Gradient Descent(2271/4999): loss=0.5783109718501136\n",
      "Gradient Descent(2272/4999): loss=0.5783105877464144\n",
      "Gradient Descent(2273/4999): loss=0.5783102038750464\n",
      "Gradient Descent(2274/4999): loss=0.5783098202355351\n",
      "Gradient Descent(2275/4999): loss=0.5783094368274079\n",
      "Gradient Descent(2276/4999): loss=0.5783090536501939\n",
      "Gradient Descent(2277/4999): loss=0.5783086707034232\n",
      "Gradient Descent(2278/4999): loss=0.5783082879866283\n",
      "Gradient Descent(2279/4999): loss=0.5783079054993425\n",
      "Gradient Descent(2280/4999): loss=0.5783075232411012\n",
      "Gradient Descent(2281/4999): loss=0.5783071412114416\n",
      "Gradient Descent(2282/4999): loss=0.5783067594099016\n",
      "Gradient Descent(2283/4999): loss=0.5783063778360213\n",
      "Gradient Descent(2284/4999): loss=0.5783059964893426\n",
      "Gradient Descent(2285/4999): loss=0.5783056153694082\n",
      "Gradient Descent(2286/4999): loss=0.5783052344757629\n",
      "Gradient Descent(2287/4999): loss=0.5783048538079527\n",
      "Gradient Descent(2288/4999): loss=0.5783044733655255\n",
      "Gradient Descent(2289/4999): loss=0.5783040931480304\n",
      "Gradient Descent(2290/4999): loss=0.5783037131550185\n",
      "Gradient Descent(2291/4999): loss=0.5783033333860417\n",
      "Gradient Descent(2292/4999): loss=0.5783029538406538\n",
      "Gradient Descent(2293/4999): loss=0.5783025745184102\n",
      "Gradient Descent(2294/4999): loss=0.5783021954188677\n",
      "Gradient Descent(2295/4999): loss=0.5783018165415849\n",
      "Gradient Descent(2296/4999): loss=0.5783014378861211\n",
      "Gradient Descent(2297/4999): loss=0.5783010594520377\n",
      "Gradient Descent(2298/4999): loss=0.5783006812388976\n",
      "Gradient Descent(2299/4999): loss=0.5783003032462649\n",
      "Gradient Descent(2300/4999): loss=0.5782999254737053\n",
      "Gradient Descent(2301/4999): loss=0.578299547920786\n",
      "Gradient Descent(2302/4999): loss=0.5782991705870755\n",
      "Gradient Descent(2303/4999): loss=0.5782987934721439\n",
      "Gradient Descent(2304/4999): loss=0.5782984165755627\n",
      "Gradient Descent(2305/4999): loss=0.5782980398969049\n",
      "Gradient Descent(2306/4999): loss=0.5782976634357446\n",
      "Gradient Descent(2307/4999): loss=0.578297287191658\n",
      "Gradient Descent(2308/4999): loss=0.578296911164222\n",
      "Gradient Descent(2309/4999): loss=0.5782965353530154\n",
      "Gradient Descent(2310/4999): loss=0.5782961597576183\n",
      "Gradient Descent(2311/4999): loss=0.5782957843776119\n",
      "Gradient Descent(2312/4999): loss=0.578295409212579\n",
      "Gradient Descent(2313/4999): loss=0.5782950342621043\n",
      "Gradient Descent(2314/4999): loss=0.5782946595257733\n",
      "Gradient Descent(2315/4999): loss=0.5782942850031729\n",
      "Gradient Descent(2316/4999): loss=0.5782939106938914\n",
      "Gradient Descent(2317/4999): loss=0.5782935365975187\n",
      "Gradient Descent(2318/4999): loss=0.578293162713646\n",
      "Gradient Descent(2319/4999): loss=0.578292789041866\n",
      "Gradient Descent(2320/4999): loss=0.5782924155817721\n",
      "Gradient Descent(2321/4999): loss=0.57829204233296\n",
      "Gradient Descent(2322/4999): loss=0.5782916692950262\n",
      "Gradient Descent(2323/4999): loss=0.5782912964675684\n",
      "Gradient Descent(2324/4999): loss=0.578290923850186\n",
      "Gradient Descent(2325/4999): loss=0.5782905514424798\n",
      "Gradient Descent(2326/4999): loss=0.5782901792440512\n",
      "Gradient Descent(2327/4999): loss=0.578289807254504\n",
      "Gradient Descent(2328/4999): loss=0.5782894354734425\n",
      "Gradient Descent(2329/4999): loss=0.5782890639004727\n",
      "Gradient Descent(2330/4999): loss=0.5782886925352017\n",
      "Gradient Descent(2331/4999): loss=0.578288321377238\n",
      "Gradient Descent(2332/4999): loss=0.5782879504261915\n",
      "Gradient Descent(2333/4999): loss=0.5782875796816733\n",
      "Gradient Descent(2334/4999): loss=0.5782872091432957\n",
      "Gradient Descent(2335/4999): loss=0.5782868388106724\n",
      "Gradient Descent(2336/4999): loss=0.5782864686834185\n",
      "Gradient Descent(2337/4999): loss=0.5782860987611499\n",
      "Gradient Descent(2338/4999): loss=0.5782857290434842\n",
      "Gradient Descent(2339/4999): loss=0.5782853595300406\n",
      "Gradient Descent(2340/4999): loss=0.5782849902204384\n",
      "Gradient Descent(2341/4999): loss=0.5782846211142996\n",
      "Gradient Descent(2342/4999): loss=0.5782842522112461\n",
      "Gradient Descent(2343/4999): loss=0.5782838835109022\n",
      "Gradient Descent(2344/4999): loss=0.5782835150128925\n",
      "Gradient Descent(2345/4999): loss=0.5782831467168437\n",
      "Gradient Descent(2346/4999): loss=0.5782827786223826\n",
      "Gradient Descent(2347/4999): loss=0.5782824107291384\n",
      "Gradient Descent(2348/4999): loss=0.5782820430367411\n",
      "Gradient Descent(2349/4999): loss=0.5782816755448218\n",
      "Gradient Descent(2350/4999): loss=0.5782813082530128\n",
      "Gradient Descent(2351/4999): loss=0.5782809411609477\n",
      "Gradient Descent(2352/4999): loss=0.5782805742682613\n",
      "Gradient Descent(2353/4999): loss=0.5782802075745894\n",
      "Gradient Descent(2354/4999): loss=0.5782798410795695\n",
      "Gradient Descent(2355/4999): loss=0.5782794747828398\n",
      "Gradient Descent(2356/4999): loss=0.57827910868404\n",
      "Gradient Descent(2357/4999): loss=0.5782787427828106\n",
      "Gradient Descent(2358/4999): loss=0.5782783770787936\n",
      "Gradient Descent(2359/4999): loss=0.5782780115716324\n",
      "Gradient Descent(2360/4999): loss=0.5782776462609709\n",
      "Gradient Descent(2361/4999): loss=0.5782772811464547\n",
      "Gradient Descent(2362/4999): loss=0.5782769162277303\n",
      "Gradient Descent(2363/4999): loss=0.5782765515044455\n",
      "Gradient Descent(2364/4999): loss=0.5782761869762493\n",
      "Gradient Descent(2365/4999): loss=0.5782758226427914\n",
      "Gradient Descent(2366/4999): loss=0.5782754585037235\n",
      "Gradient Descent(2367/4999): loss=0.5782750945586976\n",
      "Gradient Descent(2368/4999): loss=0.5782747308073672\n",
      "Gradient Descent(2369/4999): loss=0.578274367249387\n",
      "Gradient Descent(2370/4999): loss=0.5782740038844126\n",
      "Gradient Descent(2371/4999): loss=0.5782736407121009\n",
      "Gradient Descent(2372/4999): loss=0.5782732777321098\n",
      "Gradient Descent(2373/4999): loss=0.5782729149440985\n",
      "Gradient Descent(2374/4999): loss=0.578272552347727\n",
      "Gradient Descent(2375/4999): loss=0.5782721899426568\n",
      "Gradient Descent(2376/4999): loss=0.5782718277285502\n",
      "Gradient Descent(2377/4999): loss=0.5782714657050707\n",
      "Gradient Descent(2378/4999): loss=0.578271103871883\n",
      "Gradient Descent(2379/4999): loss=0.5782707422286523\n",
      "Gradient Descent(2380/4999): loss=0.5782703807750459\n",
      "Gradient Descent(2381/4999): loss=0.5782700195107312\n",
      "Gradient Descent(2382/4999): loss=0.5782696584353776\n",
      "Gradient Descent(2383/4999): loss=0.5782692975486546\n",
      "Gradient Descent(2384/4999): loss=0.5782689368502336\n",
      "Gradient Descent(2385/4999): loss=0.5782685763397865\n",
      "Gradient Descent(2386/4999): loss=0.5782682160169863\n",
      "Gradient Descent(2387/4999): loss=0.5782678558815078\n",
      "Gradient Descent(2388/4999): loss=0.578267495933026\n",
      "Gradient Descent(2389/4999): loss=0.578267136171217\n",
      "Gradient Descent(2390/4999): loss=0.5782667765957583\n",
      "Gradient Descent(2391/4999): loss=0.5782664172063287\n",
      "Gradient Descent(2392/4999): loss=0.578266058002607\n",
      "Gradient Descent(2393/4999): loss=0.5782656989842742\n",
      "Gradient Descent(2394/4999): loss=0.5782653401510116\n",
      "Gradient Descent(2395/4999): loss=0.5782649815025019\n",
      "Gradient Descent(2396/4999): loss=0.5782646230384284\n",
      "Gradient Descent(2397/4999): loss=0.578264264758476\n",
      "Gradient Descent(2398/4999): loss=0.5782639066623302\n",
      "Gradient Descent(2399/4999): loss=0.5782635487496773\n",
      "Gradient Descent(2400/4999): loss=0.5782631910202054\n",
      "Gradient Descent(2401/4999): loss=0.5782628334736026\n",
      "Gradient Descent(2402/4999): loss=0.5782624761095588\n",
      "Gradient Descent(2403/4999): loss=0.5782621189277649\n",
      "Gradient Descent(2404/4999): loss=0.578261761927912\n",
      "Gradient Descent(2405/4999): loss=0.5782614051096928\n",
      "Gradient Descent(2406/4999): loss=0.578261048472801\n",
      "Gradient Descent(2407/4999): loss=0.5782606920169311\n",
      "Gradient Descent(2408/4999): loss=0.5782603357417788\n",
      "Gradient Descent(2409/4999): loss=0.5782599796470402\n",
      "Gradient Descent(2410/4999): loss=0.578259623732413\n",
      "Gradient Descent(2411/4999): loss=0.5782592679975957\n",
      "Gradient Descent(2412/4999): loss=0.5782589124422878\n",
      "Gradient Descent(2413/4999): loss=0.5782585570661892\n",
      "Gradient Descent(2414/4999): loss=0.5782582018690017\n",
      "Gradient Descent(2415/4999): loss=0.5782578468504272\n",
      "Gradient Descent(2416/4999): loss=0.5782574920101692\n",
      "Gradient Descent(2417/4999): loss=0.5782571373479317\n",
      "Gradient Descent(2418/4999): loss=0.5782567828634199\n",
      "Gradient Descent(2419/4999): loss=0.5782564285563395\n",
      "Gradient Descent(2420/4999): loss=0.5782560744263979\n",
      "Gradient Descent(2421/4999): loss=0.5782557204733029\n",
      "Gradient Descent(2422/4999): loss=0.5782553666967631\n",
      "Gradient Descent(2423/4999): loss=0.5782550130964884\n",
      "Gradient Descent(2424/4999): loss=0.5782546596721896\n",
      "Gradient Descent(2425/4999): loss=0.5782543064235779\n",
      "Gradient Descent(2426/4999): loss=0.5782539533503662\n",
      "Gradient Descent(2427/4999): loss=0.5782536004522677\n",
      "Gradient Descent(2428/4999): loss=0.5782532477289969\n",
      "Gradient Descent(2429/4999): loss=0.5782528951802687\n",
      "Gradient Descent(2430/4999): loss=0.5782525428057994\n",
      "Gradient Descent(2431/4999): loss=0.5782521906053059\n",
      "Gradient Descent(2432/4999): loss=0.5782518385785063\n",
      "Gradient Descent(2433/4999): loss=0.5782514867251192\n",
      "Gradient Descent(2434/4999): loss=0.5782511350448645\n",
      "Gradient Descent(2435/4999): loss=0.5782507835374624\n",
      "Gradient Descent(2436/4999): loss=0.5782504322026348\n",
      "Gradient Descent(2437/4999): loss=0.5782500810401038\n",
      "Gradient Descent(2438/4999): loss=0.5782497300495925\n",
      "Gradient Descent(2439/4999): loss=0.5782493792308251\n",
      "Gradient Descent(2440/4999): loss=0.5782490285835263\n",
      "Gradient Descent(2441/4999): loss=0.5782486781074222\n",
      "Gradient Descent(2442/4999): loss=0.5782483278022393\n",
      "Gradient Descent(2443/4999): loss=0.5782479776677052\n",
      "Gradient Descent(2444/4999): loss=0.578247627703548\n",
      "Gradient Descent(2445/4999): loss=0.5782472779094973\n",
      "Gradient Descent(2446/4999): loss=0.5782469282852828\n",
      "Gradient Descent(2447/4999): loss=0.5782465788306355\n",
      "Gradient Descent(2448/4999): loss=0.5782462295452874\n",
      "Gradient Descent(2449/4999): loss=0.5782458804289707\n",
      "Gradient Descent(2450/4999): loss=0.5782455314814191\n",
      "Gradient Descent(2451/4999): loss=0.5782451827023668\n",
      "Gradient Descent(2452/4999): loss=0.5782448340915487\n",
      "Gradient Descent(2453/4999): loss=0.5782444856487009\n",
      "Gradient Descent(2454/4999): loss=0.5782441373735602\n",
      "Gradient Descent(2455/4999): loss=0.5782437892658638\n",
      "Gradient Descent(2456/4999): loss=0.5782434413253505\n",
      "Gradient Descent(2457/4999): loss=0.5782430935517591\n",
      "Gradient Descent(2458/4999): loss=0.5782427459448298\n",
      "Gradient Descent(2459/4999): loss=0.5782423985043034\n",
      "Gradient Descent(2460/4999): loss=0.5782420512299213\n",
      "Gradient Descent(2461/4999): loss=0.5782417041214262\n",
      "Gradient Descent(2462/4999): loss=0.578241357178561\n",
      "Gradient Descent(2463/4999): loss=0.57824101040107\n",
      "Gradient Descent(2464/4999): loss=0.5782406637886977\n",
      "Gradient Descent(2465/4999): loss=0.5782403173411899\n",
      "Gradient Descent(2466/4999): loss=0.5782399710582928\n",
      "Gradient Descent(2467/4999): loss=0.5782396249397536\n",
      "Gradient Descent(2468/4999): loss=0.5782392789853202\n",
      "Gradient Descent(2469/4999): loss=0.5782389331947415\n",
      "Gradient Descent(2470/4999): loss=0.5782385875677668\n",
      "Gradient Descent(2471/4999): loss=0.5782382421041463\n",
      "Gradient Descent(2472/4999): loss=0.5782378968036312\n",
      "Gradient Descent(2473/4999): loss=0.5782375516659731\n",
      "Gradient Descent(2474/4999): loss=0.5782372066909248\n",
      "Gradient Descent(2475/4999): loss=0.5782368618782394\n",
      "Gradient Descent(2476/4999): loss=0.5782365172276711\n",
      "Gradient Descent(2477/4999): loss=0.5782361727389748\n",
      "Gradient Descent(2478/4999): loss=0.5782358284119058\n",
      "Gradient Descent(2479/4999): loss=0.5782354842462206\n",
      "Gradient Descent(2480/4999): loss=0.5782351402416763\n",
      "Gradient Descent(2481/4999): loss=0.5782347963980308\n",
      "Gradient Descent(2482/4999): loss=0.5782344527150425\n",
      "Gradient Descent(2483/4999): loss=0.5782341091924706\n",
      "Gradient Descent(2484/4999): loss=0.5782337658300756\n",
      "Gradient Descent(2485/4999): loss=0.5782334226276178\n",
      "Gradient Descent(2486/4999): loss=0.5782330795848588\n",
      "Gradient Descent(2487/4999): loss=0.578232736701561\n",
      "Gradient Descent(2488/4999): loss=0.5782323939774873\n",
      "Gradient Descent(2489/4999): loss=0.5782320514124013\n",
      "Gradient Descent(2490/4999): loss=0.5782317090060675\n",
      "Gradient Descent(2491/4999): loss=0.5782313667582509\n",
      "Gradient Descent(2492/4999): loss=0.5782310246687175\n",
      "Gradient Descent(2493/4999): loss=0.5782306827372337\n",
      "Gradient Descent(2494/4999): loss=0.5782303409635665\n",
      "Gradient Descent(2495/4999): loss=0.5782299993474845\n",
      "Gradient Descent(2496/4999): loss=0.578229657888756\n",
      "Gradient Descent(2497/4999): loss=0.5782293165871503\n",
      "Gradient Descent(2498/4999): loss=0.5782289754424375\n",
      "Gradient Descent(2499/4999): loss=0.5782286344543884\n",
      "Gradient Descent(2500/4999): loss=0.5782282936227744\n",
      "Gradient Descent(2501/4999): loss=0.5782279529473677\n",
      "Gradient Descent(2502/4999): loss=0.5782276124279411\n",
      "Gradient Descent(2503/4999): loss=0.5782272720642684\n",
      "Gradient Descent(2504/4999): loss=0.5782269318561233\n",
      "Gradient Descent(2505/4999): loss=0.578226591803281\n",
      "Gradient Descent(2506/4999): loss=0.5782262519055169\n",
      "Gradient Descent(2507/4999): loss=0.5782259121626074\n",
      "Gradient Descent(2508/4999): loss=0.5782255725743294\n",
      "Gradient Descent(2509/4999): loss=0.5782252331404603\n",
      "Gradient Descent(2510/4999): loss=0.5782248938607785\n",
      "Gradient Descent(2511/4999): loss=0.5782245547350631\n",
      "Gradient Descent(2512/4999): loss=0.5782242157630932\n",
      "Gradient Descent(2513/4999): loss=0.5782238769446495\n",
      "Gradient Descent(2514/4999): loss=0.5782235382795127\n",
      "Gradient Descent(2515/4999): loss=0.5782231997674643\n",
      "Gradient Descent(2516/4999): loss=0.5782228614082866\n",
      "Gradient Descent(2517/4999): loss=0.5782225232017625\n",
      "Gradient Descent(2518/4999): loss=0.5782221851476754\n",
      "Gradient Descent(2519/4999): loss=0.5782218472458097\n",
      "Gradient Descent(2520/4999): loss=0.5782215094959499\n",
      "Gradient Descent(2521/4999): loss=0.5782211718978816\n",
      "Gradient Descent(2522/4999): loss=0.5782208344513908\n",
      "Gradient Descent(2523/4999): loss=0.5782204971562644\n",
      "Gradient Descent(2524/4999): loss=0.5782201600122896\n",
      "Gradient Descent(2525/4999): loss=0.5782198230192545\n",
      "Gradient Descent(2526/4999): loss=0.5782194861769476\n",
      "Gradient Descent(2527/4999): loss=0.5782191494851582\n",
      "Gradient Descent(2528/4999): loss=0.5782188129436762\n",
      "Gradient Descent(2529/4999): loss=0.5782184765522922\n",
      "Gradient Descent(2530/4999): loss=0.5782181403107972\n",
      "Gradient Descent(2531/4999): loss=0.5782178042189827\n",
      "Gradient Descent(2532/4999): loss=0.5782174682766416\n",
      "Gradient Descent(2533/4999): loss=0.5782171324835663\n",
      "Gradient Descent(2534/4999): loss=0.578216796839551\n",
      "Gradient Descent(2535/4999): loss=0.5782164613443894\n",
      "Gradient Descent(2536/4999): loss=0.5782161259978765\n",
      "Gradient Descent(2537/4999): loss=0.5782157907998074\n",
      "Gradient Descent(2538/4999): loss=0.5782154557499787\n",
      "Gradient Descent(2539/4999): loss=0.5782151208481864\n",
      "Gradient Descent(2540/4999): loss=0.5782147860942283\n",
      "Gradient Descent(2541/4999): loss=0.5782144514879015\n",
      "Gradient Descent(2542/4999): loss=0.5782141170290053\n",
      "Gradient Descent(2543/4999): loss=0.5782137827173379\n",
      "Gradient Descent(2544/4999): loss=0.5782134485526992\n",
      "Gradient Descent(2545/4999): loss=0.5782131145348893\n",
      "Gradient Descent(2546/4999): loss=0.578212780663709\n",
      "Gradient Descent(2547/4999): loss=0.5782124469389596\n",
      "Gradient Descent(2548/4999): loss=0.5782121133604432\n",
      "Gradient Descent(2549/4999): loss=0.5782117799279622\n",
      "Gradient Descent(2550/4999): loss=0.5782114466413195\n",
      "Gradient Descent(2551/4999): loss=0.578211113500319\n",
      "Gradient Descent(2552/4999): loss=0.5782107805047647\n",
      "Gradient Descent(2553/4999): loss=0.5782104476544617\n",
      "Gradient Descent(2554/4999): loss=0.578210114949215\n",
      "Gradient Descent(2555/4999): loss=0.5782097823888309\n",
      "Gradient Descent(2556/4999): loss=0.5782094499731156\n",
      "Gradient Descent(2557/4999): loss=0.5782091177018763\n",
      "Gradient Descent(2558/4999): loss=0.5782087855749207\n",
      "Gradient Descent(2559/4999): loss=0.5782084535920569\n",
      "Gradient Descent(2560/4999): loss=0.5782081217530934\n",
      "Gradient Descent(2561/4999): loss=0.5782077900578398\n",
      "Gradient Descent(2562/4999): loss=0.5782074585061061\n",
      "Gradient Descent(2563/4999): loss=0.5782071270977024\n",
      "Gradient Descent(2564/4999): loss=0.5782067958324394\n",
      "Gradient Descent(2565/4999): loss=0.5782064647101292\n",
      "Gradient Descent(2566/4999): loss=0.5782061337305833\n",
      "Gradient Descent(2567/4999): loss=0.5782058028936147\n",
      "Gradient Descent(2568/4999): loss=0.5782054721990363\n",
      "Gradient Descent(2569/4999): loss=0.5782051416466616\n",
      "Gradient Descent(2570/4999): loss=0.5782048112363051\n",
      "Gradient Descent(2571/4999): loss=0.5782044809677814\n",
      "Gradient Descent(2572/4999): loss=0.5782041508409056\n",
      "Gradient Descent(2573/4999): loss=0.5782038208554939\n",
      "Gradient Descent(2574/4999): loss=0.5782034910113623\n",
      "Gradient Descent(2575/4999): loss=0.5782031613083275\n",
      "Gradient Descent(2576/4999): loss=0.5782028317462073\n",
      "Gradient Descent(2577/4999): loss=0.5782025023248194\n",
      "Gradient Descent(2578/4999): loss=0.5782021730439821\n",
      "Gradient Descent(2579/4999): loss=0.5782018439035146\n",
      "Gradient Descent(2580/4999): loss=0.5782015149032361\n",
      "Gradient Descent(2581/4999): loss=0.5782011860429666\n",
      "Gradient Descent(2582/4999): loss=0.5782008573225266\n",
      "Gradient Descent(2583/4999): loss=0.5782005287417373\n",
      "Gradient Descent(2584/4999): loss=0.57820020030042\n",
      "Gradient Descent(2585/4999): loss=0.5781998719983967\n",
      "Gradient Descent(2586/4999): loss=0.5781995438354901\n",
      "Gradient Descent(2587/4999): loss=0.5781992158115229\n",
      "Gradient Descent(2588/4999): loss=0.5781988879263189\n",
      "Gradient Descent(2589/4999): loss=0.5781985601797021\n",
      "Gradient Descent(2590/4999): loss=0.5781982325714969\n",
      "Gradient Descent(2591/4999): loss=0.5781979051015285\n",
      "Gradient Descent(2592/4999): loss=0.5781975777696223\n",
      "Gradient Descent(2593/4999): loss=0.5781972505756043\n",
      "Gradient Descent(2594/4999): loss=0.5781969235193012\n",
      "Gradient Descent(2595/4999): loss=0.5781965966005396\n",
      "Gradient Descent(2596/4999): loss=0.5781962698191472\n",
      "Gradient Descent(2597/4999): loss=0.5781959431749522\n",
      "Gradient Descent(2598/4999): loss=0.5781956166677826\n",
      "Gradient Descent(2599/4999): loss=0.5781952902974677\n",
      "Gradient Descent(2600/4999): loss=0.578194964063837\n",
      "Gradient Descent(2601/4999): loss=0.5781946379667199\n",
      "Gradient Descent(2602/4999): loss=0.5781943120059471\n",
      "Gradient Descent(2603/4999): loss=0.5781939861813495\n",
      "Gradient Descent(2604/4999): loss=0.5781936604927582\n",
      "Gradient Descent(2605/4999): loss=0.5781933349400052\n",
      "Gradient Descent(2606/4999): loss=0.5781930095229227\n",
      "Gradient Descent(2607/4999): loss=0.5781926842413435\n",
      "Gradient Descent(2608/4999): loss=0.5781923590951008\n",
      "Gradient Descent(2609/4999): loss=0.5781920340840281\n",
      "Gradient Descent(2610/4999): loss=0.5781917092079598\n",
      "Gradient Descent(2611/4999): loss=0.5781913844667301\n",
      "Gradient Descent(2612/4999): loss=0.5781910598601745\n",
      "Gradient Descent(2613/4999): loss=0.5781907353881284\n",
      "Gradient Descent(2614/4999): loss=0.5781904110504273\n",
      "Gradient Descent(2615/4999): loss=0.5781900868469086\n",
      "Gradient Descent(2616/4999): loss=0.5781897627774084\n",
      "Gradient Descent(2617/4999): loss=0.5781894388417642\n",
      "Gradient Descent(2618/4999): loss=0.5781891150398139\n",
      "Gradient Descent(2619/4999): loss=0.5781887913713957\n",
      "Gradient Descent(2620/4999): loss=0.5781884678363481\n",
      "Gradient Descent(2621/4999): loss=0.5781881444345106\n",
      "Gradient Descent(2622/4999): loss=0.5781878211657223\n",
      "Gradient Descent(2623/4999): loss=0.5781874980298237\n",
      "Gradient Descent(2624/4999): loss=0.5781871750266552\n",
      "Gradient Descent(2625/4999): loss=0.5781868521560575\n",
      "Gradient Descent(2626/4999): loss=0.5781865294178719\n",
      "Gradient Descent(2627/4999): loss=0.5781862068119402\n",
      "Gradient Descent(2628/4999): loss=0.5781858843381048\n",
      "Gradient Descent(2629/4999): loss=0.5781855619962082\n",
      "Gradient Descent(2630/4999): loss=0.5781852397860935\n",
      "Gradient Descent(2631/4999): loss=0.5781849177076042\n",
      "Gradient Descent(2632/4999): loss=0.5781845957605841\n",
      "Gradient Descent(2633/4999): loss=0.5781842739448778\n",
      "Gradient Descent(2634/4999): loss=0.57818395226033\n",
      "Gradient Descent(2635/4999): loss=0.5781836307067859\n",
      "Gradient Descent(2636/4999): loss=0.5781833092840911\n",
      "Gradient Descent(2637/4999): loss=0.5781829879920917\n",
      "Gradient Descent(2638/4999): loss=0.5781826668306341\n",
      "Gradient Descent(2639/4999): loss=0.5781823457995654\n",
      "Gradient Descent(2640/4999): loss=0.5781820248987326\n",
      "Gradient Descent(2641/4999): loss=0.5781817041279835\n",
      "Gradient Descent(2642/4999): loss=0.5781813834871665\n",
      "Gradient Descent(2643/4999): loss=0.5781810629761298\n",
      "Gradient Descent(2644/4999): loss=0.5781807425947225\n",
      "Gradient Descent(2645/4999): loss=0.5781804223427942\n",
      "Gradient Descent(2646/4999): loss=0.5781801022201942\n",
      "Gradient Descent(2647/4999): loss=0.5781797822267731\n",
      "Gradient Descent(2648/4999): loss=0.5781794623623814\n",
      "Gradient Descent(2649/4999): loss=0.57817914262687\n",
      "Gradient Descent(2650/4999): loss=0.5781788230200902\n",
      "Gradient Descent(2651/4999): loss=0.5781785035418939\n",
      "Gradient Descent(2652/4999): loss=0.5781781841921333\n",
      "Gradient Descent(2653/4999): loss=0.5781778649706611\n",
      "Gradient Descent(2654/4999): loss=0.5781775458773301\n",
      "Gradient Descent(2655/4999): loss=0.5781772269119937\n",
      "Gradient Descent(2656/4999): loss=0.5781769080745058\n",
      "Gradient Descent(2657/4999): loss=0.5781765893647207\n",
      "Gradient Descent(2658/4999): loss=0.5781762707824925\n",
      "Gradient Descent(2659/4999): loss=0.5781759523276763\n",
      "Gradient Descent(2660/4999): loss=0.5781756340001276\n",
      "Gradient Descent(2661/4999): loss=0.5781753157997022\n",
      "Gradient Descent(2662/4999): loss=0.5781749977262559\n",
      "Gradient Descent(2663/4999): loss=0.5781746797796454\n",
      "Gradient Descent(2664/4999): loss=0.5781743619597276\n",
      "Gradient Descent(2665/4999): loss=0.5781740442663594\n",
      "Gradient Descent(2666/4999): loss=0.5781737266993989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2667/4999): loss=0.5781734092587039\n",
      "Gradient Descent(2668/4999): loss=0.5781730919441327\n",
      "Gradient Descent(2669/4999): loss=0.5781727747555442\n",
      "Gradient Descent(2670/4999): loss=0.5781724576927973\n",
      "Gradient Descent(2671/4999): loss=0.578172140755752\n",
      "Gradient Descent(2672/4999): loss=0.5781718239442677\n",
      "Gradient Descent(2673/4999): loss=0.5781715072582049\n",
      "Gradient Descent(2674/4999): loss=0.578171190697424\n",
      "Gradient Descent(2675/4999): loss=0.578170874261786\n",
      "Gradient Descent(2676/4999): loss=0.5781705579511528\n",
      "Gradient Descent(2677/4999): loss=0.5781702417653853\n",
      "Gradient Descent(2678/4999): loss=0.5781699257043461\n",
      "Gradient Descent(2679/4999): loss=0.5781696097678976\n",
      "Gradient Descent(2680/4999): loss=0.5781692939559024\n",
      "Gradient Descent(2681/4999): loss=0.5781689782682239\n",
      "Gradient Descent(2682/4999): loss=0.5781686627047254\n",
      "Gradient Descent(2683/4999): loss=0.5781683472652709\n",
      "Gradient Descent(2684/4999): loss=0.5781680319497245\n",
      "Gradient Descent(2685/4999): loss=0.5781677167579511\n",
      "Gradient Descent(2686/4999): loss=0.5781674016898153\n",
      "Gradient Descent(2687/4999): loss=0.5781670867451825\n",
      "Gradient Descent(2688/4999): loss=0.5781667719239185\n",
      "Gradient Descent(2689/4999): loss=0.5781664572258892\n",
      "Gradient Descent(2690/4999): loss=0.5781661426509608\n",
      "Gradient Descent(2691/4999): loss=0.5781658281990003\n",
      "Gradient Descent(2692/4999): loss=0.5781655138698746\n",
      "Gradient Descent(2693/4999): loss=0.5781651996634508\n",
      "Gradient Descent(2694/4999): loss=0.5781648855795971\n",
      "Gradient Descent(2695/4999): loss=0.5781645716181812\n",
      "Gradient Descent(2696/4999): loss=0.5781642577790717\n",
      "Gradient Descent(2697/4999): loss=0.5781639440621372\n",
      "Gradient Descent(2698/4999): loss=0.578163630467247\n",
      "Gradient Descent(2699/4999): loss=0.5781633169942704\n",
      "Gradient Descent(2700/4999): loss=0.5781630036430769\n",
      "Gradient Descent(2701/4999): loss=0.5781626904135372\n",
      "Gradient Descent(2702/4999): loss=0.5781623773055212\n",
      "Gradient Descent(2703/4999): loss=0.5781620643188999\n",
      "Gradient Descent(2704/4999): loss=0.5781617514535444\n",
      "Gradient Descent(2705/4999): loss=0.5781614387093261\n",
      "Gradient Descent(2706/4999): loss=0.5781611260861168\n",
      "Gradient Descent(2707/4999): loss=0.5781608135837883\n",
      "Gradient Descent(2708/4999): loss=0.5781605012022133\n",
      "Gradient Descent(2709/4999): loss=0.5781601889412645\n",
      "Gradient Descent(2710/4999): loss=0.5781598768008149\n",
      "Gradient Descent(2711/4999): loss=0.578159564780738\n",
      "Gradient Descent(2712/4999): loss=0.5781592528809073\n",
      "Gradient Descent(2713/4999): loss=0.5781589411011969\n",
      "Gradient Descent(2714/4999): loss=0.5781586294414812\n",
      "Gradient Descent(2715/4999): loss=0.578158317901635\n",
      "Gradient Descent(2716/4999): loss=0.5781580064815329\n",
      "Gradient Descent(2717/4999): loss=0.5781576951810505\n",
      "Gradient Descent(2718/4999): loss=0.5781573840000634\n",
      "Gradient Descent(2719/4999): loss=0.5781570729384476\n",
      "Gradient Descent(2720/4999): loss=0.578156761996079\n",
      "Gradient Descent(2721/4999): loss=0.5781564511728345\n",
      "Gradient Descent(2722/4999): loss=0.5781561404685908\n",
      "Gradient Descent(2723/4999): loss=0.5781558298832251\n",
      "Gradient Descent(2724/4999): loss=0.578155519416615\n",
      "Gradient Descent(2725/4999): loss=0.578155209068638\n",
      "Gradient Descent(2726/4999): loss=0.5781548988391725\n",
      "Gradient Descent(2727/4999): loss=0.5781545887280969\n",
      "Gradient Descent(2728/4999): loss=0.5781542787352898\n",
      "Gradient Descent(2729/4999): loss=0.5781539688606301\n",
      "Gradient Descent(2730/4999): loss=0.5781536591039974\n",
      "Gradient Descent(2731/4999): loss=0.5781533494652711\n",
      "Gradient Descent(2732/4999): loss=0.5781530399443311\n",
      "Gradient Descent(2733/4999): loss=0.5781527305410576\n",
      "Gradient Descent(2734/4999): loss=0.5781524212553316\n",
      "Gradient Descent(2735/4999): loss=0.5781521120870332\n",
      "Gradient Descent(2736/4999): loss=0.5781518030360441\n",
      "Gradient Descent(2737/4999): loss=0.5781514941022454\n",
      "Gradient Descent(2738/4999): loss=0.5781511852855188\n",
      "Gradient Descent(2739/4999): loss=0.5781508765857463\n",
      "Gradient Descent(2740/4999): loss=0.5781505680028102\n",
      "Gradient Descent(2741/4999): loss=0.5781502595365932\n",
      "Gradient Descent(2742/4999): loss=0.578149951186978\n",
      "Gradient Descent(2743/4999): loss=0.5781496429538479\n",
      "Gradient Descent(2744/4999): loss=0.5781493348370861\n",
      "Gradient Descent(2745/4999): loss=0.5781490268365767\n",
      "Gradient Descent(2746/4999): loss=0.5781487189522034\n",
      "Gradient Descent(2747/4999): loss=0.5781484111838505\n",
      "Gradient Descent(2748/4999): loss=0.5781481035314029\n",
      "Gradient Descent(2749/4999): loss=0.5781477959947453\n",
      "Gradient Descent(2750/4999): loss=0.5781474885737625\n",
      "Gradient Descent(2751/4999): loss=0.5781471812683405\n",
      "Gradient Descent(2752/4999): loss=0.5781468740783648\n",
      "Gradient Descent(2753/4999): loss=0.5781465670037212\n",
      "Gradient Descent(2754/4999): loss=0.5781462600442963\n",
      "Gradient Descent(2755/4999): loss=0.5781459531999762\n",
      "Gradient Descent(2756/4999): loss=0.5781456464706483\n",
      "Gradient Descent(2757/4999): loss=0.5781453398561991\n",
      "Gradient Descent(2758/4999): loss=0.5781450333565165\n",
      "Gradient Descent(2759/4999): loss=0.5781447269714876\n",
      "Gradient Descent(2760/4999): loss=0.5781444207010007\n",
      "Gradient Descent(2761/4999): loss=0.578144114544944\n",
      "Gradient Descent(2762/4999): loss=0.5781438085032059\n",
      "Gradient Descent(2763/4999): loss=0.578143502575675\n",
      "Gradient Descent(2764/4999): loss=0.5781431967622405\n",
      "Gradient Descent(2765/4999): loss=0.5781428910627914\n",
      "Gradient Descent(2766/4999): loss=0.5781425854772174\n",
      "Gradient Descent(2767/4999): loss=0.5781422800054082\n",
      "Gradient Descent(2768/4999): loss=0.578141974647254\n",
      "Gradient Descent(2769/4999): loss=0.5781416694026453\n",
      "Gradient Descent(2770/4999): loss=0.5781413642714723\n",
      "Gradient Descent(2771/4999): loss=0.578141059253626\n",
      "Gradient Descent(2772/4999): loss=0.5781407543489976\n",
      "Gradient Descent(2773/4999): loss=0.5781404495574781\n",
      "Gradient Descent(2774/4999): loss=0.5781401448789598\n",
      "Gradient Descent(2775/4999): loss=0.5781398403133339\n",
      "Gradient Descent(2776/4999): loss=0.578139535860493\n",
      "Gradient Descent(2777/4999): loss=0.5781392315203295\n",
      "Gradient Descent(2778/4999): loss=0.5781389272927357\n",
      "Gradient Descent(2779/4999): loss=0.5781386231776049\n",
      "Gradient Descent(2780/4999): loss=0.5781383191748303\n",
      "Gradient Descent(2781/4999): loss=0.5781380152843049\n",
      "Gradient Descent(2782/4999): loss=0.5781377115059227\n",
      "Gradient Descent(2783/4999): loss=0.5781374078395775\n",
      "Gradient Descent(2784/4999): loss=0.5781371042851635\n",
      "Gradient Descent(2785/4999): loss=0.5781368008425752\n",
      "Gradient Descent(2786/4999): loss=0.5781364975117074\n",
      "Gradient Descent(2787/4999): loss=0.5781361942924546\n",
      "Gradient Descent(2788/4999): loss=0.5781358911847122\n",
      "Gradient Descent(2789/4999): loss=0.5781355881883758\n",
      "Gradient Descent(2790/4999): loss=0.5781352853033408\n",
      "Gradient Descent(2791/4999): loss=0.5781349825295032\n",
      "Gradient Descent(2792/4999): loss=0.5781346798667594\n",
      "Gradient Descent(2793/4999): loss=0.5781343773150053\n",
      "Gradient Descent(2794/4999): loss=0.5781340748741379\n",
      "Gradient Descent(2795/4999): loss=0.5781337725440541\n",
      "Gradient Descent(2796/4999): loss=0.5781334703246507\n",
      "Gradient Descent(2797/4999): loss=0.5781331682158255\n",
      "Gradient Descent(2798/4999): loss=0.5781328662174756\n",
      "Gradient Descent(2799/4999): loss=0.5781325643294992\n",
      "Gradient Descent(2800/4999): loss=0.5781322625517942\n",
      "Gradient Descent(2801/4999): loss=0.5781319608842591\n",
      "Gradient Descent(2802/4999): loss=0.5781316593267921\n",
      "Gradient Descent(2803/4999): loss=0.5781313578792924\n",
      "Gradient Descent(2804/4999): loss=0.5781310565416586\n",
      "Gradient Descent(2805/4999): loss=0.5781307553137904\n",
      "Gradient Descent(2806/4999): loss=0.578130454195587\n",
      "Gradient Descent(2807/4999): loss=0.5781301531869479\n",
      "Gradient Descent(2808/4999): loss=0.5781298522877735\n",
      "Gradient Descent(2809/4999): loss=0.5781295514979636\n",
      "Gradient Descent(2810/4999): loss=0.578129250817419\n",
      "Gradient Descent(2811/4999): loss=0.5781289502460397\n",
      "Gradient Descent(2812/4999): loss=0.5781286497837272\n",
      "Gradient Descent(2813/4999): loss=0.5781283494303822\n",
      "Gradient Descent(2814/4999): loss=0.5781280491859062\n",
      "Gradient Descent(2815/4999): loss=0.5781277490502007\n",
      "Gradient Descent(2816/4999): loss=0.5781274490231674\n",
      "Gradient Descent(2817/4999): loss=0.5781271491047081\n",
      "Gradient Descent(2818/4999): loss=0.5781268492947254\n",
      "Gradient Descent(2819/4999): loss=0.5781265495931215\n",
      "Gradient Descent(2820/4999): loss=0.578126249999799\n",
      "Gradient Descent(2821/4999): loss=0.5781259505146609\n",
      "Gradient Descent(2822/4999): loss=0.5781256511376102\n",
      "Gradient Descent(2823/4999): loss=0.5781253518685502\n",
      "Gradient Descent(2824/4999): loss=0.5781250527073846\n",
      "Gradient Descent(2825/4999): loss=0.5781247536540169\n",
      "Gradient Descent(2826/4999): loss=0.5781244547083513\n",
      "Gradient Descent(2827/4999): loss=0.5781241558702918\n",
      "Gradient Descent(2828/4999): loss=0.5781238571397429\n",
      "Gradient Descent(2829/4999): loss=0.578123558516609\n",
      "Gradient Descent(2830/4999): loss=0.5781232600007952\n",
      "Gradient Descent(2831/4999): loss=0.5781229615922063\n",
      "Gradient Descent(2832/4999): loss=0.5781226632907477\n",
      "Gradient Descent(2833/4999): loss=0.5781223650963249\n",
      "Gradient Descent(2834/4999): loss=0.5781220670088434\n",
      "Gradient Descent(2835/4999): loss=0.5781217690282091\n",
      "Gradient Descent(2836/4999): loss=0.5781214711543283\n",
      "Gradient Descent(2837/4999): loss=0.5781211733871071\n",
      "Gradient Descent(2838/4999): loss=0.5781208757264521\n",
      "Gradient Descent(2839/4999): loss=0.5781205781722699\n",
      "Gradient Descent(2840/4999): loss=0.5781202807244674\n",
      "Gradient Descent(2841/4999): loss=0.578119983382952\n",
      "Gradient Descent(2842/4999): loss=0.5781196861476309\n",
      "Gradient Descent(2843/4999): loss=0.5781193890184115\n",
      "Gradient Descent(2844/4999): loss=0.5781190919952016\n",
      "Gradient Descent(2845/4999): loss=0.5781187950779093\n",
      "Gradient Descent(2846/4999): loss=0.5781184982664427\n",
      "Gradient Descent(2847/4999): loss=0.5781182015607099\n",
      "Gradient Descent(2848/4999): loss=0.5781179049606197\n",
      "Gradient Descent(2849/4999): loss=0.5781176084660808\n",
      "Gradient Descent(2850/4999): loss=0.5781173120770021\n",
      "Gradient Descent(2851/4999): loss=0.5781170157932928\n",
      "Gradient Descent(2852/4999): loss=0.5781167196148623\n",
      "Gradient Descent(2853/4999): loss=0.5781164235416202\n",
      "Gradient Descent(2854/4999): loss=0.5781161275734761\n",
      "Gradient Descent(2855/4999): loss=0.5781158317103398\n",
      "Gradient Descent(2856/4999): loss=0.5781155359521216\n",
      "Gradient Descent(2857/4999): loss=0.5781152402987321\n",
      "Gradient Descent(2858/4999): loss=0.5781149447500814\n",
      "Gradient Descent(2859/4999): loss=0.5781146493060804\n",
      "Gradient Descent(2860/4999): loss=0.57811435396664\n",
      "Gradient Descent(2861/4999): loss=0.5781140587316714\n",
      "Gradient Descent(2862/4999): loss=0.5781137636010858\n",
      "Gradient Descent(2863/4999): loss=0.5781134685747948\n",
      "Gradient Descent(2864/4999): loss=0.5781131736527099\n",
      "Gradient Descent(2865/4999): loss=0.5781128788347432\n",
      "Gradient Descent(2866/4999): loss=0.5781125841208063\n",
      "Gradient Descent(2867/4999): loss=0.578112289510812\n",
      "Gradient Descent(2868/4999): loss=0.5781119950046726\n",
      "Gradient Descent(2869/4999): loss=0.5781117006023004\n",
      "Gradient Descent(2870/4999): loss=0.5781114063036087\n",
      "Gradient Descent(2871/4999): loss=0.5781111121085101\n",
      "Gradient Descent(2872/4999): loss=0.578110818016918\n",
      "Gradient Descent(2873/4999): loss=0.5781105240287457\n",
      "Gradient Descent(2874/4999): loss=0.5781102301439067\n",
      "Gradient Descent(2875/4999): loss=0.578109936362315\n",
      "Gradient Descent(2876/4999): loss=0.5781096426838841\n",
      "Gradient Descent(2877/4999): loss=0.5781093491085285\n",
      "Gradient Descent(2878/4999): loss=0.5781090556361622\n",
      "Gradient Descent(2879/4999): loss=0.5781087622666997\n",
      "Gradient Descent(2880/4999): loss=0.578108469000056\n",
      "Gradient Descent(2881/4999): loss=0.5781081758361455\n",
      "Gradient Descent(2882/4999): loss=0.5781078827748833\n",
      "Gradient Descent(2883/4999): loss=0.5781075898161847\n",
      "Gradient Descent(2884/4999): loss=0.578107296959965\n",
      "Gradient Descent(2885/4999): loss=0.5781070042061398\n",
      "Gradient Descent(2886/4999): loss=0.5781067115546248\n",
      "Gradient Descent(2887/4999): loss=0.5781064190053358\n",
      "Gradient Descent(2888/4999): loss=0.5781061265581889\n",
      "Gradient Descent(2889/4999): loss=0.5781058342131006\n",
      "Gradient Descent(2890/4999): loss=0.5781055419699869\n",
      "Gradient Descent(2891/4999): loss=0.5781052498287647\n",
      "Gradient Descent(2892/4999): loss=0.5781049577893506\n",
      "Gradient Descent(2893/4999): loss=0.5781046658516618\n",
      "Gradient Descent(2894/4999): loss=0.5781043740156151\n",
      "Gradient Descent(2895/4999): loss=0.5781040822811281\n",
      "Gradient Descent(2896/4999): loss=0.5781037906481179\n",
      "Gradient Descent(2897/4999): loss=0.5781034991165024\n",
      "Gradient Descent(2898/4999): loss=0.5781032076861993\n",
      "Gradient Descent(2899/4999): loss=0.5781029163571265\n",
      "Gradient Descent(2900/4999): loss=0.5781026251292023\n",
      "Gradient Descent(2901/4999): loss=0.5781023340023451\n",
      "Gradient Descent(2902/4999): loss=0.578102042976473\n",
      "Gradient Descent(2903/4999): loss=0.5781017520515049\n",
      "Gradient Descent(2904/4999): loss=0.5781014612273596\n",
      "Gradient Descent(2905/4999): loss=0.5781011705039563\n",
      "Gradient Descent(2906/4999): loss=0.5781008798812135\n",
      "Gradient Descent(2907/4999): loss=0.578100589359051\n",
      "Gradient Descent(2908/4999): loss=0.5781002989373881\n",
      "Gradient Descent(2909/4999): loss=0.5781000086161445\n",
      "Gradient Descent(2910/4999): loss=0.5780997183952401\n",
      "Gradient Descent(2911/4999): loss=0.5780994282745948\n",
      "Gradient Descent(2912/4999): loss=0.5780991382541286\n",
      "Gradient Descent(2913/4999): loss=0.5780988483337619\n",
      "Gradient Descent(2914/4999): loss=0.5780985585134151\n",
      "Gradient Descent(2915/4999): loss=0.5780982687930089\n",
      "Gradient Descent(2916/4999): loss=0.578097979172464\n",
      "Gradient Descent(2917/4999): loss=0.5780976896517013\n",
      "Gradient Descent(2918/4999): loss=0.5780974002306418\n",
      "Gradient Descent(2919/4999): loss=0.578097110909207\n",
      "Gradient Descent(2920/4999): loss=0.5780968216873181\n",
      "Gradient Descent(2921/4999): loss=0.5780965325648969\n",
      "Gradient Descent(2922/4999): loss=0.5780962435418648\n",
      "Gradient Descent(2923/4999): loss=0.5780959546181439\n",
      "Gradient Descent(2924/4999): loss=0.5780956657936561\n",
      "Gradient Descent(2925/4999): loss=0.5780953770683237\n",
      "Gradient Descent(2926/4999): loss=0.5780950884420692\n",
      "Gradient Descent(2927/4999): loss=0.5780947999148146\n",
      "Gradient Descent(2928/4999): loss=0.5780945114864832\n",
      "Gradient Descent(2929/4999): loss=0.5780942231569971\n",
      "Gradient Descent(2930/4999): loss=0.5780939349262798\n",
      "Gradient Descent(2931/4999): loss=0.5780936467942542\n",
      "Gradient Descent(2932/4999): loss=0.5780933587608437\n",
      "Gradient Descent(2933/4999): loss=0.5780930708259714\n",
      "Gradient Descent(2934/4999): loss=0.5780927829895615\n",
      "Gradient Descent(2935/4999): loss=0.5780924952515369\n",
      "Gradient Descent(2936/4999): loss=0.578092207611822\n",
      "Gradient Descent(2937/4999): loss=0.5780919200703408\n",
      "Gradient Descent(2938/4999): loss=0.5780916326270173\n",
      "Gradient Descent(2939/4999): loss=0.5780913452817759\n",
      "Gradient Descent(2940/4999): loss=0.578091058034541\n",
      "Gradient Descent(2941/4999): loss=0.5780907708852373\n",
      "Gradient Descent(2942/4999): loss=0.5780904838337896\n",
      "Gradient Descent(2943/4999): loss=0.5780901968801225\n",
      "Gradient Descent(2944/4999): loss=0.5780899100241615\n",
      "Gradient Descent(2945/4999): loss=0.5780896232658315\n",
      "Gradient Descent(2946/4999): loss=0.5780893366050579\n",
      "Gradient Descent(2947/4999): loss=0.5780890500417663\n",
      "Gradient Descent(2948/4999): loss=0.5780887635758821\n",
      "Gradient Descent(2949/4999): loss=0.5780884772073314\n",
      "Gradient Descent(2950/4999): loss=0.5780881909360398\n",
      "Gradient Descent(2951/4999): loss=0.5780879047619336\n",
      "Gradient Descent(2952/4999): loss=0.5780876186849389\n",
      "Gradient Descent(2953/4999): loss=0.578087332704982\n",
      "Gradient Descent(2954/4999): loss=0.5780870468219895\n",
      "Gradient Descent(2955/4999): loss=0.5780867610358879\n",
      "Gradient Descent(2956/4999): loss=0.5780864753466043\n",
      "Gradient Descent(2957/4999): loss=0.5780861897540651\n",
      "Gradient Descent(2958/4999): loss=0.5780859042581977\n",
      "Gradient Descent(2959/4999): loss=0.5780856188589293\n",
      "Gradient Descent(2960/4999): loss=0.5780853335561871\n",
      "Gradient Descent(2961/4999): loss=0.5780850483498985\n",
      "Gradient Descent(2962/4999): loss=0.5780847632399915\n",
      "Gradient Descent(2963/4999): loss=0.5780844782263933\n",
      "Gradient Descent(2964/4999): loss=0.5780841933090322\n",
      "Gradient Descent(2965/4999): loss=0.578083908487836\n",
      "Gradient Descent(2966/4999): loss=0.5780836237627328\n",
      "Gradient Descent(2967/4999): loss=0.5780833391336511\n",
      "Gradient Descent(2968/4999): loss=0.5780830546005193\n",
      "Gradient Descent(2969/4999): loss=0.5780827701632659\n",
      "Gradient Descent(2970/4999): loss=0.5780824858218194\n",
      "Gradient Descent(2971/4999): loss=0.5780822015761091\n",
      "Gradient Descent(2972/4999): loss=0.5780819174260634\n",
      "Gradient Descent(2973/4999): loss=0.578081633371612\n",
      "Gradient Descent(2974/4999): loss=0.5780813494126835\n",
      "Gradient Descent(2975/4999): loss=0.5780810655492078\n",
      "Gradient Descent(2976/4999): loss=0.578080781781114\n",
      "Gradient Descent(2977/4999): loss=0.578080498108332\n",
      "Gradient Descent(2978/4999): loss=0.5780802145307914\n",
      "Gradient Descent(2979/4999): loss=0.578079931048422\n",
      "Gradient Descent(2980/4999): loss=0.5780796476611542\n",
      "Gradient Descent(2981/4999): loss=0.5780793643689176\n",
      "Gradient Descent(2982/4999): loss=0.5780790811716431\n",
      "Gradient Descent(2983/4999): loss=0.5780787980692604\n",
      "Gradient Descent(2984/4999): loss=0.5780785150617005\n",
      "Gradient Descent(2985/4999): loss=0.5780782321488941\n",
      "Gradient Descent(2986/4999): loss=0.5780779493307718\n",
      "Gradient Descent(2987/4999): loss=0.5780776666072646\n",
      "Gradient Descent(2988/4999): loss=0.5780773839783034\n",
      "Gradient Descent(2989/4999): loss=0.5780771014438196\n",
      "Gradient Descent(2990/4999): loss=0.5780768190037443\n",
      "Gradient Descent(2991/4999): loss=0.5780765366580091\n",
      "Gradient Descent(2992/4999): loss=0.5780762544065453\n",
      "Gradient Descent(2993/4999): loss=0.5780759722492849\n",
      "Gradient Descent(2994/4999): loss=0.5780756901861595\n",
      "Gradient Descent(2995/4999): loss=0.5780754082171009\n",
      "Gradient Descent(2996/4999): loss=0.5780751263420412\n",
      "Gradient Descent(2997/4999): loss=0.5780748445609128\n",
      "Gradient Descent(2998/4999): loss=0.5780745628736479\n",
      "Gradient Descent(2999/4999): loss=0.5780742812801788\n",
      "Gradient Descent(3000/4999): loss=0.578073999780438\n",
      "Gradient Descent(3001/4999): loss=0.5780737183743583\n",
      "Gradient Descent(3002/4999): loss=0.5780734370618724\n",
      "Gradient Descent(3003/4999): loss=0.5780731558429132\n",
      "Gradient Descent(3004/4999): loss=0.5780728747174139\n",
      "Gradient Descent(3005/4999): loss=0.5780725936853075\n",
      "Gradient Descent(3006/4999): loss=0.578072312746527\n",
      "Gradient Descent(3007/4999): loss=0.5780720319010061\n",
      "Gradient Descent(3008/4999): loss=0.5780717511486784\n",
      "Gradient Descent(3009/4999): loss=0.5780714704894773\n",
      "Gradient Descent(3010/4999): loss=0.5780711899233363\n",
      "Gradient Descent(3011/4999): loss=0.5780709094501898\n",
      "Gradient Descent(3012/4999): loss=0.5780706290699714\n",
      "Gradient Descent(3013/4999): loss=0.5780703487826153\n",
      "Gradient Descent(3014/4999): loss=0.5780700685880558\n",
      "Gradient Descent(3015/4999): loss=0.578069788486227\n",
      "Gradient Descent(3016/4999): loss=0.5780695084770636\n",
      "Gradient Descent(3017/4999): loss=0.5780692285604999\n",
      "Gradient Descent(3018/4999): loss=0.5780689487364707\n",
      "Gradient Descent(3019/4999): loss=0.5780686690049107\n",
      "Gradient Descent(3020/4999): loss=0.5780683893657551\n",
      "Gradient Descent(3021/4999): loss=0.5780681098189385\n",
      "Gradient Descent(3022/4999): loss=0.5780678303643961\n",
      "Gradient Descent(3023/4999): loss=0.5780675510020636\n",
      "Gradient Descent(3024/4999): loss=0.5780672717318758\n",
      "Gradient Descent(3025/4999): loss=0.5780669925537684\n",
      "Gradient Descent(3026/4999): loss=0.5780667134676768\n",
      "Gradient Descent(3027/4999): loss=0.5780664344735371\n",
      "Gradient Descent(3028/4999): loss=0.5780661555712846\n",
      "Gradient Descent(3029/4999): loss=0.5780658767608557\n",
      "Gradient Descent(3030/4999): loss=0.578065598042186\n",
      "Gradient Descent(3031/4999): loss=0.5780653194152121\n",
      "Gradient Descent(3032/4999): loss=0.5780650408798699\n",
      "Gradient Descent(3033/4999): loss=0.5780647624360957\n",
      "Gradient Descent(3034/4999): loss=0.5780644840838262\n",
      "Gradient Descent(3035/4999): loss=0.5780642058229979\n",
      "Gradient Descent(3036/4999): loss=0.5780639276535473\n",
      "Gradient Descent(3037/4999): loss=0.5780636495754116\n",
      "Gradient Descent(3038/4999): loss=0.5780633715885274\n",
      "Gradient Descent(3039/4999): loss=0.5780630936928317\n",
      "Gradient Descent(3040/4999): loss=0.5780628158882618\n",
      "Gradient Descent(3041/4999): loss=0.5780625381747548\n",
      "Gradient Descent(3042/4999): loss=0.578062260552248\n",
      "Gradient Descent(3043/4999): loss=0.578061983020679\n",
      "Gradient Descent(3044/4999): loss=0.578061705579985\n",
      "Gradient Descent(3045/4999): loss=0.5780614282301043\n",
      "Gradient Descent(3046/4999): loss=0.578061150970974\n",
      "Gradient Descent(3047/4999): loss=0.5780608738025323\n",
      "Gradient Descent(3048/4999): loss=0.5780605967247173\n",
      "Gradient Descent(3049/4999): loss=0.5780603197374666\n",
      "Gradient Descent(3050/4999): loss=0.5780600428407188\n",
      "Gradient Descent(3051/4999): loss=0.5780597660344119\n",
      "Gradient Descent(3052/4999): loss=0.5780594893184845\n",
      "Gradient Descent(3053/4999): loss=0.5780592126928751\n",
      "Gradient Descent(3054/4999): loss=0.5780589361575221\n",
      "Gradient Descent(3055/4999): loss=0.5780586597123645\n",
      "Gradient Descent(3056/4999): loss=0.5780583833573406\n",
      "Gradient Descent(3057/4999): loss=0.5780581070923898\n",
      "Gradient Descent(3058/4999): loss=0.5780578309174509\n",
      "Gradient Descent(3059/4999): loss=0.5780575548324629\n",
      "Gradient Descent(3060/4999): loss=0.5780572788373652\n",
      "Gradient Descent(3061/4999): loss=0.578057002932097\n",
      "Gradient Descent(3062/4999): loss=0.5780567271165978\n",
      "Gradient Descent(3063/4999): loss=0.5780564513908069\n",
      "Gradient Descent(3064/4999): loss=0.5780561757546641\n",
      "Gradient Descent(3065/4999): loss=0.5780559002081089\n",
      "Gradient Descent(3066/4999): loss=0.5780556247510816\n",
      "Gradient Descent(3067/4999): loss=0.5780553493835217\n",
      "Gradient Descent(3068/4999): loss=0.5780550741053692\n",
      "Gradient Descent(3069/4999): loss=0.5780547989165642\n",
      "Gradient Descent(3070/4999): loss=0.5780545238170469\n",
      "Gradient Descent(3071/4999): loss=0.578054248806758\n",
      "Gradient Descent(3072/4999): loss=0.5780539738856373\n",
      "Gradient Descent(3073/4999): loss=0.5780536990536256\n",
      "Gradient Descent(3074/4999): loss=0.5780534243106635\n",
      "Gradient Descent(3075/4999): loss=0.5780531496566917\n",
      "Gradient Descent(3076/4999): loss=0.578052875091651\n",
      "Gradient Descent(3077/4999): loss=0.5780526006154821\n",
      "Gradient Descent(3078/4999): loss=0.5780523262281262\n",
      "Gradient Descent(3079/4999): loss=0.578052051929524\n",
      "Gradient Descent(3080/4999): loss=0.5780517777196172\n",
      "Gradient Descent(3081/4999): loss=0.5780515035983467\n",
      "Gradient Descent(3082/4999): loss=0.5780512295656539\n",
      "Gradient Descent(3083/4999): loss=0.5780509556214805\n",
      "Gradient Descent(3084/4999): loss=0.5780506817657676\n",
      "Gradient Descent(3085/4999): loss=0.5780504079984572\n",
      "Gradient Descent(3086/4999): loss=0.578050134319491\n",
      "Gradient Descent(3087/4999): loss=0.5780498607288106\n",
      "Gradient Descent(3088/4999): loss=0.5780495872263581\n",
      "Gradient Descent(3089/4999): loss=0.5780493138120756\n",
      "Gradient Descent(3090/4999): loss=0.578049040485905\n",
      "Gradient Descent(3091/4999): loss=0.5780487672477885\n",
      "Gradient Descent(3092/4999): loss=0.5780484940976686\n",
      "Gradient Descent(3093/4999): loss=0.5780482210354875\n",
      "Gradient Descent(3094/4999): loss=0.5780479480611876\n",
      "Gradient Descent(3095/4999): loss=0.5780476751747117\n",
      "Gradient Descent(3096/4999): loss=0.5780474023760024\n",
      "Gradient Descent(3097/4999): loss=0.5780471296650023\n",
      "Gradient Descent(3098/4999): loss=0.5780468570416544\n",
      "Gradient Descent(3099/4999): loss=0.5780465845059014\n",
      "Gradient Descent(3100/4999): loss=0.5780463120576865\n",
      "Gradient Descent(3101/4999): loss=0.5780460396969528\n",
      "Gradient Descent(3102/4999): loss=0.5780457674236434\n",
      "Gradient Descent(3103/4999): loss=0.5780454952377015\n",
      "Gradient Descent(3104/4999): loss=0.5780452231390708\n",
      "Gradient Descent(3105/4999): loss=0.5780449511276945\n",
      "Gradient Descent(3106/4999): loss=0.5780446792035162\n",
      "Gradient Descent(3107/4999): loss=0.5780444073664796\n",
      "Gradient Descent(3108/4999): loss=0.5780441356165281\n",
      "Gradient Descent(3109/4999): loss=0.5780438639536059\n",
      "Gradient Descent(3110/4999): loss=0.5780435923776568\n",
      "Gradient Descent(3111/4999): loss=0.5780433208886248\n",
      "Gradient Descent(3112/4999): loss=0.5780430494864537\n",
      "Gradient Descent(3113/4999): loss=0.5780427781710881\n",
      "Gradient Descent(3114/4999): loss=0.5780425069424718\n",
      "Gradient Descent(3115/4999): loss=0.5780422358005494\n",
      "Gradient Descent(3116/4999): loss=0.5780419647452653\n",
      "Gradient Descent(3117/4999): loss=0.5780416937765639\n",
      "Gradient Descent(3118/4999): loss=0.5780414228943898\n",
      "Gradient Descent(3119/4999): loss=0.5780411520986876\n",
      "Gradient Descent(3120/4999): loss=0.5780408813894024\n",
      "Gradient Descent(3121/4999): loss=0.5780406107664785\n",
      "Gradient Descent(3122/4999): loss=0.5780403402298613\n",
      "Gradient Descent(3123/4999): loss=0.5780400697794954\n",
      "Gradient Descent(3124/4999): loss=0.5780397994153262\n",
      "Gradient Descent(3125/4999): loss=0.5780395291372988\n",
      "Gradient Descent(3126/4999): loss=0.5780392589453585\n",
      "Gradient Descent(3127/4999): loss=0.5780389888394503\n",
      "Gradient Descent(3128/4999): loss=0.57803871881952\n",
      "Gradient Descent(3129/4999): loss=0.5780384488855129\n",
      "Gradient Descent(3130/4999): loss=0.5780381790373746\n",
      "Gradient Descent(3131/4999): loss=0.578037909275051\n",
      "Gradient Descent(3132/4999): loss=0.5780376395984876\n",
      "Gradient Descent(3133/4999): loss=0.5780373700076301\n",
      "Gradient Descent(3134/4999): loss=0.5780371005024248\n",
      "Gradient Descent(3135/4999): loss=0.5780368310828174\n",
      "Gradient Descent(3136/4999): loss=0.5780365617487542\n",
      "Gradient Descent(3137/4999): loss=0.5780362925001808\n",
      "Gradient Descent(3138/4999): loss=0.5780360233370442\n",
      "Gradient Descent(3139/4999): loss=0.5780357542592901\n",
      "Gradient Descent(3140/4999): loss=0.5780354852668654\n",
      "Gradient Descent(3141/4999): loss=0.5780352163597162\n",
      "Gradient Descent(3142/4999): loss=0.5780349475377891\n",
      "Gradient Descent(3143/4999): loss=0.5780346788010307\n",
      "Gradient Descent(3144/4999): loss=0.578034410149388\n",
      "Gradient Descent(3145/4999): loss=0.5780341415828073\n",
      "Gradient Descent(3146/4999): loss=0.5780338731012358\n",
      "Gradient Descent(3147/4999): loss=0.5780336047046204\n",
      "Gradient Descent(3148/4999): loss=0.578033336392908\n",
      "Gradient Descent(3149/4999): loss=0.5780330681660459\n",
      "Gradient Descent(3150/4999): loss=0.5780328000239812\n",
      "Gradient Descent(3151/4999): loss=0.5780325319666609\n",
      "Gradient Descent(3152/4999): loss=0.5780322639940326\n",
      "Gradient Descent(3153/4999): loss=0.5780319961060437\n",
      "Gradient Descent(3154/4999): loss=0.5780317283026415\n",
      "Gradient Descent(3155/4999): loss=0.5780314605837737\n",
      "Gradient Descent(3156/4999): loss=0.5780311929493881\n",
      "Gradient Descent(3157/4999): loss=0.578030925399432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3158/4999): loss=0.5780306579338533\n",
      "Gradient Descent(3159/4999): loss=0.5780303905526002\n",
      "Gradient Descent(3160/4999): loss=0.5780301232556203\n",
      "Gradient Descent(3161/4999): loss=0.5780298560428617\n",
      "Gradient Descent(3162/4999): loss=0.5780295889142724\n",
      "Gradient Descent(3163/4999): loss=0.5780293218698007\n",
      "Gradient Descent(3164/4999): loss=0.5780290549093946\n",
      "Gradient Descent(3165/4999): loss=0.5780287880330028\n",
      "Gradient Descent(3166/4999): loss=0.5780285212405732\n",
      "Gradient Descent(3167/4999): loss=0.5780282545320549\n",
      "Gradient Descent(3168/4999): loss=0.5780279879073957\n",
      "Gradient Descent(3169/4999): loss=0.5780277213665446\n",
      "Gradient Descent(3170/4999): loss=0.5780274549094503\n",
      "Gradient Descent(3171/4999): loss=0.5780271885360614\n",
      "Gradient Descent(3172/4999): loss=0.5780269222463267\n",
      "Gradient Descent(3173/4999): loss=0.5780266560401952\n",
      "Gradient Descent(3174/4999): loss=0.5780263899176159\n",
      "Gradient Descent(3175/4999): loss=0.5780261238785377\n",
      "Gradient Descent(3176/4999): loss=0.5780258579229097\n",
      "Gradient Descent(3177/4999): loss=0.5780255920506813\n",
      "Gradient Descent(3178/4999): loss=0.5780253262618016\n",
      "Gradient Descent(3179/4999): loss=0.5780250605562197\n",
      "Gradient Descent(3180/4999): loss=0.5780247949338855\n",
      "Gradient Descent(3181/4999): loss=0.5780245293947479\n",
      "Gradient Descent(3182/4999): loss=0.5780242639387568\n",
      "Gradient Descent(3183/4999): loss=0.5780239985658616\n",
      "Gradient Descent(3184/4999): loss=0.5780237332760123\n",
      "Gradient Descent(3185/4999): loss=0.5780234680691583\n",
      "Gradient Descent(3186/4999): loss=0.5780232029452497\n",
      "Gradient Descent(3187/4999): loss=0.578022937904236\n",
      "Gradient Descent(3188/4999): loss=0.5780226729460674\n",
      "Gradient Descent(3189/4999): loss=0.578022408070694\n",
      "Gradient Descent(3190/4999): loss=0.5780221432780657\n",
      "Gradient Descent(3191/4999): loss=0.5780218785681327\n",
      "Gradient Descent(3192/4999): loss=0.5780216139408452\n",
      "Gradient Descent(3193/4999): loss=0.5780213493961537\n",
      "Gradient Descent(3194/4999): loss=0.5780210849340083\n",
      "Gradient Descent(3195/4999): loss=0.5780208205543598\n",
      "Gradient Descent(3196/4999): loss=0.5780205562571582\n",
      "Gradient Descent(3197/4999): loss=0.5780202920423545\n",
      "Gradient Descent(3198/4999): loss=0.5780200279098989\n",
      "Gradient Descent(3199/4999): loss=0.5780197638597425\n",
      "Gradient Descent(3200/4999): loss=0.5780194998918361\n",
      "Gradient Descent(3201/4999): loss=0.5780192360061301\n",
      "Gradient Descent(3202/4999): loss=0.5780189722025758\n",
      "Gradient Descent(3203/4999): loss=0.578018708481124\n",
      "Gradient Descent(3204/4999): loss=0.5780184448417258\n",
      "Gradient Descent(3205/4999): loss=0.5780181812843322\n",
      "Gradient Descent(3206/4999): loss=0.5780179178088946\n",
      "Gradient Descent(3207/4999): loss=0.5780176544153638\n",
      "Gradient Descent(3208/4999): loss=0.5780173911036915\n",
      "Gradient Descent(3209/4999): loss=0.578017127873829\n",
      "Gradient Descent(3210/4999): loss=0.5780168647257276\n",
      "Gradient Descent(3211/4999): loss=0.5780166016593388\n",
      "Gradient Descent(3212/4999): loss=0.5780163386746142\n",
      "Gradient Descent(3213/4999): loss=0.5780160757715054\n",
      "Gradient Descent(3214/4999): loss=0.5780158129499641\n",
      "Gradient Descent(3215/4999): loss=0.5780155502099421\n",
      "Gradient Descent(3216/4999): loss=0.5780152875513913\n",
      "Gradient Descent(3217/4999): loss=0.5780150249742634\n",
      "Gradient Descent(3218/4999): loss=0.5780147624785102\n",
      "Gradient Descent(3219/4999): loss=0.5780145000640841\n",
      "Gradient Descent(3220/4999): loss=0.578014237730937\n",
      "Gradient Descent(3221/4999): loss=0.5780139754790209\n",
      "Gradient Descent(3222/4999): loss=0.5780137133082881\n",
      "Gradient Descent(3223/4999): loss=0.578013451218691\n",
      "Gradient Descent(3224/4999): loss=0.5780131892101816\n",
      "Gradient Descent(3225/4999): loss=0.5780129272827128\n",
      "Gradient Descent(3226/4999): loss=0.5780126654362363\n",
      "Gradient Descent(3227/4999): loss=0.5780124036707054\n",
      "Gradient Descent(3228/4999): loss=0.5780121419860721\n",
      "Gradient Descent(3229/4999): loss=0.5780118803822891\n",
      "Gradient Descent(3230/4999): loss=0.5780116188593094\n",
      "Gradient Descent(3231/4999): loss=0.5780113574170856\n",
      "Gradient Descent(3232/4999): loss=0.5780110960555704\n",
      "Gradient Descent(3233/4999): loss=0.5780108347747167\n",
      "Gradient Descent(3234/4999): loss=0.5780105735744777\n",
      "Gradient Descent(3235/4999): loss=0.5780103124548062\n",
      "Gradient Descent(3236/4999): loss=0.5780100514156551\n",
      "Gradient Descent(3237/4999): loss=0.5780097904569778\n",
      "Gradient Descent(3238/4999): loss=0.5780095295787273\n",
      "Gradient Descent(3239/4999): loss=0.5780092687808571\n",
      "Gradient Descent(3240/4999): loss=0.57800900806332\n",
      "Gradient Descent(3241/4999): loss=0.5780087474260699\n",
      "Gradient Descent(3242/4999): loss=0.57800848686906\n",
      "Gradient Descent(3243/4999): loss=0.5780082263922437\n",
      "Gradient Descent(3244/4999): loss=0.5780079659955747\n",
      "Gradient Descent(3245/4999): loss=0.5780077056790064\n",
      "Gradient Descent(3246/4999): loss=0.5780074454424925\n",
      "Gradient Descent(3247/4999): loss=0.5780071852859869\n",
      "Gradient Descent(3248/4999): loss=0.5780069252094433\n",
      "Gradient Descent(3249/4999): loss=0.5780066652128155\n",
      "Gradient Descent(3250/4999): loss=0.5780064052960573\n",
      "Gradient Descent(3251/4999): loss=0.5780061454591228\n",
      "Gradient Descent(3252/4999): loss=0.5780058857019658\n",
      "Gradient Descent(3253/4999): loss=0.5780056260245406\n",
      "Gradient Descent(3254/4999): loss=0.5780053664268012\n",
      "Gradient Descent(3255/4999): loss=0.578005106908702\n",
      "Gradient Descent(3256/4999): loss=0.5780048474701969\n",
      "Gradient Descent(3257/4999): loss=0.5780045881112402\n",
      "Gradient Descent(3258/4999): loss=0.5780043288317867\n",
      "Gradient Descent(3259/4999): loss=0.5780040696317902\n",
      "Gradient Descent(3260/4999): loss=0.5780038105112056\n",
      "Gradient Descent(3261/4999): loss=0.5780035514699873\n",
      "Gradient Descent(3262/4999): loss=0.57800329250809\n",
      "Gradient Descent(3263/4999): loss=0.5780030336254681\n",
      "Gradient Descent(3264/4999): loss=0.5780027748220764\n",
      "Gradient Descent(3265/4999): loss=0.5780025160978697\n",
      "Gradient Descent(3266/4999): loss=0.5780022574528028\n",
      "Gradient Descent(3267/4999): loss=0.5780019988868303\n",
      "Gradient Descent(3268/4999): loss=0.5780017403999075\n",
      "Gradient Descent(3269/4999): loss=0.5780014819919892\n",
      "Gradient Descent(3270/4999): loss=0.5780012236630304\n",
      "Gradient Descent(3271/4999): loss=0.5780009654129862\n",
      "Gradient Descent(3272/4999): loss=0.5780007072418117\n",
      "Gradient Descent(3273/4999): loss=0.5780004491494624\n",
      "Gradient Descent(3274/4999): loss=0.5780001911358928\n",
      "Gradient Descent(3275/4999): loss=0.5779999332010592\n",
      "Gradient Descent(3276/4999): loss=0.5779996753449161\n",
      "Gradient Descent(3277/4999): loss=0.5779994175674193\n",
      "Gradient Descent(3278/4999): loss=0.5779991598685243\n",
      "Gradient Descent(3279/4999): loss=0.5779989022481866\n",
      "Gradient Descent(3280/4999): loss=0.5779986447063615\n",
      "Gradient Descent(3281/4999): loss=0.5779983872430051\n",
      "Gradient Descent(3282/4999): loss=0.5779981298580728\n",
      "Gradient Descent(3283/4999): loss=0.57799787255152\n",
      "Gradient Descent(3284/4999): loss=0.577997615323303\n",
      "Gradient Descent(3285/4999): loss=0.5779973581733774\n",
      "Gradient Descent(3286/4999): loss=0.5779971011016992\n",
      "Gradient Descent(3287/4999): loss=0.5779968441082244\n",
      "Gradient Descent(3288/4999): loss=0.5779965871929087\n",
      "Gradient Descent(3289/4999): loss=0.5779963303557086\n",
      "Gradient Descent(3290/4999): loss=0.5779960735965797\n",
      "Gradient Descent(3291/4999): loss=0.5779958169154785\n",
      "Gradient Descent(3292/4999): loss=0.5779955603123611\n",
      "Gradient Descent(3293/4999): loss=0.5779953037871837\n",
      "Gradient Descent(3294/4999): loss=0.5779950473399028\n",
      "Gradient Descent(3295/4999): loss=0.5779947909704745\n",
      "Gradient Descent(3296/4999): loss=0.5779945346788554\n",
      "Gradient Descent(3297/4999): loss=0.577994278465002\n",
      "Gradient Descent(3298/4999): loss=0.5779940223288706\n",
      "Gradient Descent(3299/4999): loss=0.5779937662704179\n",
      "Gradient Descent(3300/4999): loss=0.5779935102896006\n",
      "Gradient Descent(3301/4999): loss=0.5779932543863752\n",
      "Gradient Descent(3302/4999): loss=0.5779929985606985\n",
      "Gradient Descent(3303/4999): loss=0.5779927428125274\n",
      "Gradient Descent(3304/4999): loss=0.5779924871418185\n",
      "Gradient Descent(3305/4999): loss=0.5779922315485289\n",
      "Gradient Descent(3306/4999): loss=0.5779919760326152\n",
      "Gradient Descent(3307/4999): loss=0.5779917205940346\n",
      "Gradient Descent(3308/4999): loss=0.5779914652327443\n",
      "Gradient Descent(3309/4999): loss=0.5779912099487009\n",
      "Gradient Descent(3310/4999): loss=0.5779909547418619\n",
      "Gradient Descent(3311/4999): loss=0.5779906996121843\n",
      "Gradient Descent(3312/4999): loss=0.5779904445596254\n",
      "Gradient Descent(3313/4999): loss=0.5779901895841425\n",
      "Gradient Descent(3314/4999): loss=0.5779899346856928\n",
      "Gradient Descent(3315/4999): loss=0.5779896798642338\n",
      "Gradient Descent(3316/4999): loss=0.5779894251197228\n",
      "Gradient Descent(3317/4999): loss=0.5779891704521174\n",
      "Gradient Descent(3318/4999): loss=0.5779889158613749\n",
      "Gradient Descent(3319/4999): loss=0.5779886613474532\n",
      "Gradient Descent(3320/4999): loss=0.5779884069103095\n",
      "Gradient Descent(3321/4999): loss=0.5779881525499018\n",
      "Gradient Descent(3322/4999): loss=0.5779878982661878\n",
      "Gradient Descent(3323/4999): loss=0.577987644059125\n",
      "Gradient Descent(3324/4999): loss=0.5779873899286714\n",
      "Gradient Descent(3325/4999): loss=0.5779871358747848\n",
      "Gradient Descent(3326/4999): loss=0.5779868818974232\n",
      "Gradient Descent(3327/4999): loss=0.5779866279965443\n",
      "Gradient Descent(3328/4999): loss=0.5779863741721062\n",
      "Gradient Descent(3329/4999): loss=0.5779861204240672\n",
      "Gradient Descent(3330/4999): loss=0.5779858667523851\n",
      "Gradient Descent(3331/4999): loss=0.5779856131570181\n",
      "Gradient Descent(3332/4999): loss=0.5779853596379245\n",
      "Gradient Descent(3333/4999): loss=0.5779851061950625\n",
      "Gradient Descent(3334/4999): loss=0.5779848528283901\n",
      "Gradient Descent(3335/4999): loss=0.5779845995378661\n",
      "Gradient Descent(3336/4999): loss=0.5779843463234485\n",
      "Gradient Descent(3337/4999): loss=0.5779840931850957\n",
      "Gradient Descent(3338/4999): loss=0.5779838401227666\n",
      "Gradient Descent(3339/4999): loss=0.5779835871364193\n",
      "Gradient Descent(3340/4999): loss=0.5779833342260126\n",
      "Gradient Descent(3341/4999): loss=0.5779830813915048\n",
      "Gradient Descent(3342/4999): loss=0.5779828286328548\n",
      "Gradient Descent(3343/4999): loss=0.5779825759500211\n",
      "Gradient Descent(3344/4999): loss=0.5779823233429627\n",
      "Gradient Descent(3345/4999): loss=0.5779820708116383\n",
      "Gradient Descent(3346/4999): loss=0.5779818183560067\n",
      "Gradient Descent(3347/4999): loss=0.5779815659760267\n",
      "Gradient Descent(3348/4999): loss=0.5779813136716575\n",
      "Gradient Descent(3349/4999): loss=0.5779810614428575\n",
      "Gradient Descent(3350/4999): loss=0.5779808092895863\n",
      "Gradient Descent(3351/4999): loss=0.5779805572118029\n",
      "Gradient Descent(3352/4999): loss=0.5779803052094659\n",
      "Gradient Descent(3353/4999): loss=0.5779800532825351\n",
      "Gradient Descent(3354/4999): loss=0.577979801430969\n",
      "Gradient Descent(3355/4999): loss=0.5779795496547275\n",
      "Gradient Descent(3356/4999): loss=0.5779792979537695\n",
      "Gradient Descent(3357/4999): loss=0.5779790463280544\n",
      "Gradient Descent(3358/4999): loss=0.5779787947775415\n",
      "Gradient Descent(3359/4999): loss=0.5779785433021905\n",
      "Gradient Descent(3360/4999): loss=0.5779782919019604\n",
      "Gradient Descent(3361/4999): loss=0.5779780405768111\n",
      "Gradient Descent(3362/4999): loss=0.577977789326702\n",
      "Gradient Descent(3363/4999): loss=0.5779775381515927\n",
      "Gradient Descent(3364/4999): loss=0.5779772870514428\n",
      "Gradient Descent(3365/4999): loss=0.577977036026212\n",
      "Gradient Descent(3366/4999): loss=0.57797678507586\n",
      "Gradient Descent(3367/4999): loss=0.5779765342003466\n",
      "Gradient Descent(3368/4999): loss=0.5779762833996315\n",
      "Gradient Descent(3369/4999): loss=0.5779760326736746\n",
      "Gradient Descent(3370/4999): loss=0.5779757820224359\n",
      "Gradient Descent(3371/4999): loss=0.5779755314458752\n",
      "Gradient Descent(3372/4999): loss=0.5779752809439526\n",
      "Gradient Descent(3373/4999): loss=0.5779750305166278\n",
      "Gradient Descent(3374/4999): loss=0.5779747801638613\n",
      "Gradient Descent(3375/4999): loss=0.5779745298856129\n",
      "Gradient Descent(3376/4999): loss=0.5779742796818428\n",
      "Gradient Descent(3377/4999): loss=0.5779740295525113\n",
      "Gradient Descent(3378/4999): loss=0.5779737794975787\n",
      "Gradient Descent(3379/4999): loss=0.577973529517005\n",
      "Gradient Descent(3380/4999): loss=0.5779732796107505\n",
      "Gradient Descent(3381/4999): loss=0.5779730297787757\n",
      "Gradient Descent(3382/4999): loss=0.5779727800210411\n",
      "Gradient Descent(3383/4999): loss=0.5779725303375071\n",
      "Gradient Descent(3384/4999): loss=0.5779722807281339\n",
      "Gradient Descent(3385/4999): loss=0.5779720311928823\n",
      "Gradient Descent(3386/4999): loss=0.5779717817317129\n",
      "Gradient Descent(3387/4999): loss=0.5779715323445861\n",
      "Gradient Descent(3388/4999): loss=0.5779712830314625\n",
      "Gradient Descent(3389/4999): loss=0.577971033792303\n",
      "Gradient Descent(3390/4999): loss=0.5779707846270682\n",
      "Gradient Descent(3391/4999): loss=0.577970535535719\n",
      "Gradient Descent(3392/4999): loss=0.5779702865182159\n",
      "Gradient Descent(3393/4999): loss=0.5779700375745199\n",
      "Gradient Descent(3394/4999): loss=0.5779697887045921\n",
      "Gradient Descent(3395/4999): loss=0.5779695399083934\n",
      "Gradient Descent(3396/4999): loss=0.5779692911858842\n",
      "Gradient Descent(3397/4999): loss=0.5779690425370264\n",
      "Gradient Descent(3398/4999): loss=0.5779687939617802\n",
      "Gradient Descent(3399/4999): loss=0.5779685454601071\n",
      "Gradient Descent(3400/4999): loss=0.5779682970319684\n",
      "Gradient Descent(3401/4999): loss=0.5779680486773251\n",
      "Gradient Descent(3402/4999): loss=0.5779678003961383\n",
      "Gradient Descent(3403/4999): loss=0.5779675521883691\n",
      "Gradient Descent(3404/4999): loss=0.5779673040539791\n",
      "Gradient Descent(3405/4999): loss=0.5779670559929295\n",
      "Gradient Descent(3406/4999): loss=0.5779668080051819\n",
      "Gradient Descent(3407/4999): loss=0.5779665600906971\n",
      "Gradient Descent(3408/4999): loss=0.5779663122494372\n",
      "Gradient Descent(3409/4999): loss=0.5779660644813633\n",
      "Gradient Descent(3410/4999): loss=0.577965816786437\n",
      "Gradient Descent(3411/4999): loss=0.57796556916462\n",
      "Gradient Descent(3412/4999): loss=0.5779653216158738\n",
      "Gradient Descent(3413/4999): loss=0.5779650741401599\n",
      "Gradient Descent(3414/4999): loss=0.57796482673744\n",
      "Gradient Descent(3415/4999): loss=0.5779645794076762\n",
      "Gradient Descent(3416/4999): loss=0.5779643321508298\n",
      "Gradient Descent(3417/4999): loss=0.5779640849668626\n",
      "Gradient Descent(3418/4999): loss=0.5779638378557367\n",
      "Gradient Descent(3419/4999): loss=0.5779635908174139\n",
      "Gradient Descent(3420/4999): loss=0.5779633438518559\n",
      "Gradient Descent(3421/4999): loss=0.5779630969590248\n",
      "Gradient Descent(3422/4999): loss=0.5779628501388826\n",
      "Gradient Descent(3423/4999): loss=0.5779626033913913\n",
      "Gradient Descent(3424/4999): loss=0.5779623567165129\n",
      "Gradient Descent(3425/4999): loss=0.5779621101142095\n",
      "Gradient Descent(3426/4999): loss=0.5779618635844432\n",
      "Gradient Descent(3427/4999): loss=0.5779616171271764\n",
      "Gradient Descent(3428/4999): loss=0.577961370742371\n",
      "Gradient Descent(3429/4999): loss=0.5779611244299894\n",
      "Gradient Descent(3430/4999): loss=0.5779608781899939\n",
      "Gradient Descent(3431/4999): loss=0.5779606320223466\n",
      "Gradient Descent(3432/4999): loss=0.5779603859270102\n",
      "Gradient Descent(3433/4999): loss=0.5779601399039468\n",
      "Gradient Descent(3434/4999): loss=0.5779598939531189\n",
      "Gradient Descent(3435/4999): loss=0.5779596480744889\n",
      "Gradient Descent(3436/4999): loss=0.5779594022680196\n",
      "Gradient Descent(3437/4999): loss=0.5779591565336732\n",
      "Gradient Descent(3438/4999): loss=0.5779589108714125\n",
      "Gradient Descent(3439/4999): loss=0.5779586652811999\n",
      "Gradient Descent(3440/4999): loss=0.5779584197629982\n",
      "Gradient Descent(3441/4999): loss=0.57795817431677\n",
      "Gradient Descent(3442/4999): loss=0.577957928942478\n",
      "Gradient Descent(3443/4999): loss=0.5779576836400852\n",
      "Gradient Descent(3444/4999): loss=0.5779574384095539\n",
      "Gradient Descent(3445/4999): loss=0.5779571932508474\n",
      "Gradient Descent(3446/4999): loss=0.5779569481639283\n",
      "Gradient Descent(3447/4999): loss=0.5779567031487598\n",
      "Gradient Descent(3448/4999): loss=0.5779564582053044\n",
      "Gradient Descent(3449/4999): loss=0.5779562133335253\n",
      "Gradient Descent(3450/4999): loss=0.5779559685333853\n",
      "Gradient Descent(3451/4999): loss=0.5779557238048479\n",
      "Gradient Descent(3452/4999): loss=0.5779554791478759\n",
      "Gradient Descent(3453/4999): loss=0.5779552345624323\n",
      "Gradient Descent(3454/4999): loss=0.5779549900484804\n",
      "Gradient Descent(3455/4999): loss=0.5779547456059831\n",
      "Gradient Descent(3456/4999): loss=0.577954501234904\n",
      "Gradient Descent(3457/4999): loss=0.5779542569352062\n",
      "Gradient Descent(3458/4999): loss=0.577954012706853\n",
      "Gradient Descent(3459/4999): loss=0.5779537685498074\n",
      "Gradient Descent(3460/4999): loss=0.5779535244640335\n",
      "Gradient Descent(3461/4999): loss=0.5779532804494939\n",
      "Gradient Descent(3462/4999): loss=0.5779530365061525\n",
      "Gradient Descent(3463/4999): loss=0.5779527926339726\n",
      "Gradient Descent(3464/4999): loss=0.5779525488329177\n",
      "Gradient Descent(3465/4999): loss=0.5779523051029513\n",
      "Gradient Descent(3466/4999): loss=0.5779520614440369\n",
      "Gradient Descent(3467/4999): loss=0.5779518178561383\n",
      "Gradient Descent(3468/4999): loss=0.577951574339219\n",
      "Gradient Descent(3469/4999): loss=0.5779513308932427\n",
      "Gradient Descent(3470/4999): loss=0.5779510875181729\n",
      "Gradient Descent(3471/4999): loss=0.5779508442139736\n",
      "Gradient Descent(3472/4999): loss=0.5779506009806085\n",
      "Gradient Descent(3473/4999): loss=0.5779503578180413\n",
      "Gradient Descent(3474/4999): loss=0.5779501147262358\n",
      "Gradient Descent(3475/4999): loss=0.5779498717051562\n",
      "Gradient Descent(3476/4999): loss=0.577949628754766\n",
      "Gradient Descent(3477/4999): loss=0.5779493858750293\n",
      "Gradient Descent(3478/4999): loss=0.57794914306591\n",
      "Gradient Descent(3479/4999): loss=0.5779489003273722\n",
      "Gradient Descent(3480/4999): loss=0.5779486576593797\n",
      "Gradient Descent(3481/4999): loss=0.5779484150618971\n",
      "Gradient Descent(3482/4999): loss=0.5779481725348878\n",
      "Gradient Descent(3483/4999): loss=0.5779479300783164\n",
      "Gradient Descent(3484/4999): loss=0.5779476876921469\n",
      "Gradient Descent(3485/4999): loss=0.5779474453763435\n",
      "Gradient Descent(3486/4999): loss=0.5779472031308706\n",
      "Gradient Descent(3487/4999): loss=0.577946960955692\n",
      "Gradient Descent(3488/4999): loss=0.5779467188507726\n",
      "Gradient Descent(3489/4999): loss=0.5779464768160764\n",
      "Gradient Descent(3490/4999): loss=0.5779462348515675\n",
      "Gradient Descent(3491/4999): loss=0.5779459929572109\n",
      "Gradient Descent(3492/4999): loss=0.5779457511329706\n",
      "Gradient Descent(3493/4999): loss=0.5779455093788112\n",
      "Gradient Descent(3494/4999): loss=0.577945267694697\n",
      "Gradient Descent(3495/4999): loss=0.5779450260805928\n",
      "Gradient Descent(3496/4999): loss=0.577944784536463\n",
      "Gradient Descent(3497/4999): loss=0.5779445430622722\n",
      "Gradient Descent(3498/4999): loss=0.5779443016579849\n",
      "Gradient Descent(3499/4999): loss=0.5779440603235659\n",
      "Gradient Descent(3500/4999): loss=0.5779438190589801\n",
      "Gradient Descent(3501/4999): loss=0.5779435778641916\n",
      "Gradient Descent(3502/4999): loss=0.5779433367391658\n",
      "Gradient Descent(3503/4999): loss=0.5779430956838669\n",
      "Gradient Descent(3504/4999): loss=0.5779428546982601\n",
      "Gradient Descent(3505/4999): loss=0.5779426137823102\n",
      "Gradient Descent(3506/4999): loss=0.5779423729359819\n",
      "Gradient Descent(3507/4999): loss=0.5779421321592402\n",
      "Gradient Descent(3508/4999): loss=0.57794189145205\n",
      "Gradient Descent(3509/4999): loss=0.5779416508143763\n",
      "Gradient Descent(3510/4999): loss=0.577941410246184\n",
      "Gradient Descent(3511/4999): loss=0.5779411697474383\n",
      "Gradient Descent(3512/4999): loss=0.5779409293181039\n",
      "Gradient Descent(3513/4999): loss=0.5779406889581464\n",
      "Gradient Descent(3514/4999): loss=0.5779404486675306\n",
      "Gradient Descent(3515/4999): loss=0.5779402084462216\n",
      "Gradient Descent(3516/4999): loss=0.5779399682941847\n",
      "Gradient Descent(3517/4999): loss=0.5779397282113851\n",
      "Gradient Descent(3518/4999): loss=0.5779394881977881\n",
      "Gradient Descent(3519/4999): loss=0.5779392482533589\n",
      "Gradient Descent(3520/4999): loss=0.5779390083780627\n",
      "Gradient Descent(3521/4999): loss=0.5779387685718649\n",
      "Gradient Descent(3522/4999): loss=0.5779385288347308\n",
      "Gradient Descent(3523/4999): loss=0.5779382891666262\n",
      "Gradient Descent(3524/4999): loss=0.5779380495675159\n",
      "Gradient Descent(3525/4999): loss=0.5779378100373658\n",
      "Gradient Descent(3526/4999): loss=0.577937570576141\n",
      "Gradient Descent(3527/4999): loss=0.5779373311838076\n",
      "Gradient Descent(3528/4999): loss=0.5779370918603306\n",
      "Gradient Descent(3529/4999): loss=0.5779368526056757\n",
      "Gradient Descent(3530/4999): loss=0.5779366134198087\n",
      "Gradient Descent(3531/4999): loss=0.5779363743026951\n",
      "Gradient Descent(3532/4999): loss=0.5779361352543003\n",
      "Gradient Descent(3533/4999): loss=0.5779358962745904\n",
      "Gradient Descent(3534/4999): loss=0.5779356573635309\n",
      "Gradient Descent(3535/4999): loss=0.5779354185210879\n",
      "Gradient Descent(3536/4999): loss=0.5779351797472265\n",
      "Gradient Descent(3537/4999): loss=0.577934941041913\n",
      "Gradient Descent(3538/4999): loss=0.5779347024051134\n",
      "Gradient Descent(3539/4999): loss=0.5779344638367929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3540/4999): loss=0.577934225336918\n",
      "Gradient Descent(3541/4999): loss=0.5779339869054543\n",
      "Gradient Descent(3542/4999): loss=0.577933748542368\n",
      "Gradient Descent(3543/4999): loss=0.577933510247625\n",
      "Gradient Descent(3544/4999): loss=0.5779332720211912\n",
      "Gradient Descent(3545/4999): loss=0.5779330338630327\n",
      "Gradient Descent(3546/4999): loss=0.5779327957731155\n",
      "Gradient Descent(3547/4999): loss=0.5779325577514058\n",
      "Gradient Descent(3548/4999): loss=0.5779323197978699\n",
      "Gradient Descent(3549/4999): loss=0.5779320819124736\n",
      "Gradient Descent(3550/4999): loss=0.5779318440951833\n",
      "Gradient Descent(3551/4999): loss=0.5779316063459651\n",
      "Gradient Descent(3552/4999): loss=0.5779313686647854\n",
      "Gradient Descent(3553/4999): loss=0.5779311310516104\n",
      "Gradient Descent(3554/4999): loss=0.5779308935064061\n",
      "Gradient Descent(3555/4999): loss=0.5779306560291394\n",
      "Gradient Descent(3556/4999): loss=0.5779304186197762\n",
      "Gradient Descent(3557/4999): loss=0.5779301812782832\n",
      "Gradient Descent(3558/4999): loss=0.5779299440046264\n",
      "Gradient Descent(3559/4999): loss=0.5779297067987726\n",
      "Gradient Descent(3560/4999): loss=0.5779294696606883\n",
      "Gradient Descent(3561/4999): loss=0.5779292325903397\n",
      "Gradient Descent(3562/4999): loss=0.5779289955876934\n",
      "Gradient Descent(3563/4999): loss=0.5779287586527162\n",
      "Gradient Descent(3564/4999): loss=0.5779285217853743\n",
      "Gradient Descent(3565/4999): loss=0.5779282849856349\n",
      "Gradient Descent(3566/4999): loss=0.5779280482534639\n",
      "Gradient Descent(3567/4999): loss=0.5779278115888284\n",
      "Gradient Descent(3568/4999): loss=0.577927574991695\n",
      "Gradient Descent(3569/4999): loss=0.5779273384620304\n",
      "Gradient Descent(3570/4999): loss=0.5779271019998015\n",
      "Gradient Descent(3571/4999): loss=0.5779268656049746\n",
      "Gradient Descent(3572/4999): loss=0.5779266292775171\n",
      "Gradient Descent(3573/4999): loss=0.5779263930173957\n",
      "Gradient Descent(3574/4999): loss=0.5779261568245768\n",
      "Gradient Descent(3575/4999): loss=0.5779259206990278\n",
      "Gradient Descent(3576/4999): loss=0.5779256846407154\n",
      "Gradient Descent(3577/4999): loss=0.5779254486496065\n",
      "Gradient Descent(3578/4999): loss=0.577925212725668\n",
      "Gradient Descent(3579/4999): loss=0.5779249768688671\n",
      "Gradient Descent(3580/4999): loss=0.5779247410791707\n",
      "Gradient Descent(3581/4999): loss=0.5779245053565458\n",
      "Gradient Descent(3582/4999): loss=0.5779242697009596\n",
      "Gradient Descent(3583/4999): loss=0.5779240341123791\n",
      "Gradient Descent(3584/4999): loss=0.5779237985907716\n",
      "Gradient Descent(3585/4999): loss=0.5779235631361039\n",
      "Gradient Descent(3586/4999): loss=0.5779233277483435\n",
      "Gradient Descent(3587/4999): loss=0.5779230924274573\n",
      "Gradient Descent(3588/4999): loss=0.5779228571734129\n",
      "Gradient Descent(3589/4999): loss=0.5779226219861773\n",
      "Gradient Descent(3590/4999): loss=0.5779223868657178\n",
      "Gradient Descent(3591/4999): loss=0.5779221518120018\n",
      "Gradient Descent(3592/4999): loss=0.5779219168249967\n",
      "Gradient Descent(3593/4999): loss=0.5779216819046695\n",
      "Gradient Descent(3594/4999): loss=0.5779214470509882\n",
      "Gradient Descent(3595/4999): loss=0.5779212122639196\n",
      "Gradient Descent(3596/4999): loss=0.5779209775434316\n",
      "Gradient Descent(3597/4999): loss=0.5779207428894912\n",
      "Gradient Descent(3598/4999): loss=0.5779205083020664\n",
      "Gradient Descent(3599/4999): loss=0.5779202737811245\n",
      "Gradient Descent(3600/4999): loss=0.577920039326633\n",
      "Gradient Descent(3601/4999): loss=0.5779198049385593\n",
      "Gradient Descent(3602/4999): loss=0.5779195706168714\n",
      "Gradient Descent(3603/4999): loss=0.5779193363615367\n",
      "Gradient Descent(3604/4999): loss=0.5779191021725228\n",
      "Gradient Descent(3605/4999): loss=0.5779188680497975\n",
      "Gradient Descent(3606/4999): loss=0.5779186339933285\n",
      "Gradient Descent(3607/4999): loss=0.5779184000030835\n",
      "Gradient Descent(3608/4999): loss=0.5779181660790301\n",
      "Gradient Descent(3609/4999): loss=0.577917932221136\n",
      "Gradient Descent(3610/4999): loss=0.5779176984293696\n",
      "Gradient Descent(3611/4999): loss=0.5779174647036981\n",
      "Gradient Descent(3612/4999): loss=0.5779172310440895\n",
      "Gradient Descent(3613/4999): loss=0.577916997450512\n",
      "Gradient Descent(3614/4999): loss=0.577916763922933\n",
      "Gradient Descent(3615/4999): loss=0.5779165304613207\n",
      "Gradient Descent(3616/4999): loss=0.5779162970656432\n",
      "Gradient Descent(3617/4999): loss=0.5779160637358683\n",
      "Gradient Descent(3618/4999): loss=0.577915830471964\n",
      "Gradient Descent(3619/4999): loss=0.5779155972738984\n",
      "Gradient Descent(3620/4999): loss=0.5779153641416394\n",
      "Gradient Descent(3621/4999): loss=0.5779151310751551\n",
      "Gradient Descent(3622/4999): loss=0.5779148980744139\n",
      "Gradient Descent(3623/4999): loss=0.5779146651393835\n",
      "Gradient Descent(3624/4999): loss=0.5779144322700324\n",
      "Gradient Descent(3625/4999): loss=0.5779141994663286\n",
      "Gradient Descent(3626/4999): loss=0.5779139667282404\n",
      "Gradient Descent(3627/4999): loss=0.5779137340557359\n",
      "Gradient Descent(3628/4999): loss=0.5779135014487835\n",
      "Gradient Descent(3629/4999): loss=0.5779132689073512\n",
      "Gradient Descent(3630/4999): loss=0.5779130364314076\n",
      "Gradient Descent(3631/4999): loss=0.577912804020921\n",
      "Gradient Descent(3632/4999): loss=0.5779125716758597\n",
      "Gradient Descent(3633/4999): loss=0.5779123393961919\n",
      "Gradient Descent(3634/4999): loss=0.5779121071818861\n",
      "Gradient Descent(3635/4999): loss=0.5779118750329111\n",
      "Gradient Descent(3636/4999): loss=0.5779116429492346\n",
      "Gradient Descent(3637/4999): loss=0.5779114109308258\n",
      "Gradient Descent(3638/4999): loss=0.5779111789776527\n",
      "Gradient Descent(3639/4999): loss=0.5779109470896842\n",
      "Gradient Descent(3640/4999): loss=0.5779107152668883\n",
      "Gradient Descent(3641/4999): loss=0.5779104835092342\n",
      "Gradient Descent(3642/4999): loss=0.5779102518166901\n",
      "Gradient Descent(3643/4999): loss=0.5779100201892248\n",
      "Gradient Descent(3644/4999): loss=0.5779097886268066\n",
      "Gradient Descent(3645/4999): loss=0.5779095571294047\n",
      "Gradient Descent(3646/4999): loss=0.5779093256969873\n",
      "Gradient Descent(3647/4999): loss=0.5779090943295234\n",
      "Gradient Descent(3648/4999): loss=0.5779088630269816\n",
      "Gradient Descent(3649/4999): loss=0.5779086317893308\n",
      "Gradient Descent(3650/4999): loss=0.5779084006165397\n",
      "Gradient Descent(3651/4999): loss=0.577908169508577\n",
      "Gradient Descent(3652/4999): loss=0.5779079384654117\n",
      "Gradient Descent(3653/4999): loss=0.5779077074870126\n",
      "Gradient Descent(3654/4999): loss=0.5779074765733483\n",
      "Gradient Descent(3655/4999): loss=0.5779072457243881\n",
      "Gradient Descent(3656/4999): loss=0.5779070149401008\n",
      "Gradient Descent(3657/4999): loss=0.5779067842204554\n",
      "Gradient Descent(3658/4999): loss=0.5779065535654205\n",
      "Gradient Descent(3659/4999): loss=0.5779063229749656\n",
      "Gradient Descent(3660/4999): loss=0.5779060924490593\n",
      "Gradient Descent(3661/4999): loss=0.5779058619876711\n",
      "Gradient Descent(3662/4999): loss=0.5779056315907697\n",
      "Gradient Descent(3663/4999): loss=0.577905401258324\n",
      "Gradient Descent(3664/4999): loss=0.5779051709903037\n",
      "Gradient Descent(3665/4999): loss=0.5779049407866775\n",
      "Gradient Descent(3666/4999): loss=0.5779047106474147\n",
      "Gradient Descent(3667/4999): loss=0.5779044805724843\n",
      "Gradient Descent(3668/4999): loss=0.5779042505618559\n",
      "Gradient Descent(3669/4999): loss=0.5779040206154983\n",
      "Gradient Descent(3670/4999): loss=0.5779037907333809\n",
      "Gradient Descent(3671/4999): loss=0.5779035609154729\n",
      "Gradient Descent(3672/4999): loss=0.5779033311617436\n",
      "Gradient Descent(3673/4999): loss=0.5779031014721626\n",
      "Gradient Descent(3674/4999): loss=0.577902871846699\n",
      "Gradient Descent(3675/4999): loss=0.5779026422853221\n",
      "Gradient Descent(3676/4999): loss=0.5779024127880015\n",
      "Gradient Descent(3677/4999): loss=0.5779021833547062\n",
      "Gradient Descent(3678/4999): loss=0.5779019539854061\n",
      "Gradient Descent(3679/4999): loss=0.5779017246800703\n",
      "Gradient Descent(3680/4999): loss=0.5779014954386685\n",
      "Gradient Descent(3681/4999): loss=0.57790126626117\n",
      "Gradient Descent(3682/4999): loss=0.5779010371475446\n",
      "Gradient Descent(3683/4999): loss=0.5779008080977615\n",
      "Gradient Descent(3684/4999): loss=0.5779005791117905\n",
      "Gradient Descent(3685/4999): loss=0.5779003501896008\n",
      "Gradient Descent(3686/4999): loss=0.5779001213311625\n",
      "Gradient Descent(3687/4999): loss=0.5778998925364451\n",
      "Gradient Descent(3688/4999): loss=0.577899663805418\n",
      "Gradient Descent(3689/4999): loss=0.5778994351380513\n",
      "Gradient Descent(3690/4999): loss=0.5778992065343144\n",
      "Gradient Descent(3691/4999): loss=0.5778989779941769\n",
      "Gradient Descent(3692/4999): loss=0.5778987495176087\n",
      "Gradient Descent(3693/4999): loss=0.5778985211045795\n",
      "Gradient Descent(3694/4999): loss=0.5778982927550592\n",
      "Gradient Descent(3695/4999): loss=0.5778980644690177\n",
      "Gradient Descent(3696/4999): loss=0.5778978362464243\n",
      "Gradient Descent(3697/4999): loss=0.5778976080872494\n",
      "Gradient Descent(3698/4999): loss=0.5778973799914627\n",
      "Gradient Descent(3699/4999): loss=0.577897151959034\n",
      "Gradient Descent(3700/4999): loss=0.5778969239899332\n",
      "Gradient Descent(3701/4999): loss=0.5778966960841304\n",
      "Gradient Descent(3702/4999): loss=0.5778964682415955\n",
      "Gradient Descent(3703/4999): loss=0.5778962404622985\n",
      "Gradient Descent(3704/4999): loss=0.577896012746209\n",
      "Gradient Descent(3705/4999): loss=0.5778957850932975\n",
      "Gradient Descent(3706/4999): loss=0.5778955575035339\n",
      "Gradient Descent(3707/4999): loss=0.5778953299768885\n",
      "Gradient Descent(3708/4999): loss=0.5778951025133308\n",
      "Gradient Descent(3709/4999): loss=0.5778948751128313\n",
      "Gradient Descent(3710/4999): loss=0.57789464777536\n",
      "Gradient Descent(3711/4999): loss=0.5778944205008874\n",
      "Gradient Descent(3712/4999): loss=0.5778941932893831\n",
      "Gradient Descent(3713/4999): loss=0.5778939661408177\n",
      "Gradient Descent(3714/4999): loss=0.5778937390551612\n",
      "Gradient Descent(3715/4999): loss=0.5778935120323839\n",
      "Gradient Descent(3716/4999): loss=0.5778932850724561\n",
      "Gradient Descent(3717/4999): loss=0.577893058175348\n",
      "Gradient Descent(3718/4999): loss=0.5778928313410299\n",
      "Gradient Descent(3719/4999): loss=0.5778926045694722\n",
      "Gradient Descent(3720/4999): loss=0.5778923778606452\n",
      "Gradient Descent(3721/4999): loss=0.5778921512145191\n",
      "Gradient Descent(3722/4999): loss=0.5778919246310644\n",
      "Gradient Descent(3723/4999): loss=0.5778916981102519\n",
      "Gradient Descent(3724/4999): loss=0.5778914716520512\n",
      "Gradient Descent(3725/4999): loss=0.5778912452564334\n",
      "Gradient Descent(3726/4999): loss=0.5778910189233687\n",
      "Gradient Descent(3727/4999): loss=0.5778907926528275\n",
      "Gradient Descent(3728/4999): loss=0.5778905664447805\n",
      "Gradient Descent(3729/4999): loss=0.5778903402991982\n",
      "Gradient Descent(3730/4999): loss=0.5778901142160511\n",
      "Gradient Descent(3731/4999): loss=0.5778898881953096\n",
      "Gradient Descent(3732/4999): loss=0.5778896622369446\n",
      "Gradient Descent(3733/4999): loss=0.5778894363409265\n",
      "Gradient Descent(3734/4999): loss=0.5778892105072259\n",
      "Gradient Descent(3735/4999): loss=0.5778889847358136\n",
      "Gradient Descent(3736/4999): loss=0.57788875902666\n",
      "Gradient Descent(3737/4999): loss=0.5778885333797362\n",
      "Gradient Descent(3738/4999): loss=0.5778883077950127\n",
      "Gradient Descent(3739/4999): loss=0.5778880822724601\n",
      "Gradient Descent(3740/4999): loss=0.5778878568120492\n",
      "Gradient Descent(3741/4999): loss=0.5778876314137509\n",
      "Gradient Descent(3742/4999): loss=0.5778874060775359\n",
      "Gradient Descent(3743/4999): loss=0.5778871808033751\n",
      "Gradient Descent(3744/4999): loss=0.577886955591239\n",
      "Gradient Descent(3745/4999): loss=0.5778867304410987\n",
      "Gradient Descent(3746/4999): loss=0.5778865053529253\n",
      "Gradient Descent(3747/4999): loss=0.5778862803266895\n",
      "Gradient Descent(3748/4999): loss=0.5778860553623619\n",
      "Gradient Descent(3749/4999): loss=0.5778858304599139\n",
      "Gradient Descent(3750/4999): loss=0.5778856056193162\n",
      "Gradient Descent(3751/4999): loss=0.5778853808405398\n",
      "Gradient Descent(3752/4999): loss=0.5778851561235556\n",
      "Gradient Descent(3753/4999): loss=0.577884931468335\n",
      "Gradient Descent(3754/4999): loss=0.5778847068748485\n",
      "Gradient Descent(3755/4999): loss=0.5778844823430673\n",
      "Gradient Descent(3756/4999): loss=0.5778842578729627\n",
      "Gradient Descent(3757/4999): loss=0.5778840334645056\n",
      "Gradient Descent(3758/4999): loss=0.5778838091176672\n",
      "Gradient Descent(3759/4999): loss=0.5778835848324186\n",
      "Gradient Descent(3760/4999): loss=0.5778833606087309\n",
      "Gradient Descent(3761/4999): loss=0.5778831364465753\n",
      "Gradient Descent(3762/4999): loss=0.5778829123459229\n",
      "Gradient Descent(3763/4999): loss=0.5778826883067449\n",
      "Gradient Descent(3764/4999): loss=0.5778824643290129\n",
      "Gradient Descent(3765/4999): loss=0.5778822404126975\n",
      "Gradient Descent(3766/4999): loss=0.5778820165577706\n",
      "Gradient Descent(3767/4999): loss=0.577881792764203\n",
      "Gradient Descent(3768/4999): loss=0.5778815690319664\n",
      "Gradient Descent(3769/4999): loss=0.5778813453610318\n",
      "Gradient Descent(3770/4999): loss=0.5778811217513707\n",
      "Gradient Descent(3771/4999): loss=0.5778808982029543\n",
      "Gradient Descent(3772/4999): loss=0.5778806747157544\n",
      "Gradient Descent(3773/4999): loss=0.5778804512897419\n",
      "Gradient Descent(3774/4999): loss=0.5778802279248882\n",
      "Gradient Descent(3775/4999): loss=0.5778800046211654\n",
      "Gradient Descent(3776/4999): loss=0.5778797813785443\n",
      "Gradient Descent(3777/4999): loss=0.5778795581969967\n",
      "Gradient Descent(3778/4999): loss=0.5778793350764939\n",
      "Gradient Descent(3779/4999): loss=0.5778791120170075\n",
      "Gradient Descent(3780/4999): loss=0.577878889018509\n",
      "Gradient Descent(3781/4999): loss=0.5778786660809702\n",
      "Gradient Descent(3782/4999): loss=0.5778784432043622\n",
      "Gradient Descent(3783/4999): loss=0.577878220388657\n",
      "Gradient Descent(3784/4999): loss=0.577877997633826\n",
      "Gradient Descent(3785/4999): loss=0.5778777749398409\n",
      "Gradient Descent(3786/4999): loss=0.5778775523066733\n",
      "Gradient Descent(3787/4999): loss=0.577877329734295\n",
      "Gradient Descent(3788/4999): loss=0.5778771072226776\n",
      "Gradient Descent(3789/4999): loss=0.5778768847717927\n",
      "Gradient Descent(3790/4999): loss=0.577876662381612\n",
      "Gradient Descent(3791/4999): loss=0.5778764400521075\n",
      "Gradient Descent(3792/4999): loss=0.5778762177832507\n",
      "Gradient Descent(3793/4999): loss=0.5778759955750136\n",
      "Gradient Descent(3794/4999): loss=0.577875773427368\n",
      "Gradient Descent(3795/4999): loss=0.5778755513402853\n",
      "Gradient Descent(3796/4999): loss=0.5778753293137378\n",
      "Gradient Descent(3797/4999): loss=0.5778751073476972\n",
      "Gradient Descent(3798/4999): loss=0.5778748854421353\n",
      "Gradient Descent(3799/4999): loss=0.5778746635970241\n",
      "Gradient Descent(3800/4999): loss=0.5778744418123354\n",
      "Gradient Descent(3801/4999): loss=0.5778742200880412\n",
      "Gradient Descent(3802/4999): loss=0.5778739984241135\n",
      "Gradient Descent(3803/4999): loss=0.5778737768205241\n",
      "Gradient Descent(3804/4999): loss=0.577873555277245\n",
      "Gradient Descent(3805/4999): loss=0.5778733337942483\n",
      "Gradient Descent(3806/4999): loss=0.5778731123715062\n",
      "Gradient Descent(3807/4999): loss=0.5778728910089904\n",
      "Gradient Descent(3808/4999): loss=0.5778726697066731\n",
      "Gradient Descent(3809/4999): loss=0.5778724484645263\n",
      "Gradient Descent(3810/4999): loss=0.5778722272825221\n",
      "Gradient Descent(3811/4999): loss=0.5778720061606326\n",
      "Gradient Descent(3812/4999): loss=0.5778717850988301\n",
      "Gradient Descent(3813/4999): loss=0.5778715640970866\n",
      "Gradient Descent(3814/4999): loss=0.5778713431553744\n",
      "Gradient Descent(3815/4999): loss=0.5778711222736653\n",
      "Gradient Descent(3816/4999): loss=0.577870901451932\n",
      "Gradient Descent(3817/4999): loss=0.5778706806901464\n",
      "Gradient Descent(3818/4999): loss=0.5778704599882809\n",
      "Gradient Descent(3819/4999): loss=0.5778702393463075\n",
      "Gradient Descent(3820/4999): loss=0.5778700187641987\n",
      "Gradient Descent(3821/4999): loss=0.5778697982419269\n",
      "Gradient Descent(3822/4999): loss=0.5778695777794642\n",
      "Gradient Descent(3823/4999): loss=0.5778693573767829\n",
      "Gradient Descent(3824/4999): loss=0.5778691370338555\n",
      "Gradient Descent(3825/4999): loss=0.5778689167506542\n",
      "Gradient Descent(3826/4999): loss=0.5778686965271514\n",
      "Gradient Descent(3827/4999): loss=0.5778684763633196\n",
      "Gradient Descent(3828/4999): loss=0.5778682562591312\n",
      "Gradient Descent(3829/4999): loss=0.5778680362145587\n",
      "Gradient Descent(3830/4999): loss=0.5778678162295743\n",
      "Gradient Descent(3831/4999): loss=0.5778675963041506\n",
      "Gradient Descent(3832/4999): loss=0.5778673764382601\n",
      "Gradient Descent(3833/4999): loss=0.5778671566318753\n",
      "Gradient Descent(3834/4999): loss=0.5778669368849688\n",
      "Gradient Descent(3835/4999): loss=0.577866717197513\n",
      "Gradient Descent(3836/4999): loss=0.5778664975694806\n",
      "Gradient Descent(3837/4999): loss=0.577866278000844\n",
      "Gradient Descent(3838/4999): loss=0.5778660584915759\n",
      "Gradient Descent(3839/4999): loss=0.577865839041649\n",
      "Gradient Descent(3840/4999): loss=0.5778656196510354\n",
      "Gradient Descent(3841/4999): loss=0.5778654003197087\n",
      "Gradient Descent(3842/4999): loss=0.5778651810476407\n",
      "Gradient Descent(3843/4999): loss=0.5778649618348044\n",
      "Gradient Descent(3844/4999): loss=0.5778647426811725\n",
      "Gradient Descent(3845/4999): loss=0.5778645235867178\n",
      "Gradient Descent(3846/4999): loss=0.5778643045514127\n",
      "Gradient Descent(3847/4999): loss=0.5778640855752304\n",
      "Gradient Descent(3848/4999): loss=0.5778638666581434\n",
      "Gradient Descent(3849/4999): loss=0.5778636478001244\n",
      "Gradient Descent(3850/4999): loss=0.5778634290011463\n",
      "Gradient Descent(3851/4999): loss=0.577863210261182\n",
      "Gradient Descent(3852/4999): loss=0.5778629915802044\n",
      "Gradient Descent(3853/4999): loss=0.577862772958186\n",
      "Gradient Descent(3854/4999): loss=0.5778625543950999\n",
      "Gradient Descent(3855/4999): loss=0.5778623358909191\n",
      "Gradient Descent(3856/4999): loss=0.5778621174456162\n",
      "Gradient Descent(3857/4999): loss=0.5778618990591644\n",
      "Gradient Descent(3858/4999): loss=0.5778616807315363\n",
      "Gradient Descent(3859/4999): loss=0.5778614624627052\n",
      "Gradient Descent(3860/4999): loss=0.5778612442526441\n",
      "Gradient Descent(3861/4999): loss=0.5778610261013255\n",
      "Gradient Descent(3862/4999): loss=0.577860808008723\n",
      "Gradient Descent(3863/4999): loss=0.5778605899748092\n",
      "Gradient Descent(3864/4999): loss=0.5778603719995572\n",
      "Gradient Descent(3865/4999): loss=0.5778601540829402\n",
      "Gradient Descent(3866/4999): loss=0.5778599362249311\n",
      "Gradient Descent(3867/4999): loss=0.5778597184255034\n",
      "Gradient Descent(3868/4999): loss=0.5778595006846297\n",
      "Gradient Descent(3869/4999): loss=0.5778592830022832\n",
      "Gradient Descent(3870/4999): loss=0.5778590653784372\n",
      "Gradient Descent(3871/4999): loss=0.5778588478130648\n",
      "Gradient Descent(3872/4999): loss=0.5778586303061393\n",
      "Gradient Descent(3873/4999): loss=0.5778584128576337\n",
      "Gradient Descent(3874/4999): loss=0.577858195467521\n",
      "Gradient Descent(3875/4999): loss=0.5778579781357751\n",
      "Gradient Descent(3876/4999): loss=0.5778577608623685\n",
      "Gradient Descent(3877/4999): loss=0.5778575436472749\n",
      "Gradient Descent(3878/4999): loss=0.5778573264904674\n",
      "Gradient Descent(3879/4999): loss=0.5778571093919194\n",
      "Gradient Descent(3880/4999): loss=0.5778568923516041\n",
      "Gradient Descent(3881/4999): loss=0.5778566753694947\n",
      "Gradient Descent(3882/4999): loss=0.5778564584455648\n",
      "Gradient Descent(3883/4999): loss=0.5778562415797875\n",
      "Gradient Descent(3884/4999): loss=0.5778560247721366\n",
      "Gradient Descent(3885/4999): loss=0.577855808022585\n",
      "Gradient Descent(3886/4999): loss=0.5778555913311062\n",
      "Gradient Descent(3887/4999): loss=0.5778553746976738\n",
      "Gradient Descent(3888/4999): loss=0.5778551581222611\n",
      "Gradient Descent(3889/4999): loss=0.5778549416048416\n",
      "Gradient Descent(3890/4999): loss=0.5778547251453887\n",
      "Gradient Descent(3891/4999): loss=0.577854508743876\n",
      "Gradient Descent(3892/4999): loss=0.5778542924002769\n",
      "Gradient Descent(3893/4999): loss=0.577854076114565\n",
      "Gradient Descent(3894/4999): loss=0.5778538598867139\n",
      "Gradient Descent(3895/4999): loss=0.5778536437166968\n",
      "Gradient Descent(3896/4999): loss=0.5778534276044877\n",
      "Gradient Descent(3897/4999): loss=0.5778532115500598\n",
      "Gradient Descent(3898/4999): loss=0.577852995553387\n",
      "Gradient Descent(3899/4999): loss=0.5778527796144426\n",
      "Gradient Descent(3900/4999): loss=0.5778525637332006\n",
      "Gradient Descent(3901/4999): loss=0.5778523479096342\n",
      "Gradient Descent(3902/4999): loss=0.5778521321437178\n",
      "Gradient Descent(3903/4999): loss=0.5778519164354241\n",
      "Gradient Descent(3904/4999): loss=0.5778517007847275\n",
      "Gradient Descent(3905/4999): loss=0.5778514851916015\n",
      "Gradient Descent(3906/4999): loss=0.5778512696560197\n",
      "Gradient Descent(3907/4999): loss=0.577851054177956\n",
      "Gradient Descent(3908/4999): loss=0.5778508387573842\n",
      "Gradient Descent(3909/4999): loss=0.5778506233942778\n",
      "Gradient Descent(3910/4999): loss=0.5778504080886109\n",
      "Gradient Descent(3911/4999): loss=0.5778501928403572\n",
      "Gradient Descent(3912/4999): loss=0.5778499776494903\n",
      "Gradient Descent(3913/4999): loss=0.5778497625159844\n",
      "Gradient Descent(3914/4999): loss=0.5778495474398131\n",
      "Gradient Descent(3915/4999): loss=0.5778493324209505\n",
      "Gradient Descent(3916/4999): loss=0.5778491174593702\n",
      "Gradient Descent(3917/4999): loss=0.5778489025550462\n",
      "Gradient Descent(3918/4999): loss=0.5778486877079525\n",
      "Gradient Descent(3919/4999): loss=0.5778484729180628\n",
      "Gradient Descent(3920/4999): loss=0.5778482581853515\n",
      "Gradient Descent(3921/4999): loss=0.5778480435097921\n",
      "Gradient Descent(3922/4999): loss=0.5778478288913588\n",
      "Gradient Descent(3923/4999): loss=0.5778476143300255\n",
      "Gradient Descent(3924/4999): loss=0.5778473998257663\n",
      "Gradient Descent(3925/4999): loss=0.5778471853785551\n",
      "Gradient Descent(3926/4999): loss=0.5778469709883659\n",
      "Gradient Descent(3927/4999): loss=0.577846756655173\n",
      "Gradient Descent(3928/4999): loss=0.5778465423789504\n",
      "Gradient Descent(3929/4999): loss=0.5778463281596721\n",
      "Gradient Descent(3930/4999): loss=0.577846113997312\n",
      "Gradient Descent(3931/4999): loss=0.5778458998918446\n",
      "Gradient Descent(3932/4999): loss=0.5778456858432439\n",
      "Gradient Descent(3933/4999): loss=0.5778454718514839\n",
      "Gradient Descent(3934/4999): loss=0.577845257916539\n",
      "Gradient Descent(3935/4999): loss=0.5778450440383832\n",
      "Gradient Descent(3936/4999): loss=0.5778448302169907\n",
      "Gradient Descent(3937/4999): loss=0.5778446164523359\n",
      "Gradient Descent(3938/4999): loss=0.5778444027443926\n",
      "Gradient Descent(3939/4999): loss=0.5778441890931355\n",
      "Gradient Descent(3940/4999): loss=0.5778439754985387\n",
      "Gradient Descent(3941/4999): loss=0.5778437619605764\n",
      "Gradient Descent(3942/4999): loss=0.5778435484792229\n",
      "Gradient Descent(3943/4999): loss=0.5778433350544524\n",
      "Gradient Descent(3944/4999): loss=0.5778431216862396\n",
      "Gradient Descent(3945/4999): loss=0.5778429083745584\n",
      "Gradient Descent(3946/4999): loss=0.5778426951193832\n",
      "Gradient Descent(3947/4999): loss=0.5778424819206887\n",
      "Gradient Descent(3948/4999): loss=0.5778422687784489\n",
      "Gradient Descent(3949/4999): loss=0.5778420556926384\n",
      "Gradient Descent(3950/4999): loss=0.5778418426632316\n",
      "Gradient Descent(3951/4999): loss=0.5778416296902027\n",
      "Gradient Descent(3952/4999): loss=0.5778414167735263\n",
      "Gradient Descent(3953/4999): loss=0.5778412039131771\n",
      "Gradient Descent(3954/4999): loss=0.5778409911091289\n",
      "Gradient Descent(3955/4999): loss=0.577840778361357\n",
      "Gradient Descent(3956/4999): loss=0.5778405656698353\n",
      "Gradient Descent(3957/4999): loss=0.5778403530345386\n",
      "Gradient Descent(3958/4999): loss=0.5778401404554412\n",
      "Gradient Descent(3959/4999): loss=0.5778399279325178\n",
      "Gradient Descent(3960/4999): loss=0.5778397154657431\n",
      "Gradient Descent(3961/4999): loss=0.5778395030550912\n",
      "Gradient Descent(3962/4999): loss=0.5778392907005372\n",
      "Gradient Descent(3963/4999): loss=0.5778390784020553\n",
      "Gradient Descent(3964/4999): loss=0.5778388661596205\n",
      "Gradient Descent(3965/4999): loss=0.5778386539732071\n",
      "Gradient Descent(3966/4999): loss=0.5778384418427898\n",
      "Gradient Descent(3967/4999): loss=0.5778382297683435\n",
      "Gradient Descent(3968/4999): loss=0.5778380177498427\n",
      "Gradient Descent(3969/4999): loss=0.577837805787262\n",
      "Gradient Descent(3970/4999): loss=0.5778375938805761\n",
      "Gradient Descent(3971/4999): loss=0.5778373820297601\n",
      "Gradient Descent(3972/4999): loss=0.577837170234788\n",
      "Gradient Descent(3973/4999): loss=0.5778369584956353\n",
      "Gradient Descent(3974/4999): loss=0.5778367468122764\n",
      "Gradient Descent(3975/4999): loss=0.5778365351846861\n",
      "Gradient Descent(3976/4999): loss=0.5778363236128393\n",
      "Gradient Descent(3977/4999): loss=0.5778361120967107\n",
      "Gradient Descent(3978/4999): loss=0.5778359006362751\n",
      "Gradient Descent(3979/4999): loss=0.5778356892315073\n",
      "Gradient Descent(3980/4999): loss=0.5778354778823822\n",
      "Gradient Descent(3981/4999): loss=0.577835266588875\n",
      "Gradient Descent(3982/4999): loss=0.57783505535096\n",
      "Gradient Descent(3983/4999): loss=0.5778348441686125\n",
      "Gradient Descent(3984/4999): loss=0.5778346330418072\n",
      "Gradient Descent(3985/4999): loss=0.577834421970519\n",
      "Gradient Descent(3986/4999): loss=0.5778342109547232\n",
      "Gradient Descent(3987/4999): loss=0.5778339999943943\n",
      "Gradient Descent(3988/4999): loss=0.5778337890895074\n",
      "Gradient Descent(3989/4999): loss=0.5778335782400378\n",
      "Gradient Descent(3990/4999): loss=0.57783336744596\n",
      "Gradient Descent(3991/4999): loss=0.5778331567072491\n",
      "Gradient Descent(3992/4999): loss=0.5778329460238807\n",
      "Gradient Descent(3993/4999): loss=0.5778327353958289\n",
      "Gradient Descent(3994/4999): loss=0.5778325248230696\n",
      "Gradient Descent(3995/4999): loss=0.5778323143055774\n",
      "Gradient Descent(3996/4999): loss=0.5778321038433274\n",
      "Gradient Descent(3997/4999): loss=0.5778318934362949\n",
      "Gradient Descent(3998/4999): loss=0.5778316830844547\n",
      "Gradient Descent(3999/4999): loss=0.5778314727877824\n",
      "Gradient Descent(4000/4999): loss=0.5778312625462526\n",
      "Gradient Descent(4001/4999): loss=0.5778310523598408\n",
      "Gradient Descent(4002/4999): loss=0.5778308422285221\n",
      "Gradient Descent(4003/4999): loss=0.5778306321522715\n",
      "Gradient Descent(4004/4999): loss=0.5778304221310646\n",
      "Gradient Descent(4005/4999): loss=0.5778302121648762\n",
      "Gradient Descent(4006/4999): loss=0.5778300022536816\n",
      "Gradient Descent(4007/4999): loss=0.5778297923974561\n",
      "Gradient Descent(4008/4999): loss=0.5778295825961751\n",
      "Gradient Descent(4009/4999): loss=0.5778293728498136\n",
      "Gradient Descent(4010/4999): loss=0.5778291631583472\n",
      "Gradient Descent(4011/4999): loss=0.5778289535217508\n",
      "Gradient Descent(4012/4999): loss=0.5778287439399998\n",
      "Gradient Descent(4013/4999): loss=0.5778285344130698\n",
      "Gradient Descent(4014/4999): loss=0.577828324940936\n",
      "Gradient Descent(4015/4999): loss=0.5778281155235735\n",
      "Gradient Descent(4016/4999): loss=0.577827906160958\n",
      "Gradient Descent(4017/4999): loss=0.5778276968530647\n",
      "Gradient Descent(4018/4999): loss=0.577827487599869\n",
      "Gradient Descent(4019/4999): loss=0.5778272784013464\n",
      "Gradient Descent(4020/4999): loss=0.5778270692574721\n",
      "Gradient Descent(4021/4999): loss=0.5778268601682218\n",
      "Gradient Descent(4022/4999): loss=0.5778266511335708\n",
      "Gradient Descent(4023/4999): loss=0.5778264421534947\n",
      "Gradient Descent(4024/4999): loss=0.5778262332279688\n",
      "Gradient Descent(4025/4999): loss=0.5778260243569684\n",
      "Gradient Descent(4026/4999): loss=0.5778258155404695\n",
      "Gradient Descent(4027/4999): loss=0.5778256067784473\n",
      "Gradient Descent(4028/4999): loss=0.5778253980708775\n",
      "Gradient Descent(4029/4999): loss=0.5778251894177353\n",
      "Gradient Descent(4030/4999): loss=0.5778249808189967\n",
      "Gradient Descent(4031/4999): loss=0.5778247722746369\n",
      "Gradient Descent(4032/4999): loss=0.5778245637846318\n",
      "Gradient Descent(4033/4999): loss=0.5778243553489566\n",
      "Gradient Descent(4034/4999): loss=0.5778241469675873\n",
      "Gradient Descent(4035/4999): loss=0.5778239386404993\n",
      "Gradient Descent(4036/4999): loss=0.5778237303676684\n",
      "Gradient Descent(4037/4999): loss=0.5778235221490702\n",
      "Gradient Descent(4038/4999): loss=0.5778233139846801\n",
      "Gradient Descent(4039/4999): loss=0.5778231058744742\n",
      "Gradient Descent(4040/4999): loss=0.5778228978184279\n",
      "Gradient Descent(4041/4999): loss=0.5778226898165171\n",
      "Gradient Descent(4042/4999): loss=0.5778224818687174\n",
      "Gradient Descent(4043/4999): loss=0.5778222739750045\n",
      "Gradient Descent(4044/4999): loss=0.5778220661353541\n",
      "Gradient Descent(4045/4999): loss=0.5778218583497424\n",
      "Gradient Descent(4046/4999): loss=0.5778216506181446\n",
      "Gradient Descent(4047/4999): loss=0.5778214429405366\n",
      "Gradient Descent(4048/4999): loss=0.5778212353168947\n",
      "Gradient Descent(4049/4999): loss=0.5778210277471939\n",
      "Gradient Descent(4050/4999): loss=0.5778208202314107\n",
      "Gradient Descent(4051/4999): loss=0.5778206127695207\n",
      "Gradient Descent(4052/4999): loss=0.5778204053614998\n",
      "Gradient Descent(4053/4999): loss=0.5778201980073238\n",
      "Gradient Descent(4054/4999): loss=0.5778199907069683\n",
      "Gradient Descent(4055/4999): loss=0.5778197834604099\n",
      "Gradient Descent(4056/4999): loss=0.577819576267624\n",
      "Gradient Descent(4057/4999): loss=0.5778193691285868\n",
      "Gradient Descent(4058/4999): loss=0.5778191620432737\n",
      "Gradient Descent(4059/4999): loss=0.5778189550116611\n",
      "Gradient Descent(4060/4999): loss=0.577818748033725\n",
      "Gradient Descent(4061/4999): loss=0.5778185411094413\n",
      "Gradient Descent(4062/4999): loss=0.5778183342387858\n",
      "Gradient Descent(4063/4999): loss=0.5778181274217347\n",
      "Gradient Descent(4064/4999): loss=0.577817920658264\n",
      "Gradient Descent(4065/4999): loss=0.5778177139483497\n",
      "Gradient Descent(4066/4999): loss=0.5778175072919677\n",
      "Gradient Descent(4067/4999): loss=0.577817300689094\n",
      "Gradient Descent(4068/4999): loss=0.5778170941397052\n",
      "Gradient Descent(4069/4999): loss=0.5778168876437768\n",
      "Gradient Descent(4070/4999): loss=0.5778166812012852\n",
      "Gradient Descent(4071/4999): loss=0.5778164748122065\n",
      "Gradient Descent(4072/4999): loss=0.5778162684765166\n",
      "Gradient Descent(4073/4999): loss=0.5778160621941918\n",
      "Gradient Descent(4074/4999): loss=0.5778158559652082\n",
      "Gradient Descent(4075/4999): loss=0.577815649789542\n",
      "Gradient Descent(4076/4999): loss=0.5778154436671694\n",
      "Gradient Descent(4077/4999): loss=0.5778152375980664\n",
      "Gradient Descent(4078/4999): loss=0.5778150315822095\n",
      "Gradient Descent(4079/4999): loss=0.5778148256195745\n",
      "Gradient Descent(4080/4999): loss=0.5778146197101381\n",
      "Gradient Descent(4081/4999): loss=0.5778144138538763\n",
      "Gradient Descent(4082/4999): loss=0.5778142080507652\n",
      "Gradient Descent(4083/4999): loss=0.5778140023007814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4084/4999): loss=0.5778137966039009\n",
      "Gradient Descent(4085/4999): loss=0.5778135909601\n",
      "Gradient Descent(4086/4999): loss=0.5778133853693551\n",
      "Gradient Descent(4087/4999): loss=0.5778131798316426\n",
      "Gradient Descent(4088/4999): loss=0.5778129743469388\n",
      "Gradient Descent(4089/4999): loss=0.5778127689152198\n",
      "Gradient Descent(4090/4999): loss=0.5778125635364619\n",
      "Gradient Descent(4091/4999): loss=0.5778123582106421\n",
      "Gradient Descent(4092/4999): loss=0.5778121529377361\n",
      "Gradient Descent(4093/4999): loss=0.5778119477177208\n",
      "Gradient Descent(4094/4999): loss=0.5778117425505722\n",
      "Gradient Descent(4095/4999): loss=0.577811537436267\n",
      "Gradient Descent(4096/4999): loss=0.5778113323747813\n",
      "Gradient Descent(4097/4999): loss=0.5778111273660919\n",
      "Gradient Descent(4098/4999): loss=0.5778109224101751\n",
      "Gradient Descent(4099/4999): loss=0.5778107175070073\n",
      "Gradient Descent(4100/4999): loss=0.5778105126565651\n",
      "Gradient Descent(4101/4999): loss=0.577810307858825\n",
      "Gradient Descent(4102/4999): loss=0.5778101031137636\n",
      "Gradient Descent(4103/4999): loss=0.577809898421357\n",
      "Gradient Descent(4104/4999): loss=0.5778096937815821\n",
      "Gradient Descent(4105/4999): loss=0.5778094891944154\n",
      "Gradient Descent(4106/4999): loss=0.5778092846598334\n",
      "Gradient Descent(4107/4999): loss=0.5778090801778126\n",
      "Gradient Descent(4108/4999): loss=0.57780887574833\n",
      "Gradient Descent(4109/4999): loss=0.5778086713713615\n",
      "Gradient Descent(4110/4999): loss=0.5778084670468842\n",
      "Gradient Descent(4111/4999): loss=0.5778082627748745\n",
      "Gradient Descent(4112/4999): loss=0.5778080585553093\n",
      "Gradient Descent(4113/4999): loss=0.5778078543881651\n",
      "Gradient Descent(4114/4999): loss=0.5778076502734183\n",
      "Gradient Descent(4115/4999): loss=0.577807446211046\n",
      "Gradient Descent(4116/4999): loss=0.5778072422010246\n",
      "Gradient Descent(4117/4999): loss=0.5778070382433309\n",
      "Gradient Descent(4118/4999): loss=0.5778068343379417\n",
      "Gradient Descent(4119/4999): loss=0.5778066304848335\n",
      "Gradient Descent(4120/4999): loss=0.5778064266839832\n",
      "Gradient Descent(4121/4999): loss=0.5778062229353675\n",
      "Gradient Descent(4122/4999): loss=0.5778060192389631\n",
      "Gradient Descent(4123/4999): loss=0.5778058155947468\n",
      "Gradient Descent(4124/4999): loss=0.5778056120026955\n",
      "Gradient Descent(4125/4999): loss=0.577805408462786\n",
      "Gradient Descent(4126/4999): loss=0.5778052049749949\n",
      "Gradient Descent(4127/4999): loss=0.5778050015392991\n",
      "Gradient Descent(4128/4999): loss=0.5778047981556755\n",
      "Gradient Descent(4129/4999): loss=0.577804594824101\n",
      "Gradient Descent(4130/4999): loss=0.5778043915445523\n",
      "Gradient Descent(4131/4999): loss=0.5778041883170063\n",
      "Gradient Descent(4132/4999): loss=0.57780398514144\n",
      "Gradient Descent(4133/4999): loss=0.5778037820178301\n",
      "Gradient Descent(4134/4999): loss=0.5778035789461539\n",
      "Gradient Descent(4135/4999): loss=0.577803375926388\n",
      "Gradient Descent(4136/4999): loss=0.5778031729585091\n",
      "Gradient Descent(4137/4999): loss=0.5778029700424948\n",
      "Gradient Descent(4138/4999): loss=0.5778027671783215\n",
      "Gradient Descent(4139/4999): loss=0.5778025643659662\n",
      "Gradient Descent(4140/4999): loss=0.5778023616054063\n",
      "Gradient Descent(4141/4999): loss=0.5778021588966185\n",
      "Gradient Descent(4142/4999): loss=0.5778019562395796\n",
      "Gradient Descent(4143/4999): loss=0.5778017536342671\n",
      "Gradient Descent(4144/4999): loss=0.5778015510806578\n",
      "Gradient Descent(4145/4999): loss=0.5778013485787286\n",
      "Gradient Descent(4146/4999): loss=0.5778011461284568\n",
      "Gradient Descent(4147/4999): loss=0.5778009437298194\n",
      "Gradient Descent(4148/4999): loss=0.5778007413827931\n",
      "Gradient Descent(4149/4999): loss=0.5778005390873555\n",
      "Gradient Descent(4150/4999): loss=0.5778003368434836\n",
      "Gradient Descent(4151/4999): loss=0.5778001346511544\n",
      "Gradient Descent(4152/4999): loss=0.5777999325103451\n",
      "Gradient Descent(4153/4999): loss=0.5777997304210327\n",
      "Gradient Descent(4154/4999): loss=0.5777995283831945\n",
      "Gradient Descent(4155/4999): loss=0.5777993263968078\n",
      "Gradient Descent(4156/4999): loss=0.5777991244618493\n",
      "Gradient Descent(4157/4999): loss=0.5777989225782967\n",
      "Gradient Descent(4158/4999): loss=0.577798720746127\n",
      "Gradient Descent(4159/4999): loss=0.5777985189653173\n",
      "Gradient Descent(4160/4999): loss=0.577798317235845\n",
      "Gradient Descent(4161/4999): loss=0.5777981155576872\n",
      "Gradient Descent(4162/4999): loss=0.5777979139308212\n",
      "Gradient Descent(4163/4999): loss=0.5777977123552243\n",
      "Gradient Descent(4164/4999): loss=0.5777975108308738\n",
      "Gradient Descent(4165/4999): loss=0.5777973093577469\n",
      "Gradient Descent(4166/4999): loss=0.5777971079358207\n",
      "Gradient Descent(4167/4999): loss=0.577796906565073\n",
      "Gradient Descent(4168/4999): loss=0.5777967052454807\n",
      "Gradient Descent(4169/4999): loss=0.5777965039770214\n",
      "Gradient Descent(4170/4999): loss=0.5777963027596723\n",
      "Gradient Descent(4171/4999): loss=0.5777961015934108\n",
      "Gradient Descent(4172/4999): loss=0.5777959004782142\n",
      "Gradient Descent(4173/4999): loss=0.57779569941406\n",
      "Gradient Descent(4174/4999): loss=0.5777954984009254\n",
      "Gradient Descent(4175/4999): loss=0.5777952974387879\n",
      "Gradient Descent(4176/4999): loss=0.577795096527625\n",
      "Gradient Descent(4177/4999): loss=0.5777948956674142\n",
      "Gradient Descent(4178/4999): loss=0.5777946948581327\n",
      "Gradient Descent(4179/4999): loss=0.5777944940997579\n",
      "Gradient Descent(4180/4999): loss=0.5777942933922676\n",
      "Gradient Descent(4181/4999): loss=0.577794092735639\n",
      "Gradient Descent(4182/4999): loss=0.5777938921298496\n",
      "Gradient Descent(4183/4999): loss=0.5777936915748773\n",
      "Gradient Descent(4184/4999): loss=0.577793491070699\n",
      "Gradient Descent(4185/4999): loss=0.5777932906172927\n",
      "Gradient Descent(4186/4999): loss=0.5777930902146355\n",
      "Gradient Descent(4187/4999): loss=0.5777928898627054\n",
      "Gradient Descent(4188/4999): loss=0.5777926895614794\n",
      "Gradient Descent(4189/4999): loss=0.5777924893109359\n",
      "Gradient Descent(4190/4999): loss=0.5777922891110516\n",
      "Gradient Descent(4191/4999): loss=0.5777920889618047\n",
      "Gradient Descent(4192/4999): loss=0.5777918888631725\n",
      "Gradient Descent(4193/4999): loss=0.5777916888151327\n",
      "Gradient Descent(4194/4999): loss=0.5777914888176628\n",
      "Gradient Descent(4195/4999): loss=0.5777912888707407\n",
      "Gradient Descent(4196/4999): loss=0.577791088974344\n",
      "Gradient Descent(4197/4999): loss=0.5777908891284501\n",
      "Gradient Descent(4198/4999): loss=0.5777906893330369\n",
      "Gradient Descent(4199/4999): loss=0.5777904895880821\n",
      "Gradient Descent(4200/4999): loss=0.5777902898935632\n",
      "Gradient Descent(4201/4999): loss=0.577790090249458\n",
      "Gradient Descent(4202/4999): loss=0.5777898906557445\n",
      "Gradient Descent(4203/4999): loss=0.5777896911124001\n",
      "Gradient Descent(4204/4999): loss=0.5777894916194024\n",
      "Gradient Descent(4205/4999): loss=0.5777892921767296\n",
      "Gradient Descent(4206/4999): loss=0.5777890927843592\n",
      "Gradient Descent(4207/4999): loss=0.577788893442269\n",
      "Gradient Descent(4208/4999): loss=0.5777886941504369\n",
      "Gradient Descent(4209/4999): loss=0.5777884949088404\n",
      "Gradient Descent(4210/4999): loss=0.5777882957174578\n",
      "Gradient Descent(4211/4999): loss=0.5777880965762664\n",
      "Gradient Descent(4212/4999): loss=0.5777878974852443\n",
      "Gradient Descent(4213/4999): loss=0.5777876984443695\n",
      "Gradient Descent(4214/4999): loss=0.5777874994536196\n",
      "Gradient Descent(4215/4999): loss=0.5777873005129724\n",
      "Gradient Descent(4216/4999): loss=0.5777871016224062\n",
      "Gradient Descent(4217/4999): loss=0.5777869027818986\n",
      "Gradient Descent(4218/4999): loss=0.5777867039914274\n",
      "Gradient Descent(4219/4999): loss=0.5777865052509706\n",
      "Gradient Descent(4220/4999): loss=0.5777863065605062\n",
      "Gradient Descent(4221/4999): loss=0.5777861079200122\n",
      "Gradient Descent(4222/4999): loss=0.5777859093294665\n",
      "Gradient Descent(4223/4999): loss=0.5777857107888468\n",
      "Gradient Descent(4224/4999): loss=0.5777855122981312\n",
      "Gradient Descent(4225/4999): loss=0.5777853138572981\n",
      "Gradient Descent(4226/4999): loss=0.577785115466325\n",
      "Gradient Descent(4227/4999): loss=0.5777849171251902\n",
      "Gradient Descent(4228/4999): loss=0.5777847188338714\n",
      "Gradient Descent(4229/4999): loss=0.5777845205923469\n",
      "Gradient Descent(4230/4999): loss=0.5777843224005946\n",
      "Gradient Descent(4231/4999): loss=0.5777841242585926\n",
      "Gradient Descent(4232/4999): loss=0.5777839261663189\n",
      "Gradient Descent(4233/4999): loss=0.5777837281237518\n",
      "Gradient Descent(4234/4999): loss=0.577783530130869\n",
      "Gradient Descent(4235/4999): loss=0.5777833321876491\n",
      "Gradient Descent(4236/4999): loss=0.5777831342940698\n",
      "Gradient Descent(4237/4999): loss=0.5777829364501093\n",
      "Gradient Descent(4238/4999): loss=0.5777827386557456\n",
      "Gradient Descent(4239/4999): loss=0.5777825409109572\n",
      "Gradient Descent(4240/4999): loss=0.577782343215722\n",
      "Gradient Descent(4241/4999): loss=0.5777821455700184\n",
      "Gradient Descent(4242/4999): loss=0.5777819479738241\n",
      "Gradient Descent(4243/4999): loss=0.5777817504271179\n",
      "Gradient Descent(4244/4999): loss=0.5777815529298773\n",
      "Gradient Descent(4245/4999): loss=0.577781355482081\n",
      "Gradient Descent(4246/4999): loss=0.5777811580837073\n",
      "Gradient Descent(4247/4999): loss=0.5777809607347342\n",
      "Gradient Descent(4248/4999): loss=0.5777807634351397\n",
      "Gradient Descent(4249/4999): loss=0.5777805661849026\n",
      "Gradient Descent(4250/4999): loss=0.5777803689840008\n",
      "Gradient Descent(4251/4999): loss=0.5777801718324126\n",
      "Gradient Descent(4252/4999): loss=0.5777799747301164\n",
      "Gradient Descent(4253/4999): loss=0.5777797776770904\n",
      "Gradient Descent(4254/4999): loss=0.5777795806733129\n",
      "Gradient Descent(4255/4999): loss=0.5777793837187625\n",
      "Gradient Descent(4256/4999): loss=0.577779186813417\n",
      "Gradient Descent(4257/4999): loss=0.5777789899572552\n",
      "Gradient Descent(4258/4999): loss=0.5777787931502553\n",
      "Gradient Descent(4259/4999): loss=0.5777785963923956\n",
      "Gradient Descent(4260/4999): loss=0.5777783996836545\n",
      "Gradient Descent(4261/4999): loss=0.5777782030240104\n",
      "Gradient Descent(4262/4999): loss=0.5777780064134416\n",
      "Gradient Descent(4263/4999): loss=0.5777778098519267\n",
      "Gradient Descent(4264/4999): loss=0.577777613339444\n",
      "Gradient Descent(4265/4999): loss=0.577777416875972\n",
      "Gradient Descent(4266/4999): loss=0.577777220461489\n",
      "Gradient Descent(4267/4999): loss=0.5777770240959736\n",
      "Gradient Descent(4268/4999): loss=0.5777768277794043\n",
      "Gradient Descent(4269/4999): loss=0.5777766315117594\n",
      "Gradient Descent(4270/4999): loss=0.5777764352930173\n",
      "Gradient Descent(4271/4999): loss=0.5777762391231568\n",
      "Gradient Descent(4272/4999): loss=0.5777760430021562\n",
      "Gradient Descent(4273/4999): loss=0.5777758469299941\n",
      "Gradient Descent(4274/4999): loss=0.577775650906649\n",
      "Gradient Descent(4275/4999): loss=0.5777754549320994\n",
      "Gradient Descent(4276/4999): loss=0.5777752590063238\n",
      "Gradient Descent(4277/4999): loss=0.5777750631293009\n",
      "Gradient Descent(4278/4999): loss=0.5777748673010091\n",
      "Gradient Descent(4279/4999): loss=0.5777746715214274\n",
      "Gradient Descent(4280/4999): loss=0.5777744757905336\n",
      "Gradient Descent(4281/4999): loss=0.5777742801083072\n",
      "Gradient Descent(4282/4999): loss=0.5777740844747262\n",
      "Gradient Descent(4283/4999): loss=0.5777738888897694\n",
      "Gradient Descent(4284/4999): loss=0.5777736933534157\n",
      "Gradient Descent(4285/4999): loss=0.5777734978656431\n",
      "Gradient Descent(4286/4999): loss=0.5777733024264309\n",
      "Gradient Descent(4287/4999): loss=0.5777731070357573\n",
      "Gradient Descent(4288/4999): loss=0.5777729116936013\n",
      "Gradient Descent(4289/4999): loss=0.5777727163999417\n",
      "Gradient Descent(4290/4999): loss=0.5777725211547567\n",
      "Gradient Descent(4291/4999): loss=0.5777723259580254\n",
      "Gradient Descent(4292/4999): loss=0.5777721308097266\n",
      "Gradient Descent(4293/4999): loss=0.5777719357098384\n",
      "Gradient Descent(4294/4999): loss=0.5777717406583404\n",
      "Gradient Descent(4295/4999): loss=0.5777715456552105\n",
      "Gradient Descent(4296/4999): loss=0.5777713507004282\n",
      "Gradient Descent(4297/4999): loss=0.5777711557939718\n",
      "Gradient Descent(4298/4999): loss=0.5777709609358204\n",
      "Gradient Descent(4299/4999): loss=0.5777707661259526\n",
      "Gradient Descent(4300/4999): loss=0.5777705713643472\n",
      "Gradient Descent(4301/4999): loss=0.5777703766509832\n",
      "Gradient Descent(4302/4999): loss=0.5777701819858392\n",
      "Gradient Descent(4303/4999): loss=0.5777699873688941\n",
      "Gradient Descent(4304/4999): loss=0.5777697928001267\n",
      "Gradient Descent(4305/4999): loss=0.5777695982795162\n",
      "Gradient Descent(4306/4999): loss=0.5777694038070411\n",
      "Gradient Descent(4307/4999): loss=0.5777692093826803\n",
      "Gradient Descent(4308/4999): loss=0.5777690150064128\n",
      "Gradient Descent(4309/4999): loss=0.5777688206782176\n",
      "Gradient Descent(4310/4999): loss=0.5777686263980734\n",
      "Gradient Descent(4311/4999): loss=0.5777684321659591\n",
      "Gradient Descent(4312/4999): loss=0.5777682379818538\n",
      "Gradient Descent(4313/4999): loss=0.5777680438457367\n",
      "Gradient Descent(4314/4999): loss=0.5777678497575859\n",
      "Gradient Descent(4315/4999): loss=0.5777676557173813\n",
      "Gradient Descent(4316/4999): loss=0.5777674617251014\n",
      "Gradient Descent(4317/4999): loss=0.5777672677807252\n",
      "Gradient Descent(4318/4999): loss=0.5777670738842317\n",
      "Gradient Descent(4319/4999): loss=0.5777668800356001\n",
      "Gradient Descent(4320/4999): loss=0.5777666862348093\n",
      "Gradient Descent(4321/4999): loss=0.5777664924818382\n",
      "Gradient Descent(4322/4999): loss=0.577766298776666\n",
      "Gradient Descent(4323/4999): loss=0.5777661051192716\n",
      "Gradient Descent(4324/4999): loss=0.5777659115096344\n",
      "Gradient Descent(4325/4999): loss=0.5777657179477329\n",
      "Gradient Descent(4326/4999): loss=0.5777655244335467\n",
      "Gradient Descent(4327/4999): loss=0.5777653309670545\n",
      "Gradient Descent(4328/4999): loss=0.5777651375482358\n",
      "Gradient Descent(4329/4999): loss=0.5777649441770694\n",
      "Gradient Descent(4330/4999): loss=0.5777647508535345\n",
      "Gradient Descent(4331/4999): loss=0.5777645575776104\n",
      "Gradient Descent(4332/4999): loss=0.5777643643492758\n",
      "Gradient Descent(4333/4999): loss=0.5777641711685103\n",
      "Gradient Descent(4334/4999): loss=0.5777639780352929\n",
      "Gradient Descent(4335/4999): loss=0.5777637849496029\n",
      "Gradient Descent(4336/4999): loss=0.5777635919114192\n",
      "Gradient Descent(4337/4999): loss=0.5777633989207213\n",
      "Gradient Descent(4338/4999): loss=0.5777632059774881\n",
      "Gradient Descent(4339/4999): loss=0.577763013081699\n",
      "Gradient Descent(4340/4999): loss=0.5777628202333334\n",
      "Gradient Descent(4341/4999): loss=0.57776262743237\n",
      "Gradient Descent(4342/4999): loss=0.5777624346787885\n",
      "Gradient Descent(4343/4999): loss=0.5777622419725681\n",
      "Gradient Descent(4344/4999): loss=0.5777620493136879\n",
      "Gradient Descent(4345/4999): loss=0.5777618567021273\n",
      "Gradient Descent(4346/4999): loss=0.5777616641378657\n",
      "Gradient Descent(4347/4999): loss=0.577761471620882\n",
      "Gradient Descent(4348/4999): loss=0.5777612791511559\n",
      "Gradient Descent(4349/4999): loss=0.5777610867286666\n",
      "Gradient Descent(4350/4999): loss=0.5777608943533933\n",
      "Gradient Descent(4351/4999): loss=0.5777607020253154\n",
      "Gradient Descent(4352/4999): loss=0.5777605097444125\n",
      "Gradient Descent(4353/4999): loss=0.5777603175106634\n",
      "Gradient Descent(4354/4999): loss=0.5777601253240481\n",
      "Gradient Descent(4355/4999): loss=0.5777599331845454\n",
      "Gradient Descent(4356/4999): loss=0.5777597410921353\n",
      "Gradient Descent(4357/4999): loss=0.5777595490467966\n",
      "Gradient Descent(4358/4999): loss=0.5777593570485092\n",
      "Gradient Descent(4359/4999): loss=0.5777591650972521\n",
      "Gradient Descent(4360/4999): loss=0.5777589731930051\n",
      "Gradient Descent(4361/4999): loss=0.5777587813357474\n",
      "Gradient Descent(4362/4999): loss=0.5777585895254584\n",
      "Gradient Descent(4363/4999): loss=0.5777583977621177\n",
      "Gradient Descent(4364/4999): loss=0.5777582060457049\n",
      "Gradient Descent(4365/4999): loss=0.5777580143761991\n",
      "Gradient Descent(4366/4999): loss=0.5777578227535801\n",
      "Gradient Descent(4367/4999): loss=0.5777576311778273\n",
      "Gradient Descent(4368/4999): loss=0.5777574396489202\n",
      "Gradient Descent(4369/4999): loss=0.5777572481668386\n",
      "Gradient Descent(4370/4999): loss=0.5777570567315614\n",
      "Gradient Descent(4371/4999): loss=0.5777568653430687\n",
      "Gradient Descent(4372/4999): loss=0.5777566740013397\n",
      "Gradient Descent(4373/4999): loss=0.5777564827063543\n",
      "Gradient Descent(4374/4999): loss=0.5777562914580918\n",
      "Gradient Descent(4375/4999): loss=0.5777561002565316\n",
      "Gradient Descent(4376/4999): loss=0.577755909101654\n",
      "Gradient Descent(4377/4999): loss=0.5777557179934378\n",
      "Gradient Descent(4378/4999): loss=0.5777555269318632\n",
      "Gradient Descent(4379/4999): loss=0.5777553359169094\n",
      "Gradient Descent(4380/4999): loss=0.5777551449485562\n",
      "Gradient Descent(4381/4999): loss=0.5777549540267833\n",
      "Gradient Descent(4382/4999): loss=0.5777547631515704\n",
      "Gradient Descent(4383/4999): loss=0.577754572322897\n",
      "Gradient Descent(4384/4999): loss=0.5777543815407428\n",
      "Gradient Descent(4385/4999): loss=0.5777541908050874\n",
      "Gradient Descent(4386/4999): loss=0.5777540001159107\n",
      "Gradient Descent(4387/4999): loss=0.5777538094731923\n",
      "Gradient Descent(4388/4999): loss=0.5777536188769119\n",
      "Gradient Descent(4389/4999): loss=0.5777534283270492\n",
      "Gradient Descent(4390/4999): loss=0.5777532378235839\n",
      "Gradient Descent(4391/4999): loss=0.5777530473664959\n",
      "Gradient Descent(4392/4999): loss=0.5777528569557648\n",
      "Gradient Descent(4393/4999): loss=0.5777526665913705\n",
      "Gradient Descent(4394/4999): loss=0.5777524762732925\n",
      "Gradient Descent(4395/4999): loss=0.5777522860015109\n",
      "Gradient Descent(4396/4999): loss=0.5777520957760052\n",
      "Gradient Descent(4397/4999): loss=0.5777519055967553\n",
      "Gradient Descent(4398/4999): loss=0.5777517154637412\n",
      "Gradient Descent(4399/4999): loss=0.5777515253769424\n",
      "Gradient Descent(4400/4999): loss=0.577751335336339\n",
      "Gradient Descent(4401/4999): loss=0.5777511453419106\n",
      "Gradient Descent(4402/4999): loss=0.5777509553936373\n",
      "Gradient Descent(4403/4999): loss=0.5777507654914988\n",
      "Gradient Descent(4404/4999): loss=0.5777505756354749\n",
      "Gradient Descent(4405/4999): loss=0.5777503858255457\n",
      "Gradient Descent(4406/4999): loss=0.577750196061691\n",
      "Gradient Descent(4407/4999): loss=0.5777500063438905\n",
      "Gradient Descent(4408/4999): loss=0.5777498166721244\n",
      "Gradient Descent(4409/4999): loss=0.5777496270463723\n",
      "Gradient Descent(4410/4999): loss=0.5777494374666144\n",
      "Gradient Descent(4411/4999): loss=0.5777492479328306\n",
      "Gradient Descent(4412/4999): loss=0.5777490584450009\n",
      "Gradient Descent(4413/4999): loss=0.5777488690031047\n",
      "Gradient Descent(4414/4999): loss=0.5777486796071227\n",
      "Gradient Descent(4415/4999): loss=0.5777484902570347\n",
      "Gradient Descent(4416/4999): loss=0.5777483009528205\n",
      "Gradient Descent(4417/4999): loss=0.5777481116944599\n",
      "Gradient Descent(4418/4999): loss=0.5777479224819334\n",
      "Gradient Descent(4419/4999): loss=0.5777477333152207\n",
      "Gradient Descent(4420/4999): loss=0.577747544194302\n",
      "Gradient Descent(4421/4999): loss=0.5777473551191571\n",
      "Gradient Descent(4422/4999): loss=0.5777471660897662\n",
      "Gradient Descent(4423/4999): loss=0.5777469771061093\n",
      "Gradient Descent(4424/4999): loss=0.5777467881681666\n",
      "Gradient Descent(4425/4999): loss=0.577746599275918\n",
      "Gradient Descent(4426/4999): loss=0.5777464104293436\n",
      "Gradient Descent(4427/4999): loss=0.5777462216284236\n",
      "Gradient Descent(4428/4999): loss=0.577746032873138\n",
      "Gradient Descent(4429/4999): loss=0.577745844163467\n",
      "Gradient Descent(4430/4999): loss=0.5777456554993905\n",
      "Gradient Descent(4431/4999): loss=0.5777454668808889\n",
      "Gradient Descent(4432/4999): loss=0.5777452783079423\n",
      "Gradient Descent(4433/4999): loss=0.5777450897805306\n",
      "Gradient Descent(4434/4999): loss=0.5777449012986342\n",
      "Gradient Descent(4435/4999): loss=0.5777447128622333\n",
      "Gradient Descent(4436/4999): loss=0.5777445244713079\n",
      "Gradient Descent(4437/4999): loss=0.5777443361258384\n",
      "Gradient Descent(4438/4999): loss=0.5777441478258047\n",
      "Gradient Descent(4439/4999): loss=0.5777439595711873\n",
      "Gradient Descent(4440/4999): loss=0.5777437713619661\n",
      "Gradient Descent(4441/4999): loss=0.5777435831981217\n",
      "Gradient Descent(4442/4999): loss=0.577743395079634\n",
      "Gradient Descent(4443/4999): loss=0.5777432070064836\n",
      "Gradient Descent(4444/4999): loss=0.5777430189786503\n",
      "Gradient Descent(4445/4999): loss=0.5777428309961147\n",
      "Gradient Descent(4446/4999): loss=0.5777426430588571\n",
      "Gradient Descent(4447/4999): loss=0.5777424551668576\n",
      "Gradient Descent(4448/4999): loss=0.5777422673200966\n",
      "Gradient Descent(4449/4999): loss=0.5777420795185543\n",
      "Gradient Descent(4450/4999): loss=0.577741891762211\n",
      "Gradient Descent(4451/4999): loss=0.577741704051047\n",
      "Gradient Descent(4452/4999): loss=0.577741516385043\n",
      "Gradient Descent(4453/4999): loss=0.5777413287641788\n",
      "Gradient Descent(4454/4999): loss=0.5777411411884353\n",
      "Gradient Descent(4455/4999): loss=0.5777409536577924\n",
      "Gradient Descent(4456/4999): loss=0.5777407661722306\n",
      "Gradient Descent(4457/4999): loss=0.5777405787317303\n",
      "Gradient Descent(4458/4999): loss=0.577740391336272\n",
      "Gradient Descent(4459/4999): loss=0.5777402039858359\n",
      "Gradient Descent(4460/4999): loss=0.5777400166804025\n",
      "Gradient Descent(4461/4999): loss=0.5777398294199523\n",
      "Gradient Descent(4462/4999): loss=0.5777396422044656\n",
      "Gradient Descent(4463/4999): loss=0.5777394550339229\n",
      "Gradient Descent(4464/4999): loss=0.5777392679083047\n",
      "Gradient Descent(4465/4999): loss=0.5777390808275912\n",
      "Gradient Descent(4466/4999): loss=0.5777388937917629\n",
      "Gradient Descent(4467/4999): loss=0.5777387068008005\n",
      "Gradient Descent(4468/4999): loss=0.5777385198546846\n",
      "Gradient Descent(4469/4999): loss=0.5777383329533953\n",
      "Gradient Descent(4470/4999): loss=0.5777381460969132\n",
      "Gradient Descent(4471/4999): loss=0.577737959285219\n",
      "Gradient Descent(4472/4999): loss=0.5777377725182932\n",
      "Gradient Descent(4473/4999): loss=0.5777375857961161\n",
      "Gradient Descent(4474/4999): loss=0.5777373991186684\n",
      "Gradient Descent(4475/4999): loss=0.5777372124859305\n",
      "Gradient Descent(4476/4999): loss=0.577737025897883\n",
      "Gradient Descent(4477/4999): loss=0.5777368393545068\n",
      "Gradient Descent(4478/4999): loss=0.577736652855782\n",
      "Gradient Descent(4479/4999): loss=0.5777364664016895\n",
      "Gradient Descent(4480/4999): loss=0.5777362799922097\n",
      "Gradient Descent(4481/4999): loss=0.5777360936273233\n",
      "Gradient Descent(4482/4999): loss=0.5777359073070109\n",
      "Gradient Descent(4483/4999): loss=0.5777357210312531\n",
      "Gradient Descent(4484/4999): loss=0.5777355348000306\n",
      "Gradient Descent(4485/4999): loss=0.5777353486133239\n",
      "Gradient Descent(4486/4999): loss=0.5777351624711137\n",
      "Gradient Descent(4487/4999): loss=0.5777349763733808\n",
      "Gradient Descent(4488/4999): loss=0.5777347903201056\n",
      "Gradient Descent(4489/4999): loss=0.5777346043112691\n",
      "Gradient Descent(4490/4999): loss=0.5777344183468517\n",
      "Gradient Descent(4491/4999): loss=0.5777342324268343\n",
      "Gradient Descent(4492/4999): loss=0.5777340465511974\n",
      "Gradient Descent(4493/4999): loss=0.5777338607199219\n",
      "Gradient Descent(4494/4999): loss=0.5777336749329884\n",
      "Gradient Descent(4495/4999): loss=0.5777334891903778\n",
      "Gradient Descent(4496/4999): loss=0.5777333034920705\n",
      "Gradient Descent(4497/4999): loss=0.5777331178380477\n",
      "Gradient Descent(4498/4999): loss=0.5777329322282898\n",
      "Gradient Descent(4499/4999): loss=0.5777327466627777\n",
      "Gradient Descent(4500/4999): loss=0.5777325611414922\n",
      "Gradient Descent(4501/4999): loss=0.577732375664414\n",
      "Gradient Descent(4502/4999): loss=0.5777321902315241\n",
      "Gradient Descent(4503/4999): loss=0.5777320048428031\n",
      "Gradient Descent(4504/4999): loss=0.5777318194982318\n",
      "Gradient Descent(4505/4999): loss=0.5777316341977912\n",
      "Gradient Descent(4506/4999): loss=0.5777314489414618\n",
      "Gradient Descent(4507/4999): loss=0.577731263729225\n",
      "Gradient Descent(4508/4999): loss=0.577731078561061\n",
      "Gradient Descent(4509/4999): loss=0.5777308934369513\n",
      "Gradient Descent(4510/4999): loss=0.5777307083568762\n",
      "Gradient Descent(4511/4999): loss=0.5777305233208168\n",
      "Gradient Descent(4512/4999): loss=0.5777303383287543\n",
      "Gradient Descent(4513/4999): loss=0.577730153380669\n",
      "Gradient Descent(4514/4999): loss=0.5777299684765422\n",
      "Gradient Descent(4515/4999): loss=0.5777297836163547\n",
      "Gradient Descent(4516/4999): loss=0.5777295988000876\n",
      "Gradient Descent(4517/4999): loss=0.5777294140277216\n",
      "Gradient Descent(4518/4999): loss=0.5777292292992378\n",
      "Gradient Descent(4519/4999): loss=0.5777290446146169\n",
      "Gradient Descent(4520/4999): loss=0.5777288599738402\n",
      "Gradient Descent(4521/4999): loss=0.5777286753768883\n",
      "Gradient Descent(4522/4999): loss=0.5777284908237426\n",
      "Gradient Descent(4523/4999): loss=0.5777283063143838\n",
      "Gradient Descent(4524/4999): loss=0.5777281218487929\n",
      "Gradient Descent(4525/4999): loss=0.5777279374269509\n",
      "Gradient Descent(4526/4999): loss=0.577727753048839\n",
      "Gradient Descent(4527/4999): loss=0.5777275687144382\n",
      "Gradient Descent(4528/4999): loss=0.5777273844237293\n",
      "Gradient Descent(4529/4999): loss=0.5777272001766935\n",
      "Gradient Descent(4530/4999): loss=0.5777270159733118\n",
      "Gradient Descent(4531/4999): loss=0.5777268318135652\n",
      "Gradient Descent(4532/4999): loss=0.577726647697435\n",
      "Gradient Descent(4533/4999): loss=0.5777264636249021\n",
      "Gradient Descent(4534/4999): loss=0.5777262795959476\n",
      "Gradient Descent(4535/4999): loss=0.5777260956105524\n",
      "Gradient Descent(4536/4999): loss=0.577725911668698\n",
      "Gradient Descent(4537/4999): loss=0.5777257277703653\n",
      "Gradient Descent(4538/4999): loss=0.5777255439155354\n",
      "Gradient Descent(4539/4999): loss=0.5777253601041896\n",
      "Gradient Descent(4540/4999): loss=0.5777251763363087\n",
      "Gradient Descent(4541/4999): loss=0.5777249926118743\n",
      "Gradient Descent(4542/4999): loss=0.5777248089308671\n",
      "Gradient Descent(4543/4999): loss=0.5777246252932686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4544/4999): loss=0.5777244416990598\n",
      "Gradient Descent(4545/4999): loss=0.577724258148222\n",
      "Gradient Descent(4546/4999): loss=0.5777240746407362\n",
      "Gradient Descent(4547/4999): loss=0.5777238911765838\n",
      "Gradient Descent(4548/4999): loss=0.577723707755746\n",
      "Gradient Descent(4549/4999): loss=0.577723524378204\n",
      "Gradient Descent(4550/4999): loss=0.5777233410439389\n",
      "Gradient Descent(4551/4999): loss=0.5777231577529319\n",
      "Gradient Descent(4552/4999): loss=0.5777229745051645\n",
      "Gradient Descent(4553/4999): loss=0.577722791300618\n",
      "Gradient Descent(4554/4999): loss=0.5777226081392732\n",
      "Gradient Descent(4555/4999): loss=0.5777224250211118\n",
      "Gradient Descent(4556/4999): loss=0.5777222419461149\n",
      "Gradient Descent(4557/4999): loss=0.5777220589142639\n",
      "Gradient Descent(4558/4999): loss=0.5777218759255398\n",
      "Gradient Descent(4559/4999): loss=0.5777216929799245\n",
      "Gradient Descent(4560/4999): loss=0.5777215100773987\n",
      "Gradient Descent(4561/4999): loss=0.5777213272179441\n",
      "Gradient Descent(4562/4999): loss=0.5777211444015418\n",
      "Gradient Descent(4563/4999): loss=0.5777209616281733\n",
      "Gradient Descent(4564/4999): loss=0.5777207788978199\n",
      "Gradient Descent(4565/4999): loss=0.5777205962104631\n",
      "Gradient Descent(4566/4999): loss=0.577720413566084\n",
      "Gradient Descent(4567/4999): loss=0.5777202309646641\n",
      "Gradient Descent(4568/4999): loss=0.577720048406185\n",
      "Gradient Descent(4569/4999): loss=0.5777198658906276\n",
      "Gradient Descent(4570/4999): loss=0.5777196834179739\n",
      "Gradient Descent(4571/4999): loss=0.5777195009882046\n",
      "Gradient Descent(4572/4999): loss=0.5777193186013019\n",
      "Gradient Descent(4573/4999): loss=0.5777191362572467\n",
      "Gradient Descent(4574/4999): loss=0.5777189539560207\n",
      "Gradient Descent(4575/4999): loss=0.5777187716976051\n",
      "Gradient Descent(4576/4999): loss=0.5777185894819817\n",
      "Gradient Descent(4577/4999): loss=0.5777184073091317\n",
      "Gradient Descent(4578/4999): loss=0.5777182251790367\n",
      "Gradient Descent(4579/4999): loss=0.577718043091678\n",
      "Gradient Descent(4580/4999): loss=0.5777178610470372\n",
      "Gradient Descent(4581/4999): loss=0.577717679045096\n",
      "Gradient Descent(4582/4999): loss=0.5777174970858355\n",
      "Gradient Descent(4583/4999): loss=0.5777173151692376\n",
      "Gradient Descent(4584/4999): loss=0.5777171332952836\n",
      "Gradient Descent(4585/4999): loss=0.5777169514639552\n",
      "Gradient Descent(4586/4999): loss=0.5777167696752339\n",
      "Gradient Descent(4587/4999): loss=0.577716587929101\n",
      "Gradient Descent(4588/4999): loss=0.5777164062255383\n",
      "Gradient Descent(4589/4999): loss=0.5777162245645275\n",
      "Gradient Descent(4590/4999): loss=0.5777160429460498\n",
      "Gradient Descent(4591/4999): loss=0.5777158613700872\n",
      "Gradient Descent(4592/4999): loss=0.5777156798366211\n",
      "Gradient Descent(4593/4999): loss=0.5777154983456328\n",
      "Gradient Descent(4594/4999): loss=0.5777153168971044\n",
      "Gradient Descent(4595/4999): loss=0.5777151354910174\n",
      "Gradient Descent(4596/4999): loss=0.5777149541273533\n",
      "Gradient Descent(4597/4999): loss=0.5777147728060937\n",
      "Gradient Descent(4598/4999): loss=0.5777145915272205\n",
      "Gradient Descent(4599/4999): loss=0.5777144102907151\n",
      "Gradient Descent(4600/4999): loss=0.5777142290965593\n",
      "Gradient Descent(4601/4999): loss=0.5777140479447347\n",
      "Gradient Descent(4602/4999): loss=0.5777138668352231\n",
      "Gradient Descent(4603/4999): loss=0.5777136857680062\n",
      "Gradient Descent(4604/4999): loss=0.5777135047430654\n",
      "Gradient Descent(4605/4999): loss=0.5777133237603825\n",
      "Gradient Descent(4606/4999): loss=0.5777131428199396\n",
      "Gradient Descent(4607/4999): loss=0.5777129619217181\n",
      "Gradient Descent(4608/4999): loss=0.5777127810656997\n",
      "Gradient Descent(4609/4999): loss=0.5777126002518663\n",
      "Gradient Descent(4610/4999): loss=0.5777124194801995\n",
      "Gradient Descent(4611/4999): loss=0.5777122387506812\n",
      "Gradient Descent(4612/4999): loss=0.5777120580632932\n",
      "Gradient Descent(4613/4999): loss=0.5777118774180169\n",
      "Gradient Descent(4614/4999): loss=0.5777116968148345\n",
      "Gradient Descent(4615/4999): loss=0.5777115162537277\n",
      "Gradient Descent(4616/4999): loss=0.5777113357346784\n",
      "Gradient Descent(4617/4999): loss=0.5777111552576679\n",
      "Gradient Descent(4618/4999): loss=0.5777109748226785\n",
      "Gradient Descent(4619/4999): loss=0.577710794429692\n",
      "Gradient Descent(4620/4999): loss=0.5777106140786901\n",
      "Gradient Descent(4621/4999): loss=0.5777104337696547\n",
      "Gradient Descent(4622/4999): loss=0.5777102535025676\n",
      "Gradient Descent(4623/4999): loss=0.5777100732774108\n",
      "Gradient Descent(4624/4999): loss=0.5777098930941659\n",
      "Gradient Descent(4625/4999): loss=0.577709712952815\n",
      "Gradient Descent(4626/4999): loss=0.57770953285334\n",
      "Gradient Descent(4627/4999): loss=0.5777093527957227\n",
      "Gradient Descent(4628/4999): loss=0.5777091727799448\n",
      "Gradient Descent(4629/4999): loss=0.5777089928059888\n",
      "Gradient Descent(4630/4999): loss=0.577708812873836\n",
      "Gradient Descent(4631/4999): loss=0.5777086329834685\n",
      "Gradient Descent(4632/4999): loss=0.5777084531348686\n",
      "Gradient Descent(4633/4999): loss=0.5777082733280179\n",
      "Gradient Descent(4634/4999): loss=0.5777080935628984\n",
      "Gradient Descent(4635/4999): loss=0.5777079138394919\n",
      "Gradient Descent(4636/4999): loss=0.5777077341577807\n",
      "Gradient Descent(4637/4999): loss=0.5777075545177466\n",
      "Gradient Descent(4638/4999): loss=0.5777073749193715\n",
      "Gradient Descent(4639/4999): loss=0.5777071953626377\n",
      "Gradient Descent(4640/4999): loss=0.5777070158475268\n",
      "Gradient Descent(4641/4999): loss=0.5777068363740211\n",
      "Gradient Descent(4642/4999): loss=0.5777066569421025\n",
      "Gradient Descent(4643/4999): loss=0.5777064775517532\n",
      "Gradient Descent(4644/4999): loss=0.5777062982029549\n",
      "Gradient Descent(4645/4999): loss=0.5777061188956901\n",
      "Gradient Descent(4646/4999): loss=0.5777059396299404\n",
      "Gradient Descent(4647/4999): loss=0.5777057604056882\n",
      "Gradient Descent(4648/4999): loss=0.5777055812229154\n",
      "Gradient Descent(4649/4999): loss=0.5777054020816041\n",
      "Gradient Descent(4650/4999): loss=0.5777052229817363\n",
      "Gradient Descent(4651/4999): loss=0.5777050439232942\n",
      "Gradient Descent(4652/4999): loss=0.5777048649062599\n",
      "Gradient Descent(4653/4999): loss=0.5777046859306157\n",
      "Gradient Descent(4654/4999): loss=0.5777045069963435\n",
      "Gradient Descent(4655/4999): loss=0.5777043281034253\n",
      "Gradient Descent(4656/4999): loss=0.5777041492518435\n",
      "Gradient Descent(4657/4999): loss=0.5777039704415801\n",
      "Gradient Descent(4658/4999): loss=0.5777037916726171\n",
      "Gradient Descent(4659/4999): loss=0.5777036129449372\n",
      "Gradient Descent(4660/4999): loss=0.577703434258522\n",
      "Gradient Descent(4661/4999): loss=0.5777032556133539\n",
      "Gradient Descent(4662/4999): loss=0.5777030770094153\n",
      "Gradient Descent(4663/4999): loss=0.5777028984466879\n",
      "Gradient Descent(4664/4999): loss=0.5777027199251542\n",
      "Gradient Descent(4665/4999): loss=0.5777025414447965\n",
      "Gradient Descent(4666/4999): loss=0.5777023630055969\n",
      "Gradient Descent(4667/4999): loss=0.5777021846075377\n",
      "Gradient Descent(4668/4999): loss=0.577702006250601\n",
      "Gradient Descent(4669/4999): loss=0.5777018279347692\n",
      "Gradient Descent(4670/4999): loss=0.5777016496600244\n",
      "Gradient Descent(4671/4999): loss=0.5777014714263489\n",
      "Gradient Descent(4672/4999): loss=0.577701293233725\n",
      "Gradient Descent(4673/4999): loss=0.577701115082135\n",
      "Gradient Descent(4674/4999): loss=0.5777009369715611\n",
      "Gradient Descent(4675/4999): loss=0.5777007589019858\n",
      "Gradient Descent(4676/4999): loss=0.5777005808733912\n",
      "Gradient Descent(4677/4999): loss=0.5777004028857596\n",
      "Gradient Descent(4678/4999): loss=0.5777002249390735\n",
      "Gradient Descent(4679/4999): loss=0.5777000470333151\n",
      "Gradient Descent(4680/4999): loss=0.5776998691684666\n",
      "Gradient Descent(4681/4999): loss=0.5776996913445106\n",
      "Gradient Descent(4682/4999): loss=0.5776995135614293\n",
      "Gradient Descent(4683/4999): loss=0.5776993358192052\n",
      "Gradient Descent(4684/4999): loss=0.5776991581178205\n",
      "Gradient Descent(4685/4999): loss=0.5776989804572574\n",
      "Gradient Descent(4686/4999): loss=0.5776988028374987\n",
      "Gradient Descent(4687/4999): loss=0.5776986252585267\n",
      "Gradient Descent(4688/4999): loss=0.5776984477203237\n",
      "Gradient Descent(4689/4999): loss=0.577698270222872\n",
      "Gradient Descent(4690/4999): loss=0.5776980927661541\n",
      "Gradient Descent(4691/4999): loss=0.5776979153501527\n",
      "Gradient Descent(4692/4999): loss=0.5776977379748498\n",
      "Gradient Descent(4693/4999): loss=0.577697560640228\n",
      "Gradient Descent(4694/4999): loss=0.57769738334627\n",
      "Gradient Descent(4695/4999): loss=0.5776972060929577\n",
      "Gradient Descent(4696/4999): loss=0.5776970288802741\n",
      "Gradient Descent(4697/4999): loss=0.5776968517082015\n",
      "Gradient Descent(4698/4999): loss=0.5776966745767221\n",
      "Gradient Descent(4699/4999): loss=0.5776964974858189\n",
      "Gradient Descent(4700/4999): loss=0.577696320435474\n",
      "Gradient Descent(4701/4999): loss=0.5776961434256701\n",
      "Gradient Descent(4702/4999): loss=0.5776959664563897\n",
      "Gradient Descent(4703/4999): loss=0.5776957895276151\n",
      "Gradient Descent(4704/4999): loss=0.5776956126393292\n",
      "Gradient Descent(4705/4999): loss=0.5776954357915143\n",
      "Gradient Descent(4706/4999): loss=0.577695258984153\n",
      "Gradient Descent(4707/4999): loss=0.5776950822172277\n",
      "Gradient Descent(4708/4999): loss=0.5776949054907211\n",
      "Gradient Descent(4709/4999): loss=0.5776947288046157\n",
      "Gradient Descent(4710/4999): loss=0.5776945521588942\n",
      "Gradient Descent(4711/4999): loss=0.5776943755535393\n",
      "Gradient Descent(4712/4999): loss=0.5776941989885332\n",
      "Gradient Descent(4713/4999): loss=0.577694022463859\n",
      "Gradient Descent(4714/4999): loss=0.5776938459794988\n",
      "Gradient Descent(4715/4999): loss=0.5776936695354354\n",
      "Gradient Descent(4716/4999): loss=0.5776934931316517\n",
      "Gradient Descent(4717/4999): loss=0.5776933167681301\n",
      "Gradient Descent(4718/4999): loss=0.577693140444853\n",
      "Gradient Descent(4719/4999): loss=0.5776929641618035\n",
      "Gradient Descent(4720/4999): loss=0.577692787918964\n",
      "Gradient Descent(4721/4999): loss=0.5776926117163171\n",
      "Gradient Descent(4722/4999): loss=0.5776924355538458\n",
      "Gradient Descent(4723/4999): loss=0.5776922594315324\n",
      "Gradient Descent(4724/4999): loss=0.5776920833493597\n",
      "Gradient Descent(4725/4999): loss=0.5776919073073107\n",
      "Gradient Descent(4726/4999): loss=0.5776917313053676\n",
      "Gradient Descent(4727/4999): loss=0.5776915553435137\n",
      "Gradient Descent(4728/4999): loss=0.5776913794217311\n",
      "Gradient Descent(4729/4999): loss=0.577691203540003\n",
      "Gradient Descent(4730/4999): loss=0.5776910276983118\n",
      "Gradient Descent(4731/4999): loss=0.5776908518966405\n",
      "Gradient Descent(4732/4999): loss=0.5776906761349717\n",
      "Gradient Descent(4733/4999): loss=0.5776905004132885\n",
      "Gradient Descent(4734/4999): loss=0.577690324731573\n",
      "Gradient Descent(4735/4999): loss=0.5776901490898086\n",
      "Gradient Descent(4736/4999): loss=0.5776899734879777\n",
      "Gradient Descent(4737/4999): loss=0.5776897979260633\n",
      "Gradient Descent(4738/4999): loss=0.5776896224040482\n",
      "Gradient Descent(4739/4999): loss=0.5776894469219152\n",
      "Gradient Descent(4740/4999): loss=0.5776892714796468\n",
      "Gradient Descent(4741/4999): loss=0.5776890960772263\n",
      "Gradient Descent(4742/4999): loss=0.5776889207146362\n",
      "Gradient Descent(4743/4999): loss=0.5776887453918595\n",
      "Gradient Descent(4744/4999): loss=0.577688570108879\n",
      "Gradient Descent(4745/4999): loss=0.5776883948656777\n",
      "Gradient Descent(4746/4999): loss=0.5776882196622382\n",
      "Gradient Descent(4747/4999): loss=0.5776880444985435\n",
      "Gradient Descent(4748/4999): loss=0.5776878693745764\n",
      "Gradient Descent(4749/4999): loss=0.57768769429032\n",
      "Gradient Descent(4750/4999): loss=0.577687519245757\n",
      "Gradient Descent(4751/4999): loss=0.5776873442408703\n",
      "Gradient Descent(4752/4999): loss=0.5776871692756429\n",
      "Gradient Descent(4753/4999): loss=0.5776869943500578\n",
      "Gradient Descent(4754/4999): loss=0.5776868194640975\n",
      "Gradient Descent(4755/4999): loss=0.5776866446177454\n",
      "Gradient Descent(4756/4999): loss=0.5776864698109845\n",
      "Gradient Descent(4757/4999): loss=0.5776862950437972\n",
      "Gradient Descent(4758/4999): loss=0.5776861203161671\n",
      "Gradient Descent(4759/4999): loss=0.5776859456280766\n",
      "Gradient Descent(4760/4999): loss=0.577685770979509\n",
      "Gradient Descent(4761/4999): loss=0.5776855963704474\n",
      "Gradient Descent(4762/4999): loss=0.5776854218008743\n",
      "Gradient Descent(4763/4999): loss=0.5776852472707731\n",
      "Gradient Descent(4764/4999): loss=0.5776850727801267\n",
      "Gradient Descent(4765/4999): loss=0.5776848983289182\n",
      "Gradient Descent(4766/4999): loss=0.5776847239171303\n",
      "Gradient Descent(4767/4999): loss=0.5776845495447465\n",
      "Gradient Descent(4768/4999): loss=0.5776843752117494\n",
      "Gradient Descent(4769/4999): loss=0.5776842009181224\n",
      "Gradient Descent(4770/4999): loss=0.5776840266638483\n",
      "Gradient Descent(4771/4999): loss=0.5776838524489102\n",
      "Gradient Descent(4772/4999): loss=0.5776836782732913\n",
      "Gradient Descent(4773/4999): loss=0.5776835041369744\n",
      "Gradient Descent(4774/4999): loss=0.5776833300399429\n",
      "Gradient Descent(4775/4999): loss=0.5776831559821798\n",
      "Gradient Descent(4776/4999): loss=0.577682981963668\n",
      "Gradient Descent(4777/4999): loss=0.5776828079843909\n",
      "Gradient Descent(4778/4999): loss=0.5776826340443313\n",
      "Gradient Descent(4779/4999): loss=0.5776824601434726\n",
      "Gradient Descent(4780/4999): loss=0.5776822862817976\n",
      "Gradient Descent(4781/4999): loss=0.5776821124592899\n",
      "Gradient Descent(4782/4999): loss=0.5776819386759324\n",
      "Gradient Descent(4783/4999): loss=0.5776817649317082\n",
      "Gradient Descent(4784/4999): loss=0.5776815912266005\n",
      "Gradient Descent(4785/4999): loss=0.5776814175605922\n",
      "Gradient Descent(4786/4999): loss=0.5776812439336669\n",
      "Gradient Descent(4787/4999): loss=0.5776810703458077\n",
      "Gradient Descent(4788/4999): loss=0.5776808967969976\n",
      "Gradient Descent(4789/4999): loss=0.5776807232872201\n",
      "Gradient Descent(4790/4999): loss=0.577680549816458\n",
      "Gradient Descent(4791/4999): loss=0.5776803763846948\n",
      "Gradient Descent(4792/4999): loss=0.5776802029919137\n",
      "Gradient Descent(4793/4999): loss=0.5776800296380977\n",
      "Gradient Descent(4794/4999): loss=0.5776798563232304\n",
      "Gradient Descent(4795/4999): loss=0.5776796830472947\n",
      "Gradient Descent(4796/4999): loss=0.5776795098102742\n",
      "Gradient Descent(4797/4999): loss=0.5776793366121517\n",
      "Gradient Descent(4798/4999): loss=0.5776791634529107\n",
      "Gradient Descent(4799/4999): loss=0.5776789903325347\n",
      "Gradient Descent(4800/4999): loss=0.5776788172510067\n",
      "Gradient Descent(4801/4999): loss=0.57767864420831\n",
      "Gradient Descent(4802/4999): loss=0.577678471204428\n",
      "Gradient Descent(4803/4999): loss=0.577678298239344\n",
      "Gradient Descent(4804/4999): loss=0.5776781253130412\n",
      "Gradient Descent(4805/4999): loss=0.5776779524255031\n",
      "Gradient Descent(4806/4999): loss=0.577677779576713\n",
      "Gradient Descent(4807/4999): loss=0.5776776067666538\n",
      "Gradient Descent(4808/4999): loss=0.5776774339953094\n",
      "Gradient Descent(4809/4999): loss=0.577677261262663\n",
      "Gradient Descent(4810/4999): loss=0.5776770885686978\n",
      "Gradient Descent(4811/4999): loss=0.5776769159133973\n",
      "Gradient Descent(4812/4999): loss=0.5776767432967449\n",
      "Gradient Descent(4813/4999): loss=0.5776765707187239\n",
      "Gradient Descent(4814/4999): loss=0.5776763981793177\n",
      "Gradient Descent(4815/4999): loss=0.5776762256785095\n",
      "Gradient Descent(4816/4999): loss=0.5776760532162831\n",
      "Gradient Descent(4817/4999): loss=0.5776758807926218\n",
      "Gradient Descent(4818/4999): loss=0.5776757084075087\n",
      "Gradient Descent(4819/4999): loss=0.5776755360609276\n",
      "Gradient Descent(4820/4999): loss=0.5776753637528618\n",
      "Gradient Descent(4821/4999): loss=0.5776751914832947\n",
      "Gradient Descent(4822/4999): loss=0.5776750192522097\n",
      "Gradient Descent(4823/4999): loss=0.5776748470595904\n",
      "Gradient Descent(4824/4999): loss=0.5776746749054201\n",
      "Gradient Descent(4825/4999): loss=0.5776745027896826\n",
      "Gradient Descent(4826/4999): loss=0.5776743307123609\n",
      "Gradient Descent(4827/4999): loss=0.5776741586734387\n",
      "Gradient Descent(4828/4999): loss=0.5776739866728996\n",
      "Gradient Descent(4829/4999): loss=0.5776738147107271\n",
      "Gradient Descent(4830/4999): loss=0.5776736427869046\n",
      "Gradient Descent(4831/4999): loss=0.5776734709014156\n",
      "Gradient Descent(4832/4999): loss=0.5776732990542437\n",
      "Gradient Descent(4833/4999): loss=0.5776731272453722\n",
      "Gradient Descent(4834/4999): loss=0.577672955474785\n",
      "Gradient Descent(4835/4999): loss=0.5776727837424653\n",
      "Gradient Descent(4836/4999): loss=0.5776726120483973\n",
      "Gradient Descent(4837/4999): loss=0.5776724403925636\n",
      "Gradient Descent(4838/4999): loss=0.5776722687749485\n",
      "Gradient Descent(4839/4999): loss=0.5776720971955353\n",
      "Gradient Descent(4840/4999): loss=0.5776719256543076\n",
      "Gradient Descent(4841/4999): loss=0.5776717541512488\n",
      "Gradient Descent(4842/4999): loss=0.5776715826863429\n",
      "Gradient Descent(4843/4999): loss=0.5776714112595733\n",
      "Gradient Descent(4844/4999): loss=0.5776712398709236\n",
      "Gradient Descent(4845/4999): loss=0.5776710685203774\n",
      "Gradient Descent(4846/4999): loss=0.5776708972079184\n",
      "Gradient Descent(4847/4999): loss=0.5776707259335302\n",
      "Gradient Descent(4848/4999): loss=0.5776705546971964\n",
      "Gradient Descent(4849/4999): loss=0.5776703834989007\n",
      "Gradient Descent(4850/4999): loss=0.5776702123386268\n",
      "Gradient Descent(4851/4999): loss=0.5776700412163582\n",
      "Gradient Descent(4852/4999): loss=0.5776698701320788\n",
      "Gradient Descent(4853/4999): loss=0.5776696990857721\n",
      "Gradient Descent(4854/4999): loss=0.5776695280774219\n",
      "Gradient Descent(4855/4999): loss=0.5776693571070117\n",
      "Gradient Descent(4856/4999): loss=0.5776691861745255\n",
      "Gradient Descent(4857/4999): loss=0.5776690152799467\n",
      "Gradient Descent(4858/4999): loss=0.5776688444232593\n",
      "Gradient Descent(4859/4999): loss=0.577668673604447\n",
      "Gradient Descent(4860/4999): loss=0.5776685028234931\n",
      "Gradient Descent(4861/4999): loss=0.5776683320803818\n",
      "Gradient Descent(4862/4999): loss=0.5776681613750967\n",
      "Gradient Descent(4863/4999): loss=0.5776679907076216\n",
      "Gradient Descent(4864/4999): loss=0.5776678200779403\n",
      "Gradient Descent(4865/4999): loss=0.5776676494860363\n",
      "Gradient Descent(4866/4999): loss=0.5776674789318936\n",
      "Gradient Descent(4867/4999): loss=0.577667308415496\n",
      "Gradient Descent(4868/4999): loss=0.5776671379368271\n",
      "Gradient Descent(4869/4999): loss=0.577666967495871\n",
      "Gradient Descent(4870/4999): loss=0.5776667970926112\n",
      "Gradient Descent(4871/4999): loss=0.5776666267270316\n",
      "Gradient Descent(4872/4999): loss=0.5776664563991162\n",
      "Gradient Descent(4873/4999): loss=0.5776662861088485\n",
      "Gradient Descent(4874/4999): loss=0.5776661158562127\n",
      "Gradient Descent(4875/4999): loss=0.5776659456411926\n",
      "Gradient Descent(4876/4999): loss=0.5776657754637716\n",
      "Gradient Descent(4877/4999): loss=0.5776656053239342\n",
      "Gradient Descent(4878/4999): loss=0.5776654352216636\n",
      "Gradient Descent(4879/4999): loss=0.5776652651569442\n",
      "Gradient Descent(4880/4999): loss=0.5776650951297595\n",
      "Gradient Descent(4881/4999): loss=0.5776649251400936\n",
      "Gradient Descent(4882/4999): loss=0.5776647551879304\n",
      "Gradient Descent(4883/4999): loss=0.5776645852732538\n",
      "Gradient Descent(4884/4999): loss=0.5776644153960476\n",
      "Gradient Descent(4885/4999): loss=0.5776642455562958\n",
      "Gradient Descent(4886/4999): loss=0.5776640757539823\n",
      "Gradient Descent(4887/4999): loss=0.5776639059890908\n",
      "Gradient Descent(4888/4999): loss=0.5776637362616057\n",
      "Gradient Descent(4889/4999): loss=0.5776635665715107\n",
      "Gradient Descent(4890/4999): loss=0.5776633969187895\n",
      "Gradient Descent(4891/4999): loss=0.5776632273034266\n",
      "Gradient Descent(4892/4999): loss=0.5776630577254056\n",
      "Gradient Descent(4893/4999): loss=0.5776628881847105\n",
      "Gradient Descent(4894/4999): loss=0.5776627186813252\n",
      "Gradient Descent(4895/4999): loss=0.577662549215234\n",
      "Gradient Descent(4896/4999): loss=0.5776623797864205\n",
      "Gradient Descent(4897/4999): loss=0.577662210394869\n",
      "Gradient Descent(4898/4999): loss=0.5776620410405634\n",
      "Gradient Descent(4899/4999): loss=0.5776618717234878\n",
      "Gradient Descent(4900/4999): loss=0.577661702443626\n",
      "Gradient Descent(4901/4999): loss=0.5776615332009623\n",
      "Gradient Descent(4902/4999): loss=0.5776613639954805\n",
      "Gradient Descent(4903/4999): loss=0.5776611948271648\n",
      "Gradient Descent(4904/4999): loss=0.5776610256959993\n",
      "Gradient Descent(4905/4999): loss=0.5776608566019679\n",
      "Gradient Descent(4906/4999): loss=0.5776606875450546\n",
      "Gradient Descent(4907/4999): loss=0.5776605185252437\n",
      "Gradient Descent(4908/4999): loss=0.5776603495425192\n",
      "Gradient Descent(4909/4999): loss=0.5776601805968652\n",
      "Gradient Descent(4910/4999): loss=0.5776600116882659\n",
      "Gradient Descent(4911/4999): loss=0.5776598428167049\n",
      "Gradient Descent(4912/4999): loss=0.5776596739821669\n",
      "Gradient Descent(4913/4999): loss=0.5776595051846358\n",
      "Gradient Descent(4914/4999): loss=0.5776593364240956\n",
      "Gradient Descent(4915/4999): loss=0.5776591677005307\n",
      "Gradient Descent(4916/4999): loss=0.577658999013925\n",
      "Gradient Descent(4917/4999): loss=0.5776588303642627\n",
      "Gradient Descent(4918/4999): loss=0.577658661751528\n",
      "Gradient Descent(4919/4999): loss=0.577658493175705\n",
      "Gradient Descent(4920/4999): loss=0.5776583246367779\n",
      "Gradient Descent(4921/4999): loss=0.5776581561347309\n",
      "Gradient Descent(4922/4999): loss=0.5776579876695481\n",
      "Gradient Descent(4923/4999): loss=0.5776578192412137\n",
      "Gradient Descent(4924/4999): loss=0.5776576508497121\n",
      "Gradient Descent(4925/4999): loss=0.5776574824950272\n",
      "Gradient Descent(4926/4999): loss=0.5776573141771435\n",
      "Gradient Descent(4927/4999): loss=0.577657145896045\n",
      "Gradient Descent(4928/4999): loss=0.5776569776517159\n",
      "Gradient Descent(4929/4999): loss=0.5776568094441406\n",
      "Gradient Descent(4930/4999): loss=0.5776566412733031\n",
      "Gradient Descent(4931/4999): loss=0.5776564731391879\n",
      "Gradient Descent(4932/4999): loss=0.5776563050417791\n",
      "Gradient Descent(4933/4999): loss=0.5776561369810611\n",
      "Gradient Descent(4934/4999): loss=0.5776559689570181\n",
      "Gradient Descent(4935/4999): loss=0.5776558009696342\n",
      "Gradient Descent(4936/4999): loss=0.5776556330188938\n",
      "Gradient Descent(4937/4999): loss=0.5776554651047813\n",
      "Gradient Descent(4938/4999): loss=0.5776552972272809\n",
      "Gradient Descent(4939/4999): loss=0.5776551293863768\n",
      "Gradient Descent(4940/4999): loss=0.5776549615820535\n",
      "Gradient Descent(4941/4999): loss=0.5776547938142952\n",
      "Gradient Descent(4942/4999): loss=0.5776546260830863\n",
      "Gradient Descent(4943/4999): loss=0.577654458388411\n",
      "Gradient Descent(4944/4999): loss=0.5776542907302538\n",
      "Gradient Descent(4945/4999): loss=0.5776541231085989\n",
      "Gradient Descent(4946/4999): loss=0.5776539555234306\n",
      "Gradient Descent(4947/4999): loss=0.5776537879747334\n",
      "Gradient Descent(4948/4999): loss=0.5776536204624918\n",
      "Gradient Descent(4949/4999): loss=0.5776534529866898\n",
      "Gradient Descent(4950/4999): loss=0.5776532855473121\n",
      "Gradient Descent(4951/4999): loss=0.5776531181443428\n",
      "Gradient Descent(4952/4999): loss=0.5776529507777667\n",
      "Gradient Descent(4953/4999): loss=0.5776527834475678\n",
      "Gradient Descent(4954/4999): loss=0.5776526161537306\n",
      "Gradient Descent(4955/4999): loss=0.5776524488962396\n",
      "Gradient Descent(4956/4999): loss=0.5776522816750792\n",
      "Gradient Descent(4957/4999): loss=0.5776521144902338\n",
      "Gradient Descent(4958/4999): loss=0.5776519473416879\n",
      "Gradient Descent(4959/4999): loss=0.577651780229426\n",
      "Gradient Descent(4960/4999): loss=0.5776516131534322\n",
      "Gradient Descent(4961/4999): loss=0.5776514461136913\n",
      "Gradient Descent(4962/4999): loss=0.5776512791101878\n",
      "Gradient Descent(4963/4999): loss=0.5776511121429059\n",
      "Gradient Descent(4964/4999): loss=0.5776509452118301\n",
      "Gradient Descent(4965/4999): loss=0.577650778316945\n",
      "Gradient Descent(4966/4999): loss=0.5776506114582354\n",
      "Gradient Descent(4967/4999): loss=0.5776504446356852\n",
      "Gradient Descent(4968/4999): loss=0.5776502778492791\n",
      "Gradient Descent(4969/4999): loss=0.5776501110990018\n",
      "Gradient Descent(4970/4999): loss=0.5776499443848377\n",
      "Gradient Descent(4971/4999): loss=0.5776497777067715\n",
      "Gradient Descent(4972/4999): loss=0.5776496110647873\n",
      "Gradient Descent(4973/4999): loss=0.57764944445887\n",
      "Gradient Descent(4974/4999): loss=0.5776492778890041\n",
      "Gradient Descent(4975/4999): loss=0.577649111355174\n",
      "Gradient Descent(4976/4999): loss=0.5776489448573645\n",
      "Gradient Descent(4977/4999): loss=0.57764877839556\n",
      "Gradient Descent(4978/4999): loss=0.5776486119697449\n",
      "Gradient Descent(4979/4999): loss=0.5776484455799042\n",
      "Gradient Descent(4980/4999): loss=0.5776482792260221\n",
      "Gradient Descent(4981/4999): loss=0.5776481129080836\n",
      "Gradient Descent(4982/4999): loss=0.577647946626073\n",
      "Gradient Descent(4983/4999): loss=0.5776477803799748\n",
      "Gradient Descent(4984/4999): loss=0.577647614169774\n",
      "Gradient Descent(4985/4999): loss=0.5776474479954546\n",
      "Gradient Descent(4986/4999): loss=0.5776472818570021\n",
      "Gradient Descent(4987/4999): loss=0.5776471157544005\n",
      "Gradient Descent(4988/4999): loss=0.5776469496876346\n",
      "Gradient Descent(4989/4999): loss=0.577646783656689\n",
      "Gradient Descent(4990/4999): loss=0.5776466176615486\n",
      "Gradient Descent(4991/4999): loss=0.5776464517021976\n",
      "Gradient Descent(4992/4999): loss=0.5776462857786212\n",
      "Gradient Descent(4993/4999): loss=0.5776461198908036\n",
      "Gradient Descent(4994/4999): loss=0.5776459540387301\n",
      "Gradient Descent(4995/4999): loss=0.5776457882223847\n",
      "Gradient Descent(4996/4999): loss=0.5776456224417524\n",
      "Gradient Descent(4997/4999): loss=0.577645456696818\n",
      "Gradient Descent(4998/4999): loss=0.5776452909875661\n",
      "Gradient Descent(4999/4999): loss=0.5776451253139814\n"
     ]
    }
   ],
   "source": [
    "losses, ws = implementations.gradient_descent(ya, tXa, np.zeros(tXa.shape[1]), 5000, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.06261417,  0.09346269, -0.51469734, -0.12184504, -0.01805118,\n",
      "        0.12141591, -0.01805138,  0.28505277,  0.02562995,  0.07354738,\n",
      "        0.26678662,  0.04414842,  0.01563421,  0.05580277,  0.05216935,\n",
      "        0.0429947 ,  0.08389527,  0.03409597,  0.01753903,  0.03753528,\n",
      "        0.03753528])]\n"
     ]
    }
   ],
   "source": [
    "min_loss = min(losses)\n",
    "\n",
    "print([w for l, w in zip(losses, ws) if l == min_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
